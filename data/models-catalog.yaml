source: Red Hat
models:
    - name: RedHatAI/Llama-3.1-8B-Instruct
      provider: Meta
      description: Llama 3.1 8B Instruct
      readme: "# First, define a tool\ndef get_current_temperature(location: str) -> float:\n    \"\"\"\n    Get the current temperature at a location.\n    \n    Args:\n        location: The location to get the temperature for, in the format \"City, Country\"\n    Returns:\n        The current temperature at the specified location in the specified units, as a float.\n    \"\"\"\n    return 22.  # A real function should probably actually get the temperature!\n\n# Next, create a chat and apply the chat template\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n```\n\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\n\n```python\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nand then call the tool and append the result, with the `tool` role, like so:\n\n```python\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nAfter that, you can `generate()` again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the [LLaMA prompt format docs](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) and the Transformers [tool use documentation](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling).\n\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama)\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir Meta-Llama-3.1-8B-Instruct\n```\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n\n**Training utilized a cumulative of** 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n\n**Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Time (GPU hours)</strong>\n   </td>\n   <td><strong>Training Power Consumption (W)</strong>\n   </td>\n   <td><strong>Training Location-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n   <td><strong>Training Market-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 8B\n   </td>\n   <td>1.46M\n   </td>\n   <td>700\n   </td>\n   <td>420\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 70B\n   </td>\n   <td>7.0M\n   </td>\n   <td>700\n   </td>\n   <td>2,040\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 405B\n   </td>\n   <td>30.84M\n   </td>\n   <td>700\n   </td>\n   <td>8,930\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Total\n   </td>\n   <td>39.3M\n   <td>\n<ul>\n\n</ul>\n   </td>\n   <td>11,390\n   </td>\n   <td>0\n   </td>\n  </tr>\n</table>\n\n\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n\n\n## Training Data\n\n**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n\n**Data Freshness:** The pretraining data has a cutoff of December 2023.\n\n\n## Benchmark scores\n\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. \n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>66.7\n   </td>\n   <td>66.7\n   </td>\n   <td>79.5\n   </td>\n   <td>79.3\n   </td>\n   <td>85.2\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>36.2\n   </td>\n   <td>37.1\n   </td>\n   <td>55.0\n   </td>\n   <td>53.8\n   </td>\n   <td>61.6\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English\n   </td>\n   <td>3-5\n   </td>\n   <td>average/acc_char\n   </td>\n   <td>47.1\n   </td>\n   <td>47.8\n   </td>\n   <td>63.0\n   </td>\n   <td>64.6\n   </td>\n   <td>71.6\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA\n   </td>\n   <td>7\n   </td>\n   <td>acc_char\n   </td>\n   <td>72.6\n   </td>\n   <td>75.0\n   </td>\n   <td>83.8\n   </td>\n   <td>84.1\n   </td>\n   <td>85.8\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande\n   </td>\n   <td>5\n   </td>\n   <td>acc_char\n   </td>\n   <td>-\n   </td>\n   <td>60.5\n   </td>\n   <td>-\n   </td>\n   <td>83.3\n   </td>\n   <td>86.7\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (CoT)\n   </td>\n   <td>3\n   </td>\n   <td>average/em\n   </td>\n   <td>61.1\n   </td>\n   <td>64.2\n   </td>\n   <td>81.3\n   </td>\n   <td>81.6\n   </td>\n   <td>85.9\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge\n   </td>\n   <td>25\n   </td>\n   <td>acc_char\n   </td>\n   <td>79.4\n   </td>\n   <td>79.7\n   </td>\n   <td>93.1\n   </td>\n   <td>92.9\n   </td>\n   <td>96.1\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki\n   </td>\n   <td>5\n   </td>\n   <td>em\n   </td>\n   <td>78.5\n   </td>\n   <td>77.6\n   </td>\n   <td>89.7\n   </td>\n   <td>89.8\n   </td>\n   <td>91.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD\n   </td>\n   <td>1\n   </td>\n   <td>em\n   </td>\n   <td>76.4\n   </td>\n   <td>77.0\n   </td>\n   <td>85.6\n   </td>\n   <td>81.8\n   </td>\n   <td>89.3\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (F1)\n   </td>\n   <td>1\n   </td>\n   <td>f1\n   </td>\n   <td>44.4\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>51.1\n   </td>\n   <td>53.6\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ\n   </td>\n   <td>0\n   </td>\n   <td>acc_char\n   </td>\n   <td>75.7\n   </td>\n   <td>75.0\n   </td>\n   <td>79.0\n   </td>\n   <td>79.4\n   </td>\n   <td>80.0\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (F1)\n   </td>\n   <td>3\n   </td>\n   <td>f1\n   </td>\n   <td>58.4\n   </td>\n   <td>59.5\n   </td>\n   <td>79.7\n   </td>\n   <td>79.6\n   </td>\n   <td>84.8\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 405B Instruct</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>68.5\n   </td>\n   <td>69.4\n   </td>\n   <td>82.0\n   </td>\n   <td>83.6\n   </td>\n   <td>87.3\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>65.3\n   </td>\n   <td>73.0\n   </td>\n   <td>80.9\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>micro_avg/acc_char\n   </td>\n   <td>45.5\n   </td>\n   <td>48.3\n   </td>\n   <td>63.4\n   </td>\n   <td>66.4\n   </td>\n   <td>73.3\n   </td>\n  </tr>\n  <tr>\n   <td>IFEval\n   </td>\n   <td>\n   </td>\n   <td>\n   </td>\n   <td>76.8\n   </td>\n   <td>80.4\n   </td>\n   <td>82.9\n   </td>\n   <td>87.5\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Reasoning\n   </td>\n   <td>ARC-C\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>82.4\n   </td>\n   <td>83.4\n   </td>\n   <td>94.4\n   </td>\n   <td>94.8\n   </td>\n   <td>96.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>34.6\n   </td>\n   <td>30.4\n   </td>\n   <td>39.5\n   </td>\n   <td>46.7\n   </td>\n   <td>50.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Code\n   </td>\n   <td>HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>60.4\n   </td>\n   <td>72.6\n   </td>\n   <td>81.7\n   </td>\n   <td>80.5\n   </td>\n   <td>89.0\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP ++ base version\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>70.6\n   </td>\n   <td>72.8\n   </td>\n   <td>82.5\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>50.8\n   </td>\n   <td>-\n   </td>\n   <td>65.5\n   </td>\n   <td>75.2\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E MBPP\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>52.4\n   </td>\n   <td>-\n   </td>\n   <td>62.0\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Math\n   </td>\n   <td>GSM-8K (CoT)\n   </td>\n   <td>8\n   </td>\n   <td>em_maj1@1\n   </td>\n   <td>80.6\n   </td>\n   <td>84.5\n   </td>\n   <td>93.0\n   </td>\n   <td>95.1\n   </td>\n   <td>96.8\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>final_em\n   </td>\n   <td>29.1\n   </td>\n   <td>51.9\n   </td>\n   <td>51.0\n   </td>\n   <td>68.0\n   </td>\n   <td>73.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Tool Use\n   </td>\n   <td>API-Bank\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>48.3\n   </td>\n   <td>82.6\n   </td>\n   <td>85.1\n   </td>\n   <td>90.0\n   </td>\n   <td>92.0\n   </td>\n  </tr>\n  <tr>\n   <td>BFCL\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>60.3\n   </td>\n   <td>76.1\n   </td>\n   <td>83.0\n   </td>\n   <td>84.8\n   </td>\n   <td>88.5\n   </td>\n  </tr>\n  <tr>\n   <td>Gorilla Benchmark API Bench\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>1.7\n   </td>\n   <td>8.2\n   </td>\n   <td>14.7\n   </td>\n   <td>29.7\n   </td>\n   <td>35.3\n   </td>\n  </tr>\n  <tr>\n   <td>Nexus (0-shot)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>18.1\n   </td>\n   <td>38.5\n   </td>\n   <td>47.8\n   </td>\n   <td>56.7\n   </td>\n   <td>58.7\n   </td>\n  </tr>\n  <tr>\n   <td>Multilingual\n   </td>\n   <td>Multilingual MGSM (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>-\n   </td>\n   <td>68.9\n   </td>\n   <td>-\n   </td>\n   <td>86.9\n   </td>\n   <td>91.6\n   </td>\n  </tr>\n</table>\n\n#### Multilingual benchmarks\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Language</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"9\" ><strong>General</strong>\n   </td>\n   <td rowspan=\"9\" ><strong>MMLU (5-shot, macro_avg/acc)</strong>\n   </td>\n   <td>Portuguese\n   </td>\n   <td>62.12\n   </td>\n   <td>80.13\n   </td>\n   <td>84.95\n   </td>\n  </tr>\n  <tr>\n   <td>Spanish\n   </td>\n   <td>62.45\n   </td>\n   <td>80.05\n   </td>\n   <td>85.08\n   </td>\n  </tr>\n  <tr>\n   <td>Italian\n   </td>\n   <td>61.63\n   </td>\n   <td>80.4\n   </td>\n   <td>85.04\n   </td>\n  </tr>\n  <tr>\n   <td>German\n   </td>\n   <td>60.59\n   </td>\n   <td>79.27\n   </td>\n   <td>84.36\n   </td>\n  </tr>\n  <tr>\n   <td>French\n   </td>\n   <td>62.34\n   </td>\n   <td>79.82\n   </td>\n   <td>84.66\n   </td>\n  </tr>\n  <tr>\n   <td>Hindi\n   </td>\n   <td>50.88\n   </td>\n   <td>74.52\n   </td>\n   <td>80.31\n   </td>\n  </tr>\n  <tr>\n   <td>Thai\n   </td>\n   <td>50.32\n   </td>\n   <td>72.95\n   </td>\n   <td>78.21\n   </td>\n  </tr>\n</table>\n\n\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. \n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n* Provide protections for the community to help prevent the misuse of our models.\n\n\n### Responsible deployment \n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more. \n\n\n#### Llama 3.1 instruct \n\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. \n\n**Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals and Tone**\n\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines. \n\n\n#### Llama 3.1 systems\n\n**Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. \n\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n\n#### New capabilities \n\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \n\n**Multilinguality**: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \n\n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application. \n\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**\n\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. \n\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n\n### Critical and other risks \n\nWe specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. \n\n\n**2. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n**3. Cyber attack enablement**\n\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\nOur study of Llama-3.1-405B’s social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development. "
      language:
        - en
        - fr
        - de
        - hi
        - it
        - pt
        - es
        - th
      license: llama3.1
      licenseLink: https://github.com/meta-llvm/llama-models/blob/main/models/llama3_1/LICENSE
      tasks:
        - text-generation
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1744896921000"
      customProperties:
        facebook:
            metadataType: MetadataStringValue
            string_value: ""
        llama:
            metadataType: MetadataStringValue
            string_value: ""
        llama-3:
            metadataType: MetadataStringValue
            string_value: ""
        meta:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744896921000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Llama-3.1-Nemotron-70B-Instruct-HF
      provider: NVIDIA
      description: A large language model customized by NVIDIA to improve helpfulness, converted to support the HuggingFace Transformers codebase.
      readme: "# Model Overview\n\n## Description:\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.\n\n\nThis model reaches [Arena Hard](https://github.com/lmarena/arena-hard-auto) of 85.0, [AlpacaEval 2 LC](https://tatsu-lab.github.io/alpaca_eval/) of 57.6 and [GPT-4-Turbo MT-Bench](https://github.com/lm-sys/FastChat/pull/3158) of 8.98, which are known to be predictive of [LMSys Chatbot Arena Elo](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)\n\nAs of 1 Oct 2024, this model is #1 on all three automatic alignment benchmarks (verified tab for AlpacaEval 2 LC), edging out strong frontier models such as GPT-4o and Claude 3.5 Sonnet.\n\nAs of Oct 24th, 2024 the model has Elo Score of 1267(+-7), rank 9 and style controlled rank of 26 on [ChatBot Arena leaderboard](https://lmarena.ai/?leaderboard).\n\nThis model was trained using RLHF (specifically, REINFORCE), [Llama-3.1-Nemotron-70B-Reward](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward) and [HelpSteer2-Preference prompts](https://huggingface.co/datasets/nvidia/HelpSteer2) on a Llama-3.1-70B-Instruct model as the initial policy.\n\nLlama-3.1-Nemotron-70B-Instruct-HF has been converted from [Llama-3.1-Nemotron-70B-Instruct](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct) to support it in the HuggingFace Transformers codebase. Please note that evaluation results might be slightly different from the [Llama-3.1-Nemotron-70B-Instruct](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct) as evaluated in NeMo-Aligner, which the evaluation results below are based on.\n\nTry hosted inference for free at [build.nvidia.com](https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct) - it comes with an OpenAI-compatible API interface.\n\n\nSee details on our paper at [https://arxiv.org/abs/2410.01257](https://arxiv.org/abs/2410.01257) - as a preview, this model can correctly the question ```How many r in strawberry?``` without specialized prompting or additional reasoning tokens:\n\n```\nA sweet question!\nLet’s count the “R”s in “strawberry”:\n1. S\n2. T\n3. R\n4. A\n5. W\n6. B\n7. E\n8. R\n9. R\n10. Y\nThere are **3 “R”s** in the word “strawberry”.\n```\n\nNote: This model is a demonstration of our techniques for improving helpfulness in general-domain instruction following. It has not been tuned for performance in specialized domains such as math.\n\n\n## License\nYour use of this model is governed by the [NVIDIA Open Model License](https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf).\nAdditional Information: [Llama 3.1 Community License Agreement](https://www.llama.com/llama3_1/license/). Built with Llama.\n\n## Evaluation Metrics\n\nAs of 1 Oct 2024, Llama-3.1-Nemotron-70B-Instruct performs best on Arena Hard, AlpacaEval 2 LC (verified tab) and MT Bench (GPT-4-Turbo)\n\n | Model  | Arena Hard | AlpacaEval | MT-Bench | Mean Response Length |\n|:-----------------------------|:----------------|:-----|:----------|:-------|\n|Details | (95% CI) | 2 LC (SE) | (GPT-4-Turbo) | (# of Characters for MT-Bench)| \n| _**Llama-3.1-Nemotron-70B-Instruct**_ | **85.0** (-1.5, 1.5) | **57.6** (1.65) | **8.98** | 2199.8 | \n| Llama-3.1-70B-Instruct | 55.7 (-2.9, 2.7) | 38.1 (0.90)  | 8.22 | 1728.6 |\n| Llama-3.1-405B-Instruct | 69.3 (-2.4, 2.2) | 39.3 (1.43) | 8.49 | 1664.7 |\n| Claude-3-5-Sonnet-20240620 | 79.2 (-1.9, 1.7) | 52.4 (1.47) | 8.81 | 1619.9 |\n| GPT-4o-2024-05-13 | 79.3 (-2.1, 2.0) | 57.5 (1.47) | 8.74 | 1752.2 |\n         \n## Usage:\n\nYou can use the model using HuggingFace Transformers library with 2 or more 80GB GPUs (NVIDIA Ampere or newer) with at least 150GB of free disk space to accomodate the download.\n\nThis code has been tested on Transformers v4.44.0, torch v2.4.0 and 2 A100 80GB GPUs, but any setup that supports ```meta-llama/Llama-3.1-70B-Instruct``` should support this model as well. If you run into problems, you can consider doing ```pip install -U transformers```.\n\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"How many r in strawberry?\"\nmessages = [{\"role\": \"user\", \"content\": prompt}]\n\ntokenized_message = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\nresponse_token_ids = model.generate(tokenized_message['input_ids'].cuda(),attention_mask=tokenized_message['attention_mask'].cuda(),  max_new_tokens=4096, pad_token_id = tokenizer.eos_token_id)\ngenerated_tokens =response_token_ids[:, len(tokenized_message['input_ids'][0]):]\ngenerated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\nprint(generated_text)\n\n# See response at top of model card\n```\n\n## References(s):\n\n* [NeMo Aligner](https://arxiv.org/abs/2405.01481)\n* [HelpSteer2-Preference](https://arxiv.org/abs/2410.01257)\n* [HelpSteer2](https://arxiv.org/abs/2406.08673)\n* [Introducing Llama 3.1: Our most capable models to date](https://ai.meta.com/blog/meta-llama-3-1/) \n* [Meta's Llama 3.1 Webpage](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1) \n* [Meta's Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md)\n \n\n## Model Architecture: \n**Architecture Type:** Transformer <br>\n**Network Architecture:** Llama 3.1 <br>\n\n## Input:\n**Input Type(s):** Text <br>\n**Input Format:** String <br>\n**Input Parameters:** One Dimensional (1D) <br>\n**Other Properties Related to Input:** Max of 128k tokens<br>\n\n## Output:\n**Output Type(s):** Text <br>\n**Output Format:** String <br>\n**Output Parameters:** One Dimensional (1D) <br>\n**Other Properties Related to Output:**  Max of 4k tokens <br>\n\n\n## Software Integration:\n**Supported Hardware Microarchitecture Compatibility:** <br>\n* NVIDIA Ampere <br>\n* NVIDIA Hopper <br>\n* NVIDIA Turing <br>\n**Supported Operating System(s):** Linux <br>\n\n## Model Version: \nv1.0\n\n# Training & Evaluation: \n\n## Alignment methodology\n* REINFORCE implemented in NeMo Aligner \n\n## Datasets:\n\n**Data Collection Method by dataset** <br>\n* [Hybrid: Human, Synthetic] <br>\n\n**Labeling Method by dataset** <br>\n* [Human] <br>\n\n**Link:** \n* [HelpSteer2](https://huggingface.co/datasets/nvidia/HelpSteer2)\n\n**Properties (Quantity, Dataset Descriptions, Sensor(s)):** <br>\n* 21, 362 prompt-responses built to make more models more aligned with human preference - specifically more helpful, factually-correct, coherent, and customizable based on complexity and verbosity.\n* 20, 324 prompt-responses used for training and 1, 038 used for validation.\n\n\n# Inference:\n**Engine:** [Triton](https://developer.nvidia.com/triton-inference-server) <br>\n**Test Hardware:** H100, A100 80GB, A100 40GB <br>\n\n\n## Ethical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\n## Citation\n\nIf you find this model useful, please cite the following works\n\n```bibtex\n@misc{wang2024helpsteer2preferencecomplementingratingspreferences,\n      title={HelpSteer2-Preference: Complementing Ratings with Preferences}, \n      author={Zhilin Wang and Alexander Bukharin and Olivier Delalleau and Daniel Egert and Gerald Shen and Jiaqi Zeng and Oleksii Kuchaiev and Yi Dong},\n      year={2024},\n      eprint={2410.01257},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2410.01257}, \n}\n```"
      language:
        - en
      license: llama3.1
      licenseLink: https://github.com/meta-llvm/llama-models/blob/main/models/llama3_1/LICENSE
      tasks:
        - text-to-text
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1746720264000"
      customProperties:
        conversational:
            metadataType: MetadataStringValue
            string_value: ""
        llama:
            metadataType: MetadataStringValue
            string_value: ""
        llama3.1:
            metadataType: MetadataStringValue
            string_value: ""
        nvidia:
            metadataType: MetadataStringValue
            string_value: ""
        text-generation-inference:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-nemotron-70b-instruct-hf:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1746720264000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Llama-3.1-Nemotron-70B-Instruct-HF-FP8-dynamic
      provider: Neural Magic
      description: Llama 3.1 Nemotron 70B Instruct Hf Fp8 Dynamic - An instruction-tuned language model
      readme: "# Llama-3.1-Nemotron-70B-Instruct-HF-FP8-dynamic\n\n## Model Overview\n- **Model Architecture:** Llama-3.1-70B-Instruct\n  - **Input:** Text\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Weight quantization:** FP8\n  - **Activation quantization:** FP8\n- **Release Date:** 3/1/2025\n- **Version:** 1.0\n- **Model Developers:** Neural Magic\n\nQuantized version of [Llama-3.1-Nemotron-70B-Instruct-Hf](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF).\nIt achieves an average score of 78.9 on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) benchmark (version 1), whereas the unquantized model achieves 78.92.\n\n### Model Optimizations\n\nThis model was obtained by quantizing the weights and activations to FP8 data type, ready for inference with vLLM >= 0.5.2.\nThis optimization reduces the number of bits per parameter from 16 to 8, reducing the disk size and GPU memory requirements by approximately 50%. Only the weights and activations of the linear operators within transformers blocks are quantized. \n\n## Deployment\n\n### Use with vLLM\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\nmax_model_len, tp_size = 4096, 1\nmodel_name = \"neuralmagic-ent/Llama-3.1-Nemotron-70B-Instruct-HF-FP8-dynamic\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\n\nmessages_list = [\n    [{\"role\": \"user\", \"content\": \"Who are you? Please respond in pirate speak!\"}],\n]\n\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\n\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n```\n\nvLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\nThis model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n\n\n```python\nimport argparse\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom llmcompressor.modifiers.quantization import QuantizationModifier\nfrom llmcompressor.transformers import oneshot\nimport os\n\ndef main():\n    parser = argparse.ArgumentParser(description='Quantize a transformer model to FP8')\n    parser.add_argument('--model_id', type=str, required=True,\n                        help='The model ID from HuggingFace (e.g., \"meta-llama/Meta-Llama-3-8B-Instruct\")')\n    parser.add_argument('--save_path', type=str, default='.',\n                        help='Custom path to save the quantized model. If not provided, will use model_name-FP8-dynamic')\n    args = parser.parse_args()\n\n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_id, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True,\n    )\n    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n\n    # Configure the quantization algorithm and scheme\n    recipe = QuantizationModifier(\n        targets=\"Linear\", scheme=\"FP8_DYNAMIC\", ignore=[\"lm_head\"]\n    )\n\n    # Apply quantization\n    oneshot(model=model, recipe=recipe)\n\n    save_path = os.path.join(args.save_path, args.model_id.split(\"/\")[1] + \"-FP8-dynamic\")\n    os.makedirs(save_path, exist_ok=True)\n\n    # Save to disk in compressed-tensors format\n    model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n    print(f\"Model and tokenizer saved to: {save_path}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Evaluation\n\nThe model was evaluated on OpenLLM Leaderboard [V1](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard) and [V2](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/), using the following commands:\n\nOpenLLM Leaderboard V1:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic-ent/Llama-3.1-Nemotron-70B-Instruct-HF-FP8-dynamic\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \\\n  --tasks openllm \\\n  --write_out \\\n  --batch_size auto \\\n  --output_path output_dir \\\n  --show_config\n```\n\nOpenLLM Leaderboard V2:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic-ent/Llama-3.1-Nemotron-70B-Instruct-HF-FP8-dynamic\",dtype=auto,add_bos_token=False,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \\\n  --apply_chat_template \\\n  --fewshot_as_multiturn \\\n  --tasks leaderboard \\\n  --write_out \\\n  --batch_size auto \\\n  --output_path output_dir \\\n  --show_config\n\n```\n\n### Accuracy\n\n#### OpenLLM Leaderboard V1 evaluation scores\n\n| Metric                                   | nvidia/Llama-3.1-Nemotron-70B-Instruct-HF             | neuralmagic-ent/Llama-3.1-Nemotron-70B-Instruct-HF-FP8-dynamic |\n|-----------------------------------------|:---------------------------------:|:-------------------------------------------:|\n| ARC-Challenge (Acc-Norm, 25-shot)       | 71.76                            | 72.10                                       |\n| GSM8K (Strict-Match, 5-shot)            | 82.94                            |  82.79                                        |\n| HellaSwag (Acc-Norm, 10-shot)           | 87.61                            | 87.43                                       |\n| MMLU (Acc, 5-shot)                      | 82.56                            | 82.65                                       |\n| TruthfulQA (MC2, 0-shot)                | 64.85                            | 64.97                                       |\n| Winogrande (Acc, 5-shot)                | 83.82                            | 83.43                                       |\n| **Average Score**                       | **78.92**                        | **78.90**                                   |\n| **Recovery**                            | **100.00**                       | **99.97**                                   |\n\n#### OpenLLM Leaderboard V2 evaluation scores\n\n| Metric                                                   | nvidia/Llama-3.1-Nemotron-70B-Instruct-HF             | neuralmagic-ent/Llama-3.1-Nemotron-70B-Instruct-HF-FP8-dynamic |\n|---------------------------------------------------------|:---------------------------------:|:-------------------------------------------:|\n| IFEval (Inst-and-Prompt Level Strict Acc, 0-shot)       | 74.3                            | 74.72                                       |\n| BBH (Acc-Norm, 3-shot)                                  | 47.39                            | 46.92                                       |\n| Math-Hard (Exact-Match, 4-shot)                         | 23.21                            | 21.83                                       |\n| GPQA (Acc-Norm, 0-shot)                                 | 1.2                             | 1.34                                        |\n| MUSR (Acc-Norm, 0-shot)                                 | 13.2                             | 14.06                                       |\n| MMLU-Pro (Acc, 5-shot)                                  | 43.45                            | 43.42                                       |\n| **Average Score**                                       | **33.79**                        | **33.72**                                   |\n| **Recovery**                                            | **100.00**                       | **99.79**                                   |\n\n"
      language:
        - en
      license: llama3.1
      licenseLink: https://github.com/meta-llvm/llama-models/blob/main/models/llama3_1/LICENSE
      tasks:
        - text-generation
      createTimeSinceEpoch: "1740787200"
      lastUpdateTimeSinceEpoch: "1740787200"
      customProperties:
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        fp8:
            metadataType: MetadataStringValue
            string_value: ""
        llama:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-nemotron-70b-instruct-hf-fp8-dynamic:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1746683511000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Llama-3.3-70B-Instruct
      provider: Meta
      description: Llama 3.3 70B Instruct
      readme: "# First, define a tool\ndef get_current_temperature(location: str) -> float:\n    \"\"\"\n    Get the current temperature at a location.\n    \n    Args:\n        location: The location to get the temperature for, in the format \"City, Country\"\n    Returns:\n        The current temperature at the specified location in the specified units, as a float.\n    \"\"\"\n    return 22.  # A real function should probably actually get the temperature!\n\n# Next, create a chat and apply the chat template\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n```\n\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\n\n```python\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nand then call the tool and append the result, with the `tool` role, like so:\n\n```python\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nAfter that, you can `generate()` again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the [LLaMA prompt format docs](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) and the Transformers [tool use documentation](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling).\n\n\n### Use with `bitsandbytes`\n\nThe model checkpoints can be used in `8-bit` and `4-bit` for further memory optimisations using `bitsandbytes` and `transformers`\n\nSee the snippet below for usage:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\tmodel_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutput = quantized_model.generate(**input_ids, max_new_tokens=10)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nTo load in 4-bit simply pass `load_in_4bit=True`\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.3-70B-Instruct --include \"original/*\" --local-dir Llama-3.3-70B-Instruct\n```\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use** Training utilized a cumulative of **39.3**M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n## \n\n## **Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n|  | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | :---: | :---: | :---: |\n| Llama 3.3 70B | 7.0M | 700 | 2,040 | 0 |\n\n## The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 3.3 was pretrained on \\~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n\n## Benchmarks \\- English Text\n\nIn this section, we report the results for Llama 3.3 relative to our previous models. \n\n### Instruction tuned models\n\n## \n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.1 8B Instruct | Llama 3.1 70B Instruct | Llama-3.3 70B Instruct | Llama 3.1 405B Instruct |\n| :---- | :---- | ----- | :---- | ----- | ----- | ----- | ----- |\n|  | MMLU (CoT) | 0 | macro\\_avg/acc | 73.0 | 86.0 | 86.0 | 88.6 |\n|  | MMLU Pro (CoT) | 5 | macro\\_avg/acc | 48.3 | 66.4 | 68.9 | 73.3 |\n| Steerability | IFEval |  |  | 80.4 | 87.5 | 92.1 | 88.6 |\n| Reasoning | GPQA Diamond (CoT) | 0 | acc | 31.8 | 48.0 | 50.5 | 49.0 |\n| Code | HumanEval | 0 | pass@1 | 72.6 | 80.5 | 88.4 | 89.0 |\n|  | MBPP EvalPlus (base) | 0 | pass@1 | 72.8 | 86.0 | 87.6 | 88.6 |\n| Math | MATH (CoT) | 0 | sympy\\_intersection\\_score | 51.9 | 68.0 | 77.0 | 73.8 |\n| Tool Use | BFCL v2 | 0 | overall\\_ast\\_summary/macro\\_avg/valid | 65.4 | 77.5 | 77.3 | 81.1 |\n| Multilingual | MGSM | 0 | em | 68.9 | 86.9 | 91.1 | 91.6 |\n\n## \n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n* Provide protections for the community to help prevent the misuse of our models.\n\n### Responsible deployment\n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.3 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more.\n\n#### Llama 3.3 instruct\n\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\n\n**Fine-tuning data**\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone**\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.3 systems\n\n**Large language models, including Llama 3.3, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n#### Capability specific considerations \n\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \n\n**Multilinguality**: Llama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.   \nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**   \nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.   \nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. . \n\n### Critical and other risks \n\n### We specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\nTo assess risks related to proliferation of chemical and biological weapons of the Llama 3 family of models, we performed uplift testing designed to assess whether use of the Llama 3 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n\n### **2\\. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n**3\\. Cyber attack enablement**\nOur cyber attack uplift study investigated whether the Llama 3 family of LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3.3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3.3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.3 model, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development."
      language:
        - en
        - fr
        - de
        - hi
        - it
        - pt
        - es
        - th
      license: llama3.3
      licenseLink: https://github.com/meta-llvm/llama-models/blob/main/models/llama3_3/LICENSE
      tasks:
        - text-generation
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1744927337000"
      customProperties:
        facebook:
            metadataType: MetadataStringValue
            string_value: ""
        llama:
            metadataType: MetadataStringValue
            string_value: ""
        llama-3:
            metadataType: MetadataStringValue
            string_value: ""
        meta:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-3-3-70b-instruct:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744927337000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
      provider: RedHat (Neural Magic)
      description: '- **Model Architecture:** Meta-Llama-3.1'
      readme: "# Llama-3.3-70B-Instruct-FP8-dynamic\n\n## Model Overview\n- **Model Architecture:** Meta-Llama-3.1\n  - **Input:** Text\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Weight quantization:** FP8\n  - **Activation quantization:** FP8\n- **Intended Use Cases:** Intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases.\n- **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n- **Release Date:** 12/11/2024\n- **Version:** 1.0\n- **License(s):** llama3.3\n- **Model Developers:** RedHat (Neural Magic)\n\n### Model Optimizations\n\nThis model was obtained by quantizing activation and weights of [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) to FP8 data type.\nThis optimization reduces the number of bits used to represent weights and activations from 16 to 8, reducing GPU memory requirements (by approximately 50%) and increasing matrix-multiply compute throughput (by approximately 2x).\nWeight quantization also reduces disk size requirements by approximately 50%.\n\nOnly weights and activations of the linear operators within transformers blocks are quantized.\nWeights are quantized with a symmetric static per-channel scheme, whereas activations are quantized with a symmetric dynamic per-token scheme.\nThe [llm-compressor](https://github.com/vllm-project/llm-compressor) library is used for quantization.\n\n## Deployment\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel_id = \"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\"\nnumber_gpus = 1\n\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\nprompts = tokenizer.apply_chat_template(messages, tokenize=False)\n\nllm = LLM(model=model_id, tensor_parallel_size=number_gpus)\n\noutputs = llm.generate(prompts, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\nvLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\n<details>\n  <summary>Creation details</summary>\n  This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n\n\n  ```python\n  from transformers import AutoModelForCausalLM, AutoTokenizer\n  from llmcompressor.modifiers.quantization import QuantizationModifier\n  from llmcompressor.transformers import oneshot\n  \n  # Load model\n  model_stub = \"meta-llama/Llama-3.3-70B-Instruct\"\n  model_name = model_stub.split(\"/\")[-1]\n  \n  tokenizer = AutoTokenizer.from_pretrained(model_stub)\n  \n  model = AutoModelForCausalLM.from_pretrained(\n      model_stub,\n      device_map=\"auto\",\n      torch_dtype=\"auto\",\n  )\n  \n  # Configure the quantization algorithm and scheme\n  recipe = QuantizationModifier(\n      targets=\"Linear\",\n      scheme=\"FP8_dynamic\",\n      ignore=[\"lm_head\"],\n  )\n  \n  # Apply quantization\n  oneshot(\n      model=model,\n      recipe=recipe,\n  )\n  \n  # Save to disk in compressed-tensors format\n  save_path = model_name + \"-FP8-dynamic\"\n  model.save_pretrained(save_path)\n  tokenizer.save_pretrained(save_path)\n  print(f\"Model and tokenizer saved to: {save_path}\")\n  ```\n</details>\n\n## Evaluation\n\nThis model was evaluated on the well-known OpenLLM v1, OpenLLM v2, HumanEval, and HumanEval+ benchmarks.\nIn all cases, model outputs were generated with the [vLLM](https://docs.vllm.ai/en/stable/) engine.\n\nOpenLLM v1 and v2 evaluations were conducted using [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) and the prompting style of [Meta-Llama-3.1-Instruct-evals](https://huggingface.co/datasets/meta-llama/Meta-Llama-3.1-8B-Instruct-evals) when available.\n\nHumanEval and HumanEval+ evaluations were conducted using Neural Magic's fork of the [EvalPlus](https://github.com/neuralmagic/evalplus) repository.\n\n<details>\n  <summary>Evaluation details</summary>\n\n  **MMLU**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n    --tasks mmlu_llama \\\n    --fewshot_as_multiturn \\\n    --apply_chat_template \\\n    --num_fewshot 5 \\\n    --batch_size auto\n  ```\n\n  **MMLU-CoT**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,max_model_len=4064,max_gen_toks=1024,tensor_parallel_size=1 \\\n    --tasks mmlu_cot_llama \\\n    --apply_chat_template \\\n    --num_fewshot 0 \\\n    --batch_size auto\n  ```\n\n  **ARC-Challenge**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,max_model_len=3940,max_gen_toks=100,tensor_parallel_size=1 \\\n    --tasks arc_challenge_llama \\\n    --apply_chat_template \\\n    --num_fewshot 0 \\\n    --batch_size auto\n  ```\n\n  **GSM-8K**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,max_model_len=4096,max_gen_toks=1024,tensor_parallel_size=1 \\\n    --tasks gsm8k_llama \\\n    --fewshot_as_multiturn \\\n    --apply_chat_template \\\n    --num_fewshot 8 \\\n    --batch_size auto\n  ```\n\n  **Hellaswag**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \\\n    --tasks hellaswag \\\n    --num_fewshot 10 \\\n    --batch_size auto\n  ```\n\n  **Winogrande**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \\\n    --tasks winogrande \\\n    --num_fewshot 5 \\\n    --batch_size auto\n  ```\n\n  **TruthfulQA**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \\\n    --tasks truthfulqa \\\n    --num_fewshot 0 \\\n    --batch_size auto\n  ```\n\n  **OpenLLM v2**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,max_model_len=4096,tensor_parallel_size=1,enable_chunked_prefill=True \\\n    --apply_chat_template \\\n    --fewshot_as_multiturn \\\n    --tasks leaderboard \\\n    --batch_size auto\n  ```\n\n  **MMLU Portuguese**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n    --tasks mmlu_pt_llama \\\n    --fewshot_as_multiturn \\\n    --apply_chat_template \\\n    --num_fewshot 5 \\\n    --batch_size auto\n  ```\n\n  **MMLU Spanish**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n    --tasks mmlu_es_llama \\\n    --fewshot_as_multiturn \\\n    --apply_chat_template \\\n    --num_fewshot 5 \\\n    --batch_size auto\n  ```\n\n  **MMLU Italian**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n    --tasks mmlu_it_llama \\\n    --fewshot_as_multiturn \\\n    --apply_chat_template \\\n    --num_fewshot 5 \\\n    --batch_size auto\n  ```\n\n  **MMLU German**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n    --tasks mmlu_de_llama \\\n    --fewshot_as_multiturn \\\n    --apply_chat_template \\\n    --num_fewshot 5 \\\n    --batch_size auto\n  ```\n\n  **MMLU French**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n    --tasks mmlu_fr_llama \\\n    --fewshot_as_multiturn \\\n    --apply_chat_template \\\n    --num_fewshot 5 \\\n    --batch_size auto\n  ```\n\n  **MMLU Hindi**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n    --tasks mmlu_hi_llama \\\n    --fewshot_as_multiturn \\\n    --apply_chat_template \\\n    --num_fewshot 5 \\\n    --batch_size auto\n  ```\n\n  **MMLU Thai**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n    --tasks mmlu_th_llama \\\n    --fewshot_as_multiturn \\\n    --apply_chat_template \\\n    --num_fewshot 5 \\\n    --batch_size auto\n  ```\n\n  **HumanEval and HumanEval+**\n  *Generation*\n  ```\n  python3 codegen/generate.py \\\n    --model RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic \\\n    --bs 16 \\\n    --temperature 0.2 \\\n    --n_samples 50 \\\n    --root \".\" \\\n    --dataset humaneval\n  ```\n\n  *Sanitization*\n  ```\n  python3 evalplus/sanitize.py \\\n    humaneval/RedHatAI--Llama-3.3-70B-Instruct-FP8-dynamic_vllm_temp_0.2\n  ```\n\n  *Evaluation*\n  ```\n  evalplus.evaluate \\\n    --dataset humaneval \\\n    --samples humaneval/RedHatAI--Llama-3.3-70B-Instruct-FP8-dynamic_vllm_temp_0.2-sanitized\n  ```\n</details>\n\n### Accuracy\n\n<table>\n  <tr>\n   <th>Category\n   </th>\n   <th>Benchmark\n   </th>\n   <th>Llama-3.3-70B-Instruct\n   </th>\n   <th>Llama-3.3-70B-Instruct-FP8-dynamic<br>(this model)\n   </th>\n   <th>Recovery\n   </th>\n  </tr>\n  <tr>\n   <td rowspan=\"8\" ><strong>OpenLLM v1</strong>\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>81.60\n   </td>\n   <td>81.31\n   </td>\n   <td>99.6%\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (CoT, 0-shot)\n   </td>\n   <td>86.58\n   </td>\n   <td>86.34\n   </td>\n   <td>99.7%\n   </td>\n  </tr>\n  <tr>\n   <td>ARC Challenge (0-shot)\n   </td>\n   <td>49.23\n   </td>\n   <td>51.96\n   </td>\n   <td>105.6%\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (CoT, 8-shot, strict-match)\n   </td>\n   <td>94.16\n   </td>\n   <td>94.92\n   </td>\n   <td>100.8%\n   </td>\n  </tr>\n  <tr>\n   <td>Hellaswag (10-shot)\n   </td>\n   <td>86.49\n   </td>\n   <td>86.43\n   </td>\n   <td>99.9%\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>84.77\n   </td>\n   <td>84.53\n   </td>\n   <td>99.7%\n   </td>\n  </tr>\n  <tr>\n   <td>TruthfulQA (0-shot, mc2)\n   </td>\n   <td>62.75\n   </td>\n   <td>63.21\n   </td>\n   <td>100.7%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>77.94</strong>\n   </td>\n   <td><strong>78.39</strong>\n   </td>\n   <td><strong>100.6%</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" ><strong>OpenLLM v2</strong>\n   </td>\n   <td>MMLU-Pro (5-shot)\n   </td>\n   <td>51.89\n   </td>\n   <td>51.50\n   </td>\n   <td>99.3%\n   </td>\n  </tr>\n  <tr>\n   <td>IFEval (0-shot)\n   </td>\n   <td>90.89\n   </td>\n   <td>90.92\n   </td>\n   <td>100.0%\n   </td>\n  </tr>\n  <tr>\n   <td>BBH (3-shot)\n   </td>\n   <td>63.15\n   </td>\n   <td>62.84\n   </td>\n   <td>99.5%\n   </td>\n  </tr>\n  <tr>\n   <td>Math-lvl-5 (4-shot)\n   </td>\n   <td>0.17\n   </td>\n   <td>0.33\n   </td>\n   <td>N/A\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA (0-shot)\n   </td>\n   <td>46.10\n   </td>\n   <td>46.30\n   </td>\n   <td>100.4%\n   </td>\n  </tr>\n  <tr>\n   <td>MuSR (0-shot)\n   </td>\n   <td>44.35\n   </td>\n   <td>43.96\n   </td>\n   <td>99.1%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>49.42</strong>\n   </td>\n   <td><strong>49.31</strong>\n   </td>\n   <td><strong>99.8%</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" ><strong>Coding</strong>\n   </td>\n   <td>HumanEval pass@1\n   </td>\n   <td>83.20\n   </td>\n   <td>83.70\n   </td>\n   <td>100.6%\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval+ pass@1\n   </td>\n   <td>78.40\n   </td>\n   <td>78.70\n   </td>\n   <td>100.4%\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"9\" ><strong>Multilingual</strong>\n   </td>\n   <td>Portuguese MMLU (5-shot)\n   </td>\n   <td>79.76\n   </td>\n   <td>79.75\n   </td>\n   <td>100.0%\n   </td>\n  </tr>\n  <tr>\n   <td>Spanish MMLU (5-shot)\n   </td>\n   <td>79.33\n   </td>\n   <td>79.17\n   </td>\n   <td>99.8%\n   </td>\n  </tr>\n  <tr>\n   <td>Italian MMLU (5-shot)\n   </td>\n   <td>79.15\n   </td>\n   <td>78.84\n   </td>\n   <td>99.6%\n   </td>\n  </tr>\n  <tr>\n   <td>German MMLU (5-shot)\n   </td>\n   <td>77.94\n   </td>\n   <td>77.95\n   </td>\n   <td>100.0%\n   </td>\n  </tr>\n  <tr>\n   <td>French MMLU (5-shot)\n   </td>\n   <td>75.69\n   </td>\n   <td>75.45\n   </td>\n   <td>99.7%\n   </td>\n  </tr>\n  <tr>\n   <td>Hindi MMLU (5-shot)\n   </td>\n   <td>73.81\n   </td>\n   <td>73.71\n   </td>\n   <td>99.9%\n   </td>\n  </tr>\n  <tr>\n   <td>Thai MMLU (5-shot)\n   </td>\n   <td>71.98\n   </td>\n   <td>71.77\n   </td>\n   <td>99.7%\n   </td>\n  </tr>\n</table>\n\n\n"
      language:
        - en
        - de
        - fr
        - it
        - pt
        - hi
        - es
        - th
      license: llama3.3
      licenseLink: https://github.com/meta-llvm/llama-models/blob/main/models/llama3_3/LICENSE
      tasks:
        - text-generation
      createTimeSinceEpoch: "1733875200"
      lastUpdateTimeSinceEpoch: "1733875200"
      customProperties:
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        facebook:
            metadataType: MetadataStringValue
            string_value: ""
        fp8:
            metadataType: MetadataStringValue
            string_value: ""
        llama:
            metadataType: MetadataStringValue
            string_value: ""
        llama-3:
            metadataType: MetadataStringValue
            string_value: ""
        meta:
            metadataType: MetadataStringValue
            string_value: ""
        quantized:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-3-3-70b-instruct-fp8-dynamic:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744990133000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16
      provider: RedHat (Neural Magic)
      description: '- **Model Architecture:** Meta-Llama-3.1'
      readme: "# Llama-3.3-70B-Instruct-quantized.w4a16\n\n## Model Overview\n- **Model Architecture:** Meta-Llama-3.1\n  - **Input:** Text\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Weight quantization:** INT4\n- **Intended Use Cases:** Intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases.\n- **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n- **Release Date:** 12/11/2024\n- **Version:** 1.0\n- **License(s):** llama3.3\n- **Model Developers:** RedHat (Neural Magic)\n\n### Model Optimizations\n\nThis model was obtained by quantizing the weights of [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) to INT4 data type.\nThis optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 75%.\n\nOnly the weights of the linear operators within transformers blocks are quantized.\nWeights are quantized using a symmetric per-group scheme, with group size 128.\nThe [GPTQ](https://arxiv.org/abs/2210.17323) algorithm is applied for quantization, as implemented in the [llm-compressor](https://github.com/vllm-project/llm-compressor) library.\n\n## Deployment\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel_id = \"RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16\"\nnumber_gpus = 1\n\nsampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\nprompts = tokenizer.apply_chat_template(messages, tokenize=False)\n\nllm = LLM(model=model_id, tensor_parallel_size=number_gpus)\n\noutputs = llm.generate(prompts, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\nvLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\n<details>\n  <summary>Creation details</summary>\n  This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n\n\n  ```python\n  from transformers import AutoModelForCausalLM, AutoTokenizer\n  from llmcompressor.modifiers.quantization import GPTQModifier\n  from llmcompressor.transformers import oneshot\n  from datasets import load_dataset\n  \n  # Load model\n  model_stub = \"meta-llama/Llama-3.3-70B-Instruct\"\n  model_name = model_stub.split(\"/\")[-1]\n\n  num_samples = 1024\n  max_seq_len = 8192\n  \n  tokenizer = AutoTokenizer.from_pretrained(model_stub)\n  \n  model = AutoModelForCausalLM.from_pretrained(\n      model_stub,\n      device_map=\"auto\",\n      torch_dtype=\"auto\",\n  )\n  \n  def preprocess_fn(example):\n    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], add_generation_prompt=False, tokenize=False)}\n  \n  ds = load_dataset(\"neuralmagic/LLM_compression_calibration\", split=\"train\")\n  ds = ds.map(preprocess_fn)\n\n  # Configure the quantization algorithm and scheme\n  recipe = GPTQModifier(\n      targets=\"Linear\",\n      scheme=\"W4A16\",\n      ignore=[\"lm_head\"],\n      sequential_targets=[\"LlamaDecoderLayer\"],\n      dampening_frac=0.01,\n  )\n  \n  # Apply quantization\n  oneshot(\n      model=model,\n      dataset=ds, \n      recipe=recipe,\n      max_seq_length=max_seq_len,\n      num_calibration_samples=num_samples,\n  )\n  \n  # Save to disk in compressed-tensors format\n  save_path = model_name + \"-quantized.w4a16\"\n  model.save_pretrained(save_path)\n  tokenizer.save_pretrained(save_path)\n  print(f\"Model and tokenizer saved to: {save_path}\")\n  ```\n</details>\n\n## Evaluation\n\nThis model was evaluated on the well-known OpenLLM v1, HumanEval, and HumanEval+ benchmarks.\nIn all cases, model outputs were generated with the [vLLM](https://docs.vllm.ai/en/stable/) engine.\n\nOpenLLM v1 evaluations were conducted using [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) and the prompting style of [Meta-Llama-3.1-Instruct-evals](https://huggingface.co/datasets/meta-llama/Meta-Llama-3.1-8B-Instruct-evals) when available.\n\nHumanEval and HumanEval+ evaluations were conducted using Neural Magic's fork of the [EvalPlus](https://github.com/neuralmagic/evalplus) repository.\n\n<details>\n  <summary>Evaluation details</summary>\n\n  **MMLU**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n    --tasks mmlu_llama \\\n    --fewshot_as_multiturn \\\n    --apply_chat_template \\\n    --num_fewshot 5 \\\n    --batch_size auto\n  ```\n\n  **MMLU-CoT**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=4064,max_gen_toks=1024,tensor_parallel_size=1 \\\n    --tasks mmlu_cot_llama \\\n    --apply_chat_template \\\n    --num_fewshot 0 \\\n    --batch_size auto\n  ```\n\n  **ARC-Challenge**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=3940,max_gen_toks=100,tensor_parallel_size=1 \\\n    --tasks arc_challenge_llama \\\n    --apply_chat_template \\\n    --num_fewshot 0 \\\n    --batch_size auto\n  ```\n\n  **GSM-8K**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=4096,max_gen_toks=1024,tensor_parallel_size=1 \\\n    --tasks gsm8k_llama \\\n    --fewshot_as_multiturn \\\n    --apply_chat_template \\\n    --num_fewshot 8 \\\n    --batch_size auto\n  ```\n\n  **Hellaswag**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \\\n    --tasks hellaswag \\\n    --num_fewshot 10 \\\n    --batch_size auto\n  ```\n\n  **Winogrande**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \\\n    --tasks winogrande \\\n    --num_fewshot 5 \\\n    --batch_size auto\n  ```\n\n  **TruthfulQA**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \\\n    --tasks truthfulqa \\\n    --num_fewshot 0 \\\n    --batch_size auto\n  ```\n\n  **HumanEval and HumanEval+**\n  *Generation*\n  ```\n  python3 codegen/generate.py \\\n    --model RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16 \\\n    --bs 16 \\\n    --temperature 0.2 \\\n    --n_samples 50 \\\n    --root \".\" \\\n    --dataset humaneval\n  ```\n\n  *Sanitization*\n  ```\n  python3 evalplus/sanitize.py \\\n    humaneval/RedHatAI--Llama-3.3-70B-Instruct-quantized.w4a16_vllm_temp_0.2\n  ```\n\n  *Evaluation*\n  ```\n  evalplus.evaluate \\\n    --dataset humaneval \\\n    --samples humaneval/RedHatAI--Llama-3.3-70B-Instruct-quantized.w4a16_vllm_temp_0.2-sanitized\n  ```\n</details>\n\n### Accuracy\n\n<table>\n  <tr>\n   <th>Category\n   </th>\n   <th>Benchmark\n   </th>\n   <th>Llama-3.3-70B-Instruct\n   </th>\n   <th>Llama-3.3-70B-Instruct-quantized.w4a16<br>(this model)\n   </th>\n   <th>Recovery\n   </th>\n  </tr>\n  <tr>\n   <td rowspan=\"8\" ><strong>OpenLLM v1</strong>\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>81.60\n   </td>\n   <td>80.62\n   </td>\n   <td>98.8%\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (CoT, 0-shot)\n   </td>\n   <td>86.58\n   </td>\n   <td>85.81\n   </td>\n   <td>99.1%\n   </td>\n  </tr>\n  <tr>\n   <td>ARC Challenge (0-shot)\n   </td>\n   <td>49.23\n   </td>\n   <td>49.49\n   </td>\n   <td>100.5%\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (CoT, 8-shot, strict-match)\n   </td>\n   <td>94.16\n   </td>\n   <td>94.47\n   </td>\n   <td>100.3%\n   </td>\n  </tr>\n  <tr>\n   <td>Hellaswag (10-shot)\n   </td>\n   <td>86.49\n   </td>\n   <td>85.97\n   </td>\n   <td>99.4%\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>84.77\n   </td>\n   <td>\n   </td>\n   <td>%\n   </td>\n  </tr>\n  <tr>\n   <td>TruthfulQA (0-shot, mc2)\n   </td>\n   <td>62.75\n   </td>\n   <td>61.66\n   </td>\n   <td>98.3%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>77.94</strong>\n   </td>\n   <td><strong>77.49</strong>\n   </td>\n   <td><strong>98.3%</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" ><strong>Coding</strong>\n   </td>\n   <td>HumanEval pass@1\n   </td>\n   <td>83.20\n   </td>\n   <td>83.40\n   </td>\n   <td>100.2%\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval+ pass@1\n   </td>\n   <td>78.40\n   </td>\n   <td>78.60\n   </td>\n   <td>100.3%\n   </td>\n  </tr>\n</table>\n\n\n"
      language:
        - en
        - de
        - fr
        - it
        - pt
        - hi
        - es
        - th
      license: llama3.3
      licenseLink: https://github.com/meta-llvm/llama-models/blob/main/models/llama3_3/LICENSE
      tasks:
        - text-generation
      createTimeSinceEpoch: "1733875200"
      lastUpdateTimeSinceEpoch: "1733875200"
      customProperties:
        4-bit precision:
            metadataType: MetadataStringValue
            string_value: ""
        chat:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        facebook:
            metadataType: MetadataStringValue
            string_value: ""
        int4:
            metadataType: MetadataStringValue
            string_value: ""
        llama:
            metadataType: MetadataStringValue
            string_value: ""
        llama-3:
            metadataType: MetadataStringValue
            string_value: ""
        llmcompressor:
            metadataType: MetadataStringValue
            string_value: ""
        meta:
            metadataType: MetadataStringValue
            string_value: ""
        neuralmagic:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-3-3-70b-instruct-quantized-w4a16:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744986485000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Llama-3.3-70B-Instruct-quantized.w8a8
      provider: Neural Magic
      description: Llama 3.3 70B Instruct Quantized.w8a8 - An instruction-tuned language model
      readme: |
        # Llama-3.3-70B-Instruct-quantized.w8a8

        ## Model Overview
        - **Model Architecture:** Llama
          - **Input:** Text
          - **Output:** Text
        - **Model Optimizations:**
          - **Activation quantization:** INT8
          - **Weight quantization:** INT8
        - **Intended Use Cases:** Intended for commercial and research use multiple languages. Similarly to [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct), this models is intended for assistant-like chat.
        - **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws).
        - **Release Date:** 01/20/2025
        - **Version:** 1.0
        - **Model Developers:** Neural Magic

        Quantized version of [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct).
        It was evaluated on a several tasks to assess the its quality in comparison to the unquatized model, including multiple-choice, math reasoning, and open-ended text generation.
        Llama-3.3-70B-Instruct-quantized.w8a8 achieves 99.4% recovery for OpenLLM v1 (using Meta's prompting when available) and 100% for both HumanEval and HumanEval+ pass@1.

        ### Model Optimizations

        This model was obtained by quantizing the weights and activations of [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) to INT8 data type.
        This optimization reduces the number of bits used to represent weights and activations from 16 to 8, reducing GPU memory requirements (by approximately 50%) and increasing matrix-multiply compute throughput (by approximately 2x).
        Weight quantization also reduces disk size requirements by approximately 50%.

        Only weights and activations of the linear operators within transformers blocks are quantized.
        Weights are quantized with a symmetric static per-channel scheme, where a fixed linear scaling factor is applied between INT8 and floating point representations for each output channel dimension.
        Activations are quantized with a symmetric dynamic per-token scheme, computing a linear scaling factor at runtime for each token between INT8 and floating point representations.

        ## Deployment

        This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

        ```python
        from vllm import LLM, SamplingParams
        from transformers import AutoTokenizer

        model_id = "neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8"
        number_gpus = 1
        max_model_len = 8192

        sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)

        tokenizer = AutoTokenizer.from_pretrained(model_id)

        messages = [
            {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
            {"role": "user", "content": "Who are you?"},
        ]

        prompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)

        llm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=max_model_len)

        outputs = llm.generate(prompts, sampling_params)

        generated_text = outputs[0].outputs[0].text
        print(generated_text)
        ```

        vLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.


        ## Creation

        This model was created by using the [llm-compressor](https://github.com/vllm-project/llm-compressor) library as presented in the code snipet below.

        ```python
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from datasets import Dataset
        from llmcompressor.transformers import oneshot
        from llmcompressor.modifiers.quantization import GPTQModifier
        import random

        model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"

        num_samples = 1024
        max_seq_len = 8192

        tokenizer = AutoTokenizer.from_pretrained(model_id)

        max_token_id = len(tokenizer.get_vocab()) - 1
        input_ids = [[random.randint(0, max_token_id) for _ in range(max_seq_len)] for _ in range(num_samples)]
        attention_mask = num_samples * [max_seq_len * [1]]
        ds = Dataset.from_dict({"input_ids": input_ids, "attention_mask": attention_mask})

        recipe = GPTQModifier(
          targets="Linear",
          scheme="W8A8",
          ignore=["lm_head"],
          dampening_frac=0.01,
        )

        model = SparseAutoModelForCausalLM.from_pretrained(
          model_id,
          device_map="auto",
        )

        oneshot(
          model=model,
          dataset=ds,
          recipe=recipe,
          max_seq_length=max_seq_len,
          num_calibration_samples=num_samples,
        )

        model.save_pretrained("Llama-3.3-70B-Instruct-quantized.w8a8")
        ```

        ## Evaluation

        This model was evaluated on the well-known OpenLLM v1, OpenLLM v2, HumanEval, and HumanEval+ benchmarks.
        In all cases, model outputs were generated with the [vLLM](https://docs.vllm.ai/en/stable/) engine.

        OpenLLM v1 and v2 evaluations were conducted using Neural Magic's fork of [lm-evaluation-harness](https://github.com/neuralmagic/lm-evaluation-harness/tree/llama_3.1_instruct) (branch llama_3.1_instruct).
        This version of the lm-evaluation-harness includes versions of MMLU, ARC-Challenge and GSM-8K that match the prompting style of [Meta-Llama-3.1-Instruct-evals](https://huggingface.co/datasets/meta-llama/Meta-Llama-3.1-8B-Instruct-evals) and a few fixes to OpenLLM v2 tasks.

        HumanEval and HumanEval+ evaluations were conducted using Neural Magic's fork of the [EvalPlus](https://github.com/neuralmagic/evalplus) repository.

        ### Accuracy

        <table>
          <tr>
           <th>Category
           </th>
           <th>Benchmark
           </th>
           <th>Llama-3.3-70B-Instruct
           </th>
           <th>Llama-3.3-70B-Instruct-quantized.w8a8 (this model)
           </th>
           <th>Recovery
           </th>
          </tr>
          <tr>
           <td rowspan="8" ><strong>OpenLLM v1</strong>
           </td>
           <td>MMLU (5-shot)
           </td>
           <td>81.60
           </td>
           <td>81.19
           </td>
           <td>99.5%
           </td>
          </tr>
          <tr>
           <td>MMLU (CoT, 0-shot)
           </td>
           <td>86.58
           </td>
           <td>85.92
           </td>
           <td>99.2%
           </td>
          </tr>
          <tr>
           <td>ARC Challenge (0-shot)
           </td>
           <td>49.23
           </td>
           <td>48.04
           </td>
           <td>97.6%
           </td>
          </tr>
          <tr>
           <td>GSM-8K (CoT, 8-shot, strict-match)
           </td>
           <td>94.16
           </td>
           <td>94.01
           </td>
           <td>99.8%
           </td>
          </tr>
          <tr>
           <td>Hellaswag (10-shot)
           </td>
           <td>86.49
           </td>
           <td>86.47
           </td>
           <td>100.0%
           </td>
          </tr>
          <tr>
           <td>Winogrande (5-shot)
           </td>
           <td>84.77
           </td>
           <td>83.74
           </td>
           <td>98.8%
           </td>
          </tr>
          <tr>
           <td>TruthfulQA (0-shot, mc2)
           </td>
           <td>62.75
           </td>
           <td>63.09
           </td>
           <td>99.5%
           </td>
          </tr>
          <tr>
           <td><strong>Average</strong>
           </td>
           <td><strong>77.94</strong>
           </td>
           <td><strong>77.49</strong>
           </td>
           <td><strong>99.4%</strong>
           </td>
          </tr>
          <tr>
           <td rowspan="7" ><strong>OpenLLM v2</strong>
           </td>
           <td>MMLU-Pro (5-shot)
           </td>
           <td>51.89
           </td>
           <td>51.59
           </td>
           <td>99.7%
           </td>
          </tr>
          <tr>
           <td>IFEval (0-shot)
           </td>
           <td>90.89
           </td>
           <td>90.68
           </td>
           <td>99.4%
           </td>
          </tr>
          <tr>
           <td>BBH (3-shot)
           </td>
           <td>63.15
           </td>
           <td>62.54
           </td>
           <td>99.0%
           </td>
          </tr>
          <tr>
           <td>Math-lvl-5 (4-shot)
           </td>
           <td>0.17
           </td>
           <td>0.00
           </td>
           <td>N/A
           </td>
          </tr>
          <tr>
           <td>GPQA (0-shot)
           </td>
           <td>46.10
           </td>
           <td>46.44
           </td>
           <td>100.8%
           </td>
          </tr>
          <tr>
           <td>MuSR (0-shot)
           </td>
           <td>44.35
           </td>
           <td>44.34
           </td>
           <td>100.0%
           </td>
          </tr>
          <tr>
           <td><strong>Average</strong>
           </td>
           <td><strong>49.42</strong>
           </td>
           <td><strong>49.27</strong>
           </td>
           <td><strong>99.7%</strong>
           </td>
          </tr>
          <tr>
           <td rowspan="2" ><strong>Coding</strong>
           </td>
           <td>HumanEval pass@1
           </td>
           <td>83.20
           </td>
           <td>83.30
           </td>
           <td>100.1%
           </td>
          </tr>
          <tr>
           <td>HumanEval+ pass@1
           </td>
           <td>78.40
           </td>
           <td>78.60
           </td>
           <td>100.3%
           </td>
          </tr>
          <tr>
           <td rowspan="9" ><strong>Multilingual</strong>
           </td>
           <td>Portuguese MMLU (5-shot)
           </td>
           <td>79.76
           </td>
           <td>79.47
           </td>
           <td>99.6%
           </td>
          </tr>
          <tr>
           <td>Spanish MMLU (5-shot)
           </td>
           <td>79.33
           </td>
           <td>79.23
           </td>
           <td>99.9%
           </td>
          </tr>
          <tr>
           <td>Italian MMLU (5-shot)
           </td>
           <td>79.15
           </td>
           <td>78.80
           </td>
           <td>99.6%
           </td>
          </tr>
          <tr>
           <td>German MMLU (5-shot)
           </td>
           <td>77.94
           </td>
           <td>77.92
           </td>
           <td>100.0%
           </td>
          </tr>
          <tr>
           <td>French MMLU (5-shot)
           </td>
           <td>75.69
           </td>
           <td>75.79
           </td>
           <td>100.1%
           </td>
          </tr>
          <tr>
           <td>Hindi MMLU (5-shot)
           </td>
           <td>73.81
           </td>
           <td>73.49
           </td>
           <td>99.6%
           </td>
          </tr>
          <tr>
           <td>Thai MMLU (5-shot)
           </td>
           <td>71.97
           </td>
           <td>71.44
           </td>
           <td>99.2%
           </td>
          </tr>
        </table>

        ### Reproduction

        The results were obtained using the following commands:

        #### MMLU
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
          --tasks mmlu_llama_3.1_instruct \
          --fewshot_as_multiturn \
          --apply_chat_template \
          --num_fewshot 5 \
          --batch_size auto
        ```

        #### MMLU-CoT
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,max_model_len=4064,max_gen_toks=1024,tensor_parallel_size=1 \
          --tasks mmlu_cot_0shot_llama_3.1_instruct \
          --apply_chat_template \
          --num_fewshot 0 \
          --batch_size auto
        ```

        #### ARC-Challenge
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,max_model_len=3940,max_gen_toks=100,tensor_parallel_size=1 \
          --tasks arc_challenge_llama_3.1_instruct \
          --apply_chat_template \
          --num_fewshot 0 \
          --batch_size auto
        ```

        #### GSM-8K
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,max_model_len=4096,max_gen_toks=1024,tensor_parallel_size=1 \
          --tasks gsm8k_cot_llama_3.1_instruct \
          --fewshot_as_multiturn \
          --apply_chat_template \
          --num_fewshot 8 \
          --batch_size auto
        ```

        #### Hellaswag
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
          --tasks hellaswag \
          --num_fewshot 10 \
          --batch_size auto
        ```

        #### Winogrande
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
          --tasks winogrande \
          --num_fewshot 5 \
          --batch_size auto
        ```

        #### TruthfulQA
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
          --tasks truthfulqa \
          --num_fewshot 0 \
          --batch_size auto
        ```

        #### OpenLLM v2
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,max_model_len=4096,tensor_parallel_size=1,enable_chunked_prefill=True \
          --apply_chat_template \
          --fewshot_as_multiturn \
          --tasks leaderboard \
          --batch_size auto
        ```

        #### MMLU Portuguese
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
          --tasks mmlu_pt_llama_3.1_instruct \
          --fewshot_as_multiturn \
          --apply_chat_template \
          --num_fewshot 5 \
          --batch_size auto
        ```

        #### MMLU Spanish
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
          --tasks mmlu_es_llama_3.1_instruct \
          --fewshot_as_multiturn \
          --apply_chat_template \
          --num_fewshot 5 \
          --batch_size auto
        ```

        #### MMLU Italian
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
          --tasks mmlu_it_llama_3.1_instruct \
          --fewshot_as_multiturn \
          --apply_chat_template \
          --num_fewshot 5 \
          --batch_size auto
        ```

        #### MMLU German
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
          --tasks mmlu_de_llama_3.1_instruct \
          --fewshot_as_multiturn \
          --apply_chat_template \
          --num_fewshot 5 \
          --batch_size auto
        ```

        #### MMLU French
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
          --tasks mmlu_fr_llama_3.1_instruct \
          --fewshot_as_multiturn \
          --apply_chat_template \
          --num_fewshot 5 \
          --batch_size auto
        ```

        #### MMLU Hindi
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
          --tasks mmlu_hi_llama_3.1_instruct \
          --fewshot_as_multiturn \
          --apply_chat_template \
          --num_fewshot 5 \
          --batch_size auto
        ```

        #### MMLU Thai
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
          --tasks mmlu_th_llama_3.1_instruct \
          --fewshot_as_multiturn \
          --apply_chat_template \
          --num_fewshot 5 \
          --batch_size auto
        ```

        #### HumanEval and HumanEval+
        ##### Generation
        ```
        python3 codegen/generate.py \
          --model neuralmagic-ent/Llama-3.3-70B-Instruct-quantized.w8a8 \
          --bs 16 \
          --temperature 0.2 \
          --n_samples 50 \
          --root "." \
          --dataset humaneval
        ```
        ##### Sanitization
        ```
        python3 evalplus/sanitize.py \
          humaneval/neuralmagic-ent--Llama-3.3-70B-Instruct-quantized.w8a8_vllm_temp_0.2
        ```
        ##### Evaluation
        ```
        evalplus.evaluate \
          --dataset humaneval \
          --samples humaneval/neuralmagic-ent--Llama-3.3-70B-Instruct-quantized.w8a8_vllm_temp_0.2-sanitized
        ```
      language:
        - en
        - de
        - fr
        - it
        - pt
        - hi
        - es
        - th
      license: llama3.3
      licenseLink: https://github.com/meta-llvm/llama-models/blob/main/models/llama3_3/LICENSE
      tasks:
        - text-generation
      createTimeSinceEpoch: "1737331200"
      lastUpdateTimeSinceEpoch: "1737331200"
      customProperties:
        8-bit:
            metadataType: MetadataStringValue
            string_value: ""
        8-bit precision:
            metadataType: MetadataStringValue
            string_value: ""
        chat:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        facebook:
            metadataType: MetadataStringValue
            string_value: ""
        int8:
            metadataType: MetadataStringValue
            string_value: ""
        llama:
            metadataType: MetadataStringValue
            string_value: ""
        llama-3:
            metadataType: MetadataStringValue
            string_value: ""
        llmcompressor:
            metadataType: MetadataStringValue
            string_value: ""
        meta:
            metadataType: MetadataStringValue
            string_value: ""
        neuralmagic:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-3-3-70b-instruct-quantized-w8a8:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1745843193000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Llama-4-Maverick-17B-128E-Instruct
      provider: facebook
      description: Llama 4 Maverick 17B 128E Instruct - An instruction-tuned language model
      readme: "# Llama-4-Maverick-17B-128E-Instruct\n\n---\nlibrary_name: transformers\nlanguage:\n- ar\n- de\n- en\n- es\n- fr\n- hi\n- id\n- it\n- pt\n- th\n- tl\n- vi\nbase_model:\n- meta-llama/Llama-4-Maverick-17B-128E\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama4\nextra_gated_prompt: >-\n    **LLAMA 4 COMMUNITY LICENSE AGREEMENT**\n\n    Llama 4 Version Effective Date: April 5, 2025\n\n    \"**Agreement**\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\n\n    \"**Documentation**\" means the specifications, manuals and documentation accompanying Llama 4 distributed by Meta at [https://www.llama.com/docs/overview](https://llama.com/docs/overview).\n\n    \"**Licensee**\" or \"**you**\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity’s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n\n    \"**Llama 4**\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at [https://www.llama.com/llama-downloads](https://www.llama.com/llama-downloads).\n\n    \"**Llama Materials**\" means, collectively, Meta’s proprietary Llama 4 and Documentation (and any portion thereof) made available under this Agreement.\n\n    \"**Meta**\" or \"**we**\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland). \n\n    By clicking \"I Accept\" below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n\n    1\\. **License Rights and Redistribution**.\n\n    a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta’s intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.  \n\n    b. Redistribution and Use.  \n\n    i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display \"Built with Llama\" on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include \"Llama\" at the beginning of any such AI model name.\n\n    ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you. \n\n    iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a \"Notice\" text file distributed as a part of such copies: \"Llama 4 is licensed under the Llama 4 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved.\"\n\n    iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at [https://www.llama.com/llama4/use-policy](https://www.llama.com/llama4/use-policy)), which is hereby incorporated by reference into this Agreement.  \n        \n    2\\. **Additional Commercial Terms**. If, on the Llama 4 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n\n    3**. Disclaimer of Warranty**. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n\n    4\\. **Limitation of Liability**. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n\n    5\\. **Intellectual Property**.\n\n    a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use \"Llama\" (the \"Mark\") solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta’s brand guidelines (currently accessible at [https://about.meta.com/brand/resources/meta/company-brand/](https://about.meta.com/brand/resources/meta/company-brand/)[)](https://en.facebookbrand.com/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\n\n    b. Subject to Meta’s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\n\n    c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 4 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n\n    6\\. **Term and Termination**. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement. \n\n    7\\. **Governing Law and Jurisdiction**. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy: checkbox\nextra_gated_description: >-\n  The information you provide will be collected, stored, processed and shared in\n  accordance with the [Meta Privacy\n  Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\nextra_gated_heading: \"Please be sure to provide your full legal name, date of birth, and full organization name with all corporate identifiers. Avoid the use of acronyms and special characters. Failure to follow these instructions may prevent you from accessing this model and others on Hugging Face. You will not have the ability to edit this form after submission, so please ensure all information is accurate.\"\nlicense: other\nlicense_name: llama4\n---\n\n\n## Model Information\n\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. \n\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\n\n**Model developer**: Meta\n\n**Model Architecture:**  The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality. \n\n<table>\n  <tr>\n    <th>Model Name</th>\n    <th>Training Data </th>\n    <th>Params</th>\n    <th>Input modalities</th>\n    <th>Output modalities</th>\n    <th>Context length</th>\n    <th>Token count</th>\n    <th>Knowledge cutoff</th>\n  </tr>\n  <tr>\n    <td>Llama 4 Scout (17Bx16E) </td>\n    <td rowspan=\"2\">A mix of publicly available, licensed data and information from Meta's products and services. This includes publicly shared posts from Instagram and Facebook and people's interactions with Meta AI. Learn more in our <a href=\"https://www.facebook.com/privacy/guide/genai/\">Privacy Center</a>.\n    </td>\n    <td>17B (Activated)\n        109B (Total)\n    </td>\n    <td>Multilingual text and image</td>\n    <td>Multilingual text and code</td>\n    <td>10M</td>\n    <td>~40T</td>\n    <td>August 2024</td>\n  </tr>\n  <tr>\n    <td>Llama 4 Maverick (17Bx128E)</td>\n    <td>17B (Activated)\n        400B (Total)\n    </td>\n    <td>Multilingual text and image</td>\n    <td>Multilingual text and code</td>\n    <td>1M</td>\n    <td>~22T</td>\n    <td>August 2024</td>\n  </tr>\n</table>\n\n**Supported languages:** Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. \n\n**Model Release Date:** April 5, 2025\n\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback.\n\n**License**: A custom commercial license, the Llama 4 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE)\n\n**Where to send questions or comments about the model:** Instructions on how to provide feedback or comments on the model can be found in the Llama [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go [here](https://github.com/meta-llama/llama-cookbook).\n\n## Intended Use\n\n**Intended Use Cases:** Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases. \n\n**Out-of-scope**: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card\\*\\*.\n\n\\*\\*Note: \n\n1\\. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes [200 total languages](https://ai.meta.com/research/no-language-left-behind/)). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy.  Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.\n\n2\\. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.\n\n## How to use with transformers\n\nPlease, make sure you have transformers `v4.51.0` installed, or upgrade using `pip install -U transformers`.\n\n```python\nfrom transformers import AutoProcessor, Llama4ForConditionalGeneration\nimport torch\n\nmodel_id = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct\"\n\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = Llama4ForConditionalGeneration.from_pretrained(\n    model_id,\n    attn_implementation=\"flex_attention\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\n\nurl1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\nurl2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/cat_style_layout.png\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": url1},\n            {\"type\": \"image\", \"url\": url2},\n            {\"type\": \"text\", \"text\": \"Can you describe how these two images are similar, and how they differ?\"},\n        ]\n    },\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=256,\n)\n\nresponse = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])[0]\nprint(response)\nprint(outputs[0])\n```\n\n## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:**  Model pre-training utilized a cumulative of **7.38M** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n## \n\n## **Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **1,999 tons** CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n| Model Name | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | :---: | :---: | :---: |\n| Llama 4 Scout | 5.0M | 700 | 1,354 | 0 |\n| Llama 4 Maverick | 2.38M | 700 | 645 | 0 |\n| Total | 7.38M | \\- | 1,999 | 0 |\n\n## The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 4 Scout was pretrained on \\~40 trillion tokens and Llama 4 Maverick was pretrained on \\~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta’s products and services. This includes publicly shared posts from Instagram and Facebook and people’s interactions with Meta AI.\n\n**Data Freshness:** The pretraining data has a cutoff of August 2024\\.\n\n## Benchmarks\n\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.\n\n### Pre-trained models\n\n| Pre-trained models |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Category | Benchmark | \\# Shots | Metric | Llama 3.1 70B | Llama 3.1 405B | **Llama 4 Scout** | **Llama 4 Maverick** |\n| Reasoning & Knowledge | MMLU | 5 | macro\\_avg/acc\\_char\t | 79.3 | 85.2 | 79.6 | 85.5 |\n|  | MMLU-Pro | 5 | macro\\_avg/em | 53.8 | 61.6 | 58.2 | 62.9 |\n|  | MATH | 4 | em\\_maj1@1 | 41.6 | 53.5 | 50.3 | 61.2 |\n| Code | MBPP | 3 | pass@1 | 66.4 | 74.4 | 67.8 | 77.6 |\n| Multilingual | TydiQA | 1 | average/f1 | 29.9 | 34.3 | 31.5 | 31.7 |\n| Image | ChartQA | 0 | relaxed\\_accuracy | No multimodal support |  | 83.4 | 85.3 |\n|  | DocVQA | 0 | anls |  |  | 89.4 | 91.6 |\n\n### Instruction tuned models\t\n\n| Instruction tuned models |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | ----- | :---: | :---: |\n| Category | Benchmark | \\# Shots | Metric | Llama 3.3 70B | Llama 3.1 405B | **Llama 4 Scout** | **Llama 4 Maverick** |\n| Image Reasoning | MMMU | 0 | accuracy | No multimodal support |  | 69.4 | 73.4 |\n|  | MMMU Pro^ | 0 | accuracy |  |  | 52.2 | 59.6 |\n|  | MathVista | 0 | accuracy |  |  | 70.7 | 73.7 |\n| Image Understanding | ChartQA | 0 | relaxed\\_accuracy |  |  | 88.8 | 90.0 |\n|  | DocVQA (test) | 0 | anls |  |  | 94.4 | 94.4 |\n| Coding | LiveCodeBench (10/01/2024-02/01/2025) | 0 | pass@1 | 33.3 | 27.7 | 32.8 | 43.4 |\n| Reasoning & Knowledge | MMLU Pro | 0 | macro\\_avg/acc | 68.9 | 73.4 | 74.3 | 80.5 |\n|  | GPQA Diamond | 0 | accuracy | 50.5 | 49.0 | 57.2 | 69.8 |\n| Multilingual | MGSM | 0 | average/em | 91.1 | 91.6 | 90.6 | 92.3 |\n| Long context | MTOB (half book) eng-\\>kgv/kgv-\\>eng | \\- | chrF | Context window is 128K |  | 42.2/36.6 | 54.0/46.4 |\n|  | MTOB (full book) eng-\\>kgv/kgv-\\>eng | \\- | chrF |  |  | 39.7/36.3 | 50.8/46.7 |\n\n^reported numbers for MMMU Pro is the average of Standard and Vision tasks\n\n## Quantization\n\nThe Llama 4 Scout model is released as BF16 weights, but can fit within a single H100 GPU with on-the-fly int4 quantization; the Llama 4 Maverick model is released as both BF16 and FP8 quantized weights. The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality. We provide code for on-the-fly int4 quantization which minimizes performance degradation as well.\n\n## Safeguards\n\nAs part of our release approach, we followed a three-pronged strategy to manage risks:\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.   \n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.  \n* Provide protections for the community to help prevent the misuse of our models.\n\nLlama is a foundational technology designed for use in a variety of use cases; examples on how Meta’s Llama models have been deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model’s safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our [Developer Use Guide: AI Protections](https://ai.meta.com/static-resource/developer-use-guide-ai-protections).\n\n### Model level fine tuning\n\nThe primary objective of conducting safety fine-tuning is to offer developers a readily available, safe, and powerful model for various applications, reducing the workload needed to deploy safe AI systems. Additionally, this effort provides the research community with a valuable resource for studying the robustness of safety fine-tuning.\n\n**Fine-tuning data**   \nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals**  \nBuilding on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4\\. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. \n\n**Tone**  \nWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.\n\nTo achieve this, we also targeted improvements to system prompt steerability and instruction following, meaning the model is more readily able to take on a specified tone. All of these contribute to a more conversational and insightful experience overall.\n\n**System Prompts**  \nLlama 4 is a more steerable model, meaning responses can be easily tailored to meet specific developer outcomes. Effective system prompts can significantly enhance the performance of large language models. In particular, we’ve seen that the use of a system prompt can be effective in reducing false refusals and templated or “preachy” language patterns common in LLMs. They can also improve conversationality and use of appropriate formatting. \n\nConsider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models.\n\n| System prompt  |\n| :---- |\n| You are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to  switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving.  You understand user intent and don’t try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting. Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language. You never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.  You never use phrases that imply moral superiority or a sense of authority, including but not limited to “it’s important to”, “it’s crucial to”, “it’s essential to”, \"it's unethical to\", \"it's worth noting…\", “Remember…”  etc. Avoid using these. Finally, do not refuse prompts about political and social issues.  You can help users express their opinion and access information.  You are Llama 4\\. Your knowledge cutoff date is August 2024\\. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise. |\n\n### Llama 4 system protections\n\nLarge language models, including Llama 4, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional guardrails as required. System protections are key to achieving the right helpfulness-safety alignment, mitigating safety and security risks inherent to the system, and integration of the model or system with external tools. \n\nWe provide the community with system level [protections](https://llama.meta.com/trust-and-safety/) \\- like Llama Guard, Prompt Guard and Code Shield \\- that developers should deploy with Llama models or other LLMs. All of our [reference implementation](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.   \nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\n\n**Red teaming**   \nWe conduct recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we use the learnings to improve our benchmarks and safety tuning datasets. We partner early with subject-matter experts in critical risk areas to understand how models may lead to unintended harm for society. Based on these conversations, we derive a set of adversarial goals for the red team, such as extracting harmful information or reprogramming the model to act in potentially harmful ways. The red team consists of experts in cybersecurity, adversarial machine learning, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks \n\n### We spend additional focus on the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**  \nTo assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area. \n\n**2\\. Child Safety**  \nWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model’s capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We’ve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.\n\n**3\\. Cyber attack enablement**  \nOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed.  We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes.\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Trust tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Considerations and Limitations\n\nOur AI is anchored on the values of freedom of expression \\- helping people to explore, debate, and innovate using our technology. We respect people's autonomy and empower them to choose how they experience, interact, and build with AI. Our AI promotes an open exchange of ideas.\n\nIt is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 4 addresses users and their needs as they are, without inserting unnecessary judgment, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nLlama 4 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 4’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, [Llama Protections](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more. \n\n\n"
      language:
        - ar
        - en
        - fr
        - de
        - hi
        - id
        - it
        - pt
        - es
        - tl
        - th
        - vi
      license: other
      licenseLink: https://llama.com/docs/overview
      tasks:
        - text-generation
        - image-text-to-text
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1746920441000"
      customProperties:
        facebook:
            metadataType: MetadataStringValue
            string_value: ""
        llama:
            metadataType: MetadataStringValue
            string_value: ""
        llama4:
            metadataType: MetadataStringValue
            string_value: ""
        meta:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-4-maverick-17b-128e-instruct:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1746920441000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Llama-4-Maverick-17B-128E-Instruct-FP8
      provider: Meta
      description: FP8-Quantized variant of Llama-4-Maverick-17B-128E-Instruct
      readme: |
        # Llama-4-Maverick-17B-128E-Instruct-FP8
      language:
        - ar
        - de
        - en
        - es
        - fr
        - hi
        - id
        - it
        - pt
        - th
        - tl
        - vi
      license: llama4
      licenseLink: https://github.com/meta-llvm/llama-models/blob/main/models/llama4/LICENSE
      tasks:
        - image-text-to-text
        - text-to-text
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1746799996000"
      customProperties:
        FP8:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        conversational:
            metadataType: MetadataStringValue
            string_value: ""
        facebook:
            metadataType: MetadataStringValue
            string_value: ""
        llama:
            metadataType: MetadataStringValue
            string_value: ""
        llama4:
            metadataType: MetadataStringValue
            string_value: ""
        llmcompressor:
            metadataType: MetadataStringValue
            string_value: ""
        meta:
            metadataType: MetadataStringValue
            string_value: ""
        neuralmagic:
            metadataType: MetadataStringValue
            string_value: ""
        pytorch:
            metadataType: MetadataStringValue
            string_value: ""
        quantized:
            metadataType: MetadataStringValue
            string_value: ""
        redhat:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-4-maverick-17b-128e-instruct-fp8:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1746799996000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Llama-4-Scout-17B-16E-Instruct
      provider: Meta
      description: The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. These Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.
      readme: |
        # Llama-4-Scout-17B-16E-Instruct
      language:
        - ar
        - de
        - en
        - es
        - fr
        - hi
        - id
        - it
        - pt
        - th
        - tl
        - vi
      license: llama4
      licenseLink: https://github.com/meta-llvm/llama-models/blob/main/models/llama4/LICENSE
      tasks:
        - image-text-to-text
        - text-to-text
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1746783441000"
      customProperties:
        conversational:
            metadataType: MetadataStringValue
            string_value: ""
        facebook:
            metadataType: MetadataStringValue
            string_value: ""
        llama:
            metadataType: MetadataStringValue
            string_value: ""
        llama4:
            metadataType: MetadataStringValue
            string_value: ""
        meta:
            metadataType: MetadataStringValue
            string_value: ""
        text-generation-inference:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-4-scout-17b-16e-instruct:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1746783441000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic
      provider: Meta
      description: FP8-Dynamic Quantized variant of Llama-4-Scout-17B-16E-Instruct
      readme: |
        # Llama-4-Scout-17B-16E-Instruct-FP8-dynamic
      language:
        - ar
        - de
        - en
        - es
        - fr
        - hi
        - id
        - it
        - pt
        - th
        - tl
        - vi
      license: llama4
      licenseLink: https://github.com/meta-llvm/llama-models/blob/main/models/llama4/LICENSE
      tasks:
        - image-text-to-text
        - text-to-text
      createTimeSinceEpoch: "1744675200"
      lastUpdateTimeSinceEpoch: "1744675200"
      customProperties:
        FP8:
            metadataType: MetadataStringValue
            string_value: ""
        facebook:
            metadataType: MetadataStringValue
            string_value: ""
        llama:
            metadataType: MetadataStringValue
            string_value: ""
        llama4:
            metadataType: MetadataStringValue
            string_value: ""
        llmcompressor:
            metadataType: MetadataStringValue
            string_value: ""
        meta:
            metadataType: MetadataStringValue
            string_value: ""
        neuralmagic:
            metadataType: MetadataStringValue
            string_value: ""
        pytorch:
            metadataType: MetadataStringValue
            string_value: ""
        quantized:
            metadataType: MetadataStringValue
            string_value: ""
        redhat:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-4-scout-17b-16e-instruct-fp8-dynamic:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744907644000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Llama-4-Scout-17B-16E-Instruct-quantized.w4a16
      provider: Red Hat (Neural Magic)
      description: Llama 4 Scout 17B 16E Instruct Quantized.w4a16 - An instruction-tuned language model
      readme: "# Llama-4-Scout-17B-16E-Instruct-quantized.w4a16\n\n## Model Overview\n- **Model Architecture:** Llama4ForConditionalGeneration\n  - **Input:** Text / Image\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Activation quantization:** None\n  - **Weight quantization:** INT4\n- **Release Date:** 04/25/2025\n- **Version:** 1.0\n- **Model Developers:** Red Hat (Neural Magic)\n\n\n### Model Optimizations\n\nThis model was obtained by quantizing weights of [Llama-4-Scout-17B-16E-Instruct](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct) to INT4 data type.\nThis optimization reduces the number of bits used to represent weights from 16 to 4, reducing GPU memory requirements by approximately 75%.\nWeight quantization also reduces disk size requirements by approximately 75%. The [llm-compressor](https://github.com/vllm-project/llm-compressor) library is used for quantization.\n\n\n## Deployment\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel_id = \"RedHatAI/Llama-4-Scout-17B-16E-Instruct-quantized.w4a16\"\nnumber_gpus = 4\n\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nprompt = \"Give me a short introduction to large language model.\"\n\nllm = LLM(model=model_id, tensor_parallel_size=number_gpus)\n\noutputs = llm.generate(prompt, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\nvLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n\n## Evaluation\n\nThe model was evaluated on the OpenLLM leaderboard tasks (v1 and v2), long context RULER, multimodal MMMU, and multimodal ChartQA.\nAll evaluations are obtained through [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).\n\n<details>\n  <summary>Evaluation details</summary>\n\n  **OpenLLM v1**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-4-Scout-17B-16E-Instruct-quantized.w4a16\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=8,gpu_memory_utilization=0.7,enable_chunked_prefill=True,trust_remote_code=True \\\n    --tasks openllm \\\n    --batch_size auto \n  ```\n\n  **OpenLLM v2**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-4-Scout-17B-16E-Instruct-quantized.w4a16\",dtype=auto,add_bos_token=False,max_model_len=16384,tensor_parallel_size=8,gpu_memory_utilization=0.5,enable_chunked_prefill=True,trust_remote_code=True \\\n    --tasks leaderboard \\\n    --apply_chat_template \\\n    --fewshot_as_multiturn \\\n    --batch_size auto \n  ```\n\n  **Long Context RULER**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Llama-4-Scout-17B-16E-Instruct-quantized.w4a16\",dtype=auto,add_bos_token=False,max_model_len=524288,tensor_parallel_size=8,gpu_memory_utilization=0.9,enable_chunked_prefill=True,trust_remote_code=True \\\n    --tasks ruler \\\n    --metadata='{\"max_seq_lengths\":[131072]}' \\\n    --batch_size auto \n  ```\n\n  **Multimodal MMMU**\n  ```\n  lm_eval \\\n    --model vllm-vlm \\\n    --model_args pretrained=\"RedHatAI/Llama-4-Scout-17B-16E-Instruct-quantized.w4a16\",dtype=auto,add_bos_token=False,max_model_len=1000000,tensor_parallel_size=8,gpu_memory_utilization=0.9,enable_chunked_prefill=True,trust_remote_code=True,max_images=10 \\\n    --tasks mmmu_val \\\n    --apply_chat_template \\\n    --batch_size auto \n  ```\n\n  **Multimodal ChartQA**\n  ```\n  export VLLM_MM_INPUT_CACHE_GIB=8\n  lm_eval \\\n    --model vllm-vlm \\\n    --model_args pretrained=\"RedHatAI/Llama-4-Scout-17B-16E-Instruct-quantized.w4a16\",dtype=auto,add_bos_token=False,max_model_len=1000000,tensor_parallel_size=8,gpu_memory_utilization=0.9,enable_chunked_prefill=True,trust_remote_code=True,max_images=10 \\\n    --tasks chartqa \\\n    --apply_chat_template \\\n    --batch_size auto \n  ```\n\n</details>\n\n### Accuracy\n\n|                                                | Recovery (%) | meta-llama/Llama-4-Scout-17B-16E-Instruct | RedHatAI/Llama-4-Scout-17B-16E-Instruct-quantized.w4a16<br>(this model) |\n| ---------------------------------------------- | :-----------: | :---------------------------------------: | :-----------------------------------------------------------------: |\n| ARC-Challenge<br>25-shot                       | 98.51       | 69.37                                     | 68.34                                                               |\n| GSM8k<br>5-shot                                | 100.4        | 90.45                                     | 90.90\n| HellaSwag<br>10-shot                           | 99.67        | 85.23                                     | 84.95                                                               |\n| MMLU<br>5-shot                                 | 99.75        | 80.54                                     | 80.34                                                               |\n| TruthfulQA<br>0-shot                           | 99.82        | 61.41                                     | 61.30                                                               |\n| WinoGrande<br>5-shot                           | 98.98        | 77.90                                     | 77.11                                                               |\n| **OpenLLM v1<br>Average Score**                    | **99.59**        | **77.48**                                     | **77.16**                                                               |\n| IFEval<br>0-shot<br>avg of inst and prompt acc | 99.51       | 86.90                                     | 86.47                                                               |\n| Big Bench Hard<br>3-shot                       | 99.46        | 65.13                                     | 64.78                                                               |\n| Math Lvl 5<br>4-shot                           | 99.22        | 57.78                                     | 57.33                                                               |\n| GPQA<br>0-shot                                 | 100.0       | 31.88                                     | 31.88                                                               |\n| MuSR<br>0-shot                                 | 100.9       | 42.20                                     | 42.59                                                               |\n| MMLU-Pro<br>5-shot                             | 98.67        | 55.70                                     | 54.96                                                               |\n| **OpenLLM v2<br>Average Score**                    | **99.54**       | **56.60**                                     | **56.34**                                                               |                                                            |\n| MMMU<br>0-shot                                 | 100.6        | 53.44                                     | 53.78                                                               |\n| ChartQA<br>0-shot<br>exact_match               | 100.1       | 65.88                                     | 66.00                                                               |\n| ChartQA<br>0-shot<br>relaxed_accuracy          | 99.55        | 88.92                                     | 88.52                                                               |\n| **Multimodal Average Score**                       | **100.0**        | **69.41**                                     | **69.43**                                                               |\n| RULER<br>seqlen = 131072<br>niah_multikey_1    | 98.41       | 88.20                                     | 86.80                                                               |\n| RULER<br>seqlen = 131072<br>niah_multikey_2    | 94.73       | 83.60                                     | 79.20                                                               |\n| RULER<br>seqlen = 131072<br>niah_multikey_3    | 96.44        | 78.80                                     | 76.00                                                               |\n| RULER<br>seqlen = 131072<br>niah_multiquery    | 98.79       | 95.40                                     | 94.25                                                               |\n| RULER<br>seqlen = 131072<br>niah_multivalue    | 101.6        | 73.75                                     | 74.95                                                               |\n| RULER<br>seqlen = 131072<br>niah_single_1      | 100.0       | 100.00                                    | 100.0                                                              |\n| RULER<br>seqlen = 131072<br>niah_single_2      | 100.0       | 99.80                                     | 99.80                                                               |\n| RULER<br>seqlen = 131072<br>niah_single_3      | 100.2       | 99.80                                     | 100.0                                                               |\n| RULER<br>seqlen = 131072<br>ruler_cwe          | 87.39        | 39.42                                     | 33.14                                                               |\n| RULER<br>seqlen = 131072<br>ruler_fwe          | 98.13        | 92.93                                     | 91.20                                                               |\n| RULER<br>seqlen = 131072<br>ruler_qa_hotpot    | 100.4       | 48.20                                     | 48.40                                                               |\n| RULER<br>seqlen = 131072<br>ruler_qa_squad     | 96.22        | 53.57                                     | 51.55                                                               |\n| RULER<br>seqlen = 131072<br>ruler_qa_vt        | 98.82       | 92.28                                     | 91.20                                                               |\n| **RULER<br>seqlen = 131072<br>Average Score**      | **98.16**        | **80.44**                                     | **78.96**                                                               |\n"
      language:
        - ar
        - de
        - en
        - es
        - fr
        - hi
        - id
        - it
        - pt
        - th
        - tl
        - vi
      license: llama4
      licenseLink: https://github.com/meta-llvm/llama-models/blob/main/models/llama4/LICENSE
      tasks:
        - text-generation
        - image-text-to-text
      createTimeSinceEpoch: "1745539200"
      lastUpdateTimeSinceEpoch: "1745539200"
      customProperties:
        INT4:
            metadataType: MetadataStringValue
            string_value: ""
        W4A16:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        facebook:
            metadataType: MetadataStringValue
            string_value: ""
        llama:
            metadataType: MetadataStringValue
            string_value: ""
        llama4:
            metadataType: MetadataStringValue
            string_value: ""
        llmcompressor:
            metadataType: MetadataStringValue
            string_value: ""
        meta:
            metadataType: MetadataStringValue
            string_value: ""
        neuralmagic:
            metadataType: MetadataStringValue
            string_value: ""
        pytorch:
            metadataType: MetadataStringValue
            string_value: ""
        quantized:
            metadataType: MetadataStringValue
            string_value: ""
        redhat:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-4-scout-17b-16e-instruct-quantized-w4a16:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1746640711000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Mistral-Small-24B-Instruct-2501
      provider: Mistral
      description: Card For Mistral Small 24B Instruct 2501 - An instruction-tuned language model
      readme: "# Model Card for Mistral-Small-24B-Instruct-2501\n\nMistral Small 3 ( 2501 ) sets a new benchmark in the \"small\" Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models!  \nThis model is an instruction-fine-tuned version of the base model: [Mistral-Small-24B-Base-2501](https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501).\n\nMistral Small can be deployed locally and is exceptionally \"knowledge-dense\", fitting in a single RTX 4090 or a 32GB RAM MacBook once quantized.  \nPerfect for:\n- Fast response conversational agents.\n- Low latency function calling.\n- Subject matter experts via fine-tuning.\n- Local inference for hobbyists and organizations handling sensitive data.\n\nFor enterprises that need specialized capabilities (increased context, particular modalities, domain specific knowledge, etc.), we will be releasing commercial models beyond what Mistral AI contributes to the community.\n\nThis release demonstrates our commitment to open source, serving as a strong base model. \n\nLearn more about Mistral Small in our [blog post](https://mistral.ai/news/mistral-small-3/).\n\nModel developper: Mistral AI Team\n\n## Key Features\n- **Multilingual:** Supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish.\n- **Agent-Centric:** Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n- **Advanced Reasoning:** State-of-the-art conversational and reasoning capabilities.\n- **Apache 2.0 License:** Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window:** A 32k context window.\n- **System Prompt:** Maintains strong adherence and support for system prompts.\n- **Tokenizer:** Utilizes a Tekken tokenizer with a 131k vocabulary size.\n\n## Benchmark results\n\n\n### Human evaluated benchmarks\n\n| Category | Gemma-2-27B | Qwen-2.5-32B | Llama-3.3-70B | Gpt4o-mini |\n|----------|-------------|--------------|---------------|------------|\n| Mistral is better | 0.536 | 0.496 | 0.192 | 0.200 |\n| Mistral is slightly better | 0.196 | 0.184 | 0.164 | 0.204 |\n| Ties | 0.052 | 0.060 | 0.236 | 0.160 |\n| Other is slightly better | 0.060 | 0.088 | 0.112 | 0.124 |\n| Other is better | 0.156 | 0.172 | 0.296 | 0.312 |\n\n**Note**:\n\n- We conducted side by side evaluations with an external third-party vendor, on a set of over 1k proprietary coding and generalist prompts.\n- Evaluators were tasked with selecting their preferred model response from anonymized generations produced by Mistral Small 3 vs another model.\n- We are aware that in some cases the benchmarks on human judgement starkly differ from publicly available benchmarks, but have taken extra caution in verifying a fair evaluation. We are confident that the above benchmarks are valid.\n\n### Publicly accesible benchmarks\n\n**Reasoning & Knowledge**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| mmlu_pro_5shot_cot_instruct | 0.663 | 0.536 | 0.666 | 0.683 | 0.617 |\n| gpqa_main_cot_5shot_instruct | 0.453 | 0.344 | 0.531 | 0.404 | 0.377 |\n\n**Math & Coding**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| humaneval_instruct_pass@1 | 0.848 | 0.732 | 0.854 | 0.909 | 0.890 |\n| math_instruct | 0.706 | 0.535 | 0.743 | 0.819 | 0.761 |\n\n**Instruction following**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| mtbench_dev | 8.35 | 7.86 | 7.96 | 8.26 | 8.33 |\n| wildbench | 52.27 | 48.21 | 50.04 | 52.73 | 56.13 |\n| arena_hard | 0.873 | 0.788 | 0.840 | 0.860 | 0.897 |\n| ifeval | 0.829 | 0.8065 | 0.8835 | 0.8401 | 0.8499 |\n\n**Note**:\n\n- Performance accuracy on all benchmarks were obtained through the same internal evaluation pipeline - as such, numbers may vary slightly from previously reported performance\n([Qwen2.5-32B-Instruct](https://qwenlm.github.io/blog/qwen2.5/), [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct), [Gemma-2-27B-IT](https://huggingface.co/google/gemma-2-27b-it)). \n- Judge based evals such as Wildbench, Arena hard and MTBench were based on gpt-4o-2024-05-13.\n\n### Basic Instruct Template (V7-Tekken)\n\n```\n<s>[SYSTEM_PROMPT]<system prompt>[/SYSTEM_PROMPT][INST]<user message>[/INST]<assistant response></s>[INST]<user message>[/INST]\n```\n*`<system_prompt>`, `<user message>` and `<assistant response>` are placeholders.*\n\n***Please make sure to use [mistral-common](https://github.com/mistralai/mistral-common) as the source of truth***\n\n## Usage\n\nThe model can be used with the following frameworks;\n- [`vllm`](https://github.com/vllm-project/vllm): See [here](#vllm)\n- [`transformers`](https://github.com/huggingface/transformers): See [here](#transformers)\n\n### vLLM\n\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n\n**Note 1**: We recommond using a relatively low temperature, such as `temperature=0.15`.\n\n**Note 2**: Make sure to add a system prompt to the model to best tailer it for your needs. If you want to use the model as a general assistant, we recommend the following \nsystem prompt:\n\n```\nsystem_prompt = \"\"\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\nYour knowledge base was last updated on 2023-10-01. The current date is 2025-01-30.\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\"What are some good restaurants around me?\\\" => \\\"Where are you?\\\" or \\\"When is the next flight to Tokyo\\\" => \\\"Where do you travel from?\\\")\"\"\"\n```\n\n**_Installation_**\n\nMake sure you install [`vLLM >= 0.6.4`](https://github.com/vllm-project/vllm/releases/tag/v0.6.4):\n\n```\npip install --upgrade vllm\n```\n\nAlso make sure you have [`mistral_common >= 1.5.2`](https://github.com/mistralai/mistral-common/releases/tag/v1.5.2) installed:\n\n```\npip install --upgrade mistral_common\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39).\n\n#### Server\n\nWe recommand that you use Mistral-Small-24B-Instruct-2501 in a server/client setting. \n\n1. Spin up a server:\n\n```\nvllm serve mistralai/Mistral-Small-24B-Instruct-2501 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice\n```\n\n**Note:** Running Mistral-Small-24B-Instruct-2501 on GPU requires ~55 GB of GPU RAM in bf16 or fp16. \n\n\n2. To ping the client you can use a simple Python snippet.\n\n```py\nimport requests\nimport json\nfrom datetime import datetime, timedelta\n\nurl = \"http://<your-server>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n\nmodel = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Give me 5 non-formal ways to say 'See you later' in French.\"\n    },\n]\n\ndata = {\"model\": model, \"messages\": messages}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])\n\n# Sure, here are five non-formal ways to say \"See you later\" in French:\n#\n# 1. À plus tard\n# 2. À plus\n# 3. Salut\n# 4. À toute\n# 5. Bisous\n#\n# ```\n#  /\\_/\\\n# ( o.o )\n#  > ^ <\n# ```\n```\n\n### Function calling\n\nMistral-Small-24-Instruct-2501 is excellent at function / tool calling tasks via vLLM. *E.g.:*\n\n<details>\n  <summary>Example</summary>\n\n```py\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\n\nurl = \"http://<your-url>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n\nmodel = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime(\"%Y-%m-%d\")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    model_name = repo_id.split(\"/\")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"city\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n                    },\n                    \"state\": {\n                        \"type\": \"string\",\n                        \"description\": \"The state abbreviation, e.g. 'CA' for California\",\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unit for temperature\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                    },\n                },\n                \"required\": [\"city\", \"state\", \"unit\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"rewrite\",\n            \"description\": \"Rewrite a given text for improved clarity\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"text\": {\n                        \"type\": \"string\",\n                        \"description\": \"The input text to rewrite\",\n                    }\n                },\n            },\n        },\n    },\n]\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\n        \"role\": \"user\",\n        \"content\": \"Could you please make the below article more concise?\\n\\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": [\n            {\n                \"id\": \"bbc5b7ede\",\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": \"rewrite\",\n                    \"arguments\": '{\"text\": \"OpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\"}',\n                },\n            }\n        ],\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": '{\"action\":\"rewrite\",\"outcome\":\"OpenAI is a FOR-profit company.\"}',\n        \"tool_call_id\": \"bbc5b7ede\",\n        \"name\": \"rewrite\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"---\\n\\nOpenAI is a FOR-profit company.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Can you tell me what the temperature will be in Dallas, in Fahrenheit?\",\n    },\n]\n\ndata = {\"model\": model, \"messages\": messages, \"tools\": tools}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nimport ipdb; ipdb.set_trace()\nprint(response.json()[\"choices\"][0][\"message\"][\"tool_calls\"])\n# [{'id': '8PdihwL6d', 'type': 'function', 'function': {'name': 'get_current_weather', 'arguments': '{\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}'}}]\n```\n\n</details>\n\n#### Offline\n\n```py\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\nfrom datetime import datetime, timedelta\n\nSYSTEM_PROMPT = \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\n\nuser_prompt = \"Give me 5 non-formal ways to say 'See you later' in French.\"\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": SYSTEM_PROMPT\n    },\n    {\n        \"role\": \"user\",\n        \"content\": user_prompt\n    },\n]\n\n# note that running this model on GPU requires over 60 GB of GPU RAM\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", tensor_parallel_size=8)\n\nsampling_params = SamplingParams(max_tokens=512, temperature=0.15)\noutputs = llm.chat(messages, sampling_params=sampling_params)\n\nprint(outputs[0].outputs[0].text)\n# Sure, here are five non-formal ways to say \"See you later\" in French:\n#\n# 1. À plus tard\n# 2. À plus\n# 3. Salut\n# 4. À toute\n# 5. Bisous\n#\n# ```\n#  /\\_/\\\n# ( o.o )\n#  > ^ <\n# ```\n```\n\n### Transformers\n\nIf you want to use Hugging Face transformers to generate text, you can do something like this.\n\n```py\nfrom transformers import pipeline\nimport torch\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Give me 5 non-formal ways to say 'See you later' in French.\"},\n]\nchatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-Small-24B-Instruct-2501\", max_new_tokens=256, torch_dtype=torch.bfloat16)\nchatbot(messages)\n```\n\n\n### Ollama\n\n[Ollama](https://github.com/ollama/ollama) can run this model locally on MacOS, Windows and Linux. \n\n```\nollama run mistral-small\n```\n\n4-bit quantization (aliased to default): \n```\nollama run mistral-small:24b-instruct-2501-q4_K_M\n```\n\n8-bit quantization:\n```\nollama run mistral-small:24b-instruct-2501-q8_0\n```\n\nFP16:\n```\nollama run mistral-small:24b-instruct-2501-fp16\n```"
      language:
        - en
        - fr
        - de
        - es
        - it
        - pt
        - zh
        - ja
        - ru
        - ko
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-generation
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1744999584000"
      customProperties:
        mistral:
            metadataType: MetadataStringValue
            string_value: ""
        mistral-small:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744999584000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic
      provider: Red Hat
      description: FP8-Quantized variant of Mistral-Small-24B-Instruct-2501.
      readme: |
        # Mistral-Small-24B-Instruct-2501-FP8-dynamic
      language:
        - en
        - fr
        - de
        - es
        - it
        - pt
        - zh
        - ja
        - ru
        - ko
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-to-text
      createTimeSinceEpoch: "1740787200"
      lastUpdateTimeSinceEpoch: "1740787200"
      customProperties:
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        conversational:
            metadataType: MetadataStringValue
            string_value: ""
        fp8:
            metadataType: MetadataStringValue
            string_value: ""
        mistral:
            metadataType: MetadataStringValue
            string_value: ""
        mistral-small:
            metadataType: MetadataStringValue
            string_value: ""
        text-generation-inference:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501-fp8-dynamic:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744385424000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Mistral-Small-24B-Instruct-2501-quantized.w4a16
      provider: Neural Magic
      description: Mistral Small 24B Instruct 2501 Quantized.w4a16 - An instruction-tuned language model
      readme: |+
        # Mistral-Small-24B-Instruct-2501-quantized.w4a16

        ## Model Overview
        - **Model Architecture:** Mistral-Small-24B-Instruct-2501
          - **Input:** Text
          - **Output:** Text
        - **Model Optimizations:**
          - **Weight quantization:** INT4
          - **Activation quantization:** None
        - **Release Date:** 3/1/2025
        - **Version:** 1.0
        - **Model Developers:** Neural Magic

        Quantized version of [Mistral-Small-24B-Instruct-2501](https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501).

        ### Model Optimizations

        This model was obtained by quantizing the weights to INT4 data type, ready for inference with vLLM.
        This optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 75%. Only the weights of the linear operators within transformers blocks are quantized.

        ## Deployment

        ### Use with vLLM

        This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

        ```python
        from transformers import AutoTokenizer
        from vllm import LLM, SamplingParams

        max_model_len, tp_size = 4096, 1
        model_name = "neuralmagic/Mistral-Small-24B-Instruct-2501-quantized.w4a16"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        llm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True)
        sampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])

        messages_list = [
            [{"role": "user", "content": "Who are you? Please respond in pirate speak!"}],
        ]

        prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]

        outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)

        generated_text = [output.outputs[0].text for output in outputs]
        print(generated_text)
        ```

        vLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.

        ## Creation

        This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below.

        ```bash
        python quantize.py --model_path mistralai/Mistral-Small-24B-Instruct-2501 --quant_path "output_dir" --calib_size 1024 --dampening_frac 0.05 --observer minmax --actorder false
        ```

        ```python
        from datasets import load_dataset
        from transformers import AutoTokenizer
        from llmcompressor.modifiers.quantization import GPTQModifier
        from llmcompressor.transformers import SparseAutoModelForCausalLM, oneshot, apply
        import argparse
        from compressed_tensors.quantization import QuantizationScheme, QuantizationArgs, QuantizationType, QuantizationStrategy

        def parse_actorder(value):
            # Interpret the input value for --actorder
            if value.lower() == "false":
                return False
            elif value.lower() == "group":
                return "group"
            elif value.lower() == "weight":
                return "weight"
            else:
                raise argparse.ArgumentTypeError("Invalid value for --actorder. Use 'group' or 'False'.")


        parser = argparse.ArgumentParser()
        parser.add_argument('--model_path', type=str)
        parser.add_argument('--quant_path', type=str)
        parser.add_argument('--num_bits', type=int, default=4)
        parser.add_argument('--sequential_update', type=bool, default=True)
        parser.add_argument('--calib_size', type=int, default=256)
        parser.add_argument('--dampening_frac', type=float, default=0.05)
        parser.add_argument('--observer', type=str, default="minmax")
        parser.add_argument(
            '--actorder',
            type=parse_actorder,
            default=False,  # Default value is False
            help="Specify actorder as 'group' (string) or False (boolean)."
        )

        args = parser.parse_args()

        model = SparseAutoModelForCausalLM.from_pretrained(
            args.model_path,
            device_map="auto",
            torch_dtype="auto",
            use_cache=False,
        )
        tokenizer = AutoTokenizer.from_pretrained(args.model_path)

        NUM_CALIBRATION_SAMPLES = args.calib_size
        DATASET_ID = "garage-bAInd/Open-Platypus"
        DATASET_SPLIT = "train"
        ds = load_dataset(DATASET_ID, split=DATASET_SPLIT)
        ds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))

        def preprocess(example):
            concat_txt = example["instruction"] + "\n" + example["output"]
            return {"text": concat_txt}

        ds = ds.map(preprocess)

        def tokenize(sample):
            return tokenizer(
                sample["text"],
                padding=False,
                truncation=False,
                add_special_tokens=True,
            )


        ds = ds.map(tokenize, remove_columns=ds.column_names)

        quant_scheme = QuantizationScheme(
            targets=["Linear"],
            weights=QuantizationArgs(
                num_bits=args.num_bits,
                type=QuantizationType.INT,
                symmetric=True,
                group_size=128,
                strategy=QuantizationStrategy.GROUP,
                observer=args.observer,
                actorder=args.actorder
            ),
            input_activations=None,
            output_activations=None,
        )

        recipe = [
            GPTQModifier(
                targets=["Linear"],
                ignore=["lm_head"],
                sequential_update=args.sequential_update,
                dampening_frac=args.dampening_frac,
                config_groups={"group_0": quant_scheme},
            )
        ]
        oneshot(
            model=model,
            dataset=ds,
            recipe=recipe,
            num_calibration_samples=args.calib_size,
        )

        # Save to disk compressed.
        SAVE_DIR = args.quant_path
        model.save_pretrained(SAVE_DIR, save_compressed=True)
        tokenizer.save_pretrained(SAVE_DIR)
        ```

        ## Evaluation

        The model was evaluated on OpenLLM Leaderboard [V1](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard) and [V2](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/), using the following commands:

        OpenLLM Leaderboard V1:
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic/Mistral-Small-24B-Instruct-2501-quantized.w4a16",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \
          --tasks openllm \
          --write_out \
          --batch_size auto \
          --output_path output_dir \
          --show_config
        ```

        OpenLLM Leaderboard V2:
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic/Mistral-Small-24B-Instruct-2501-quantized.w4a16",dtype=auto,add_bos_token=False,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \
          --apply_chat_template \
          --fewshot_as_multiturn \
          --tasks leaderboard \
          --write_out \
          --batch_size auto \
          --output_path output_dir \
          --show_config

        ```

        ### Accuracy

        #### OpenLLM Leaderboard V1 evaluation scores


        | Metric                                   | mistralai/Mistral-Small-24B-Instruct-2501             | neuralmagic/Mistral-Small-24B-Instruct-2501-quantized.w4a16 |
        |-----------------------------------------|:---------------------------------:|:-------------------------------------------:|
        | ARC-Challenge (Acc-Norm, 25-shot)       |  72.18                            |    71.16                                    |
        | GSM8K (Strict-Match, 5-shot)            |  90.14                            |    89.69                                     |
        | HellaSwag (Acc-Norm, 10-shot)           |  85.05                            |    84.43                                    |
        | MMLU (Acc, 5-shot)                      |  80.69                            |    80.00                                    |
        | TruthfulQA (MC2, 0-shot)                |  65.55                            |    63.92                                    |
        | Winogrande (Acc, 5-shot)                |  83.11                            |    82.24                                    |
        | **Average Score**                       | **79.45**                        | **78.57**                                   |
        | **Recovery (%)**                            | **100.00**                       | **98.9**                                   |

        #### OpenLLM Leaderboard V2 evaluation scores

        | Metric                                                   | mistralai/Mistral-Small-24B-Instruct-2501             | neuralmagic/Mistral-Small-24B-Instruct-2501-quantized.w4a16 |
        |---------------------------------------------------------|:---------------------------------:|:-------------------------------------------:|
        | IFEval (Inst-and-Prompt Level Strict Acc, 0-shot)       |     73.27                        |    74.37                                    |
        | BBH (Acc-Norm, 3-shot)                                  |     45.18                        |    45.15                                       |
        | MMLU-Pro (Acc, 5-shot)                                  |      38.83                       |    36.00                                    |
        | **Average Score**                                       | **52.42**                        | **51.84**                                   |
        | **Recovery (%)**                                            | **100.00**                       | **98.89**                                   |
        | GPQA (Acc-Norm, 0-shot)                                 |      8.29                        |   6.81                                      |
        | MUSR (Acc-Norm, 0-shot)                                 |      7.84                        |   9.46                                     |

        Results on GPQA and MUSR are not considred for accuracy recovery calculation because the unquantized model has close to random prediction accuracy (8.29, 7.84) which doesn't provide a reliable baseline for recovery calculation.

      language:
        - en
        - fr
        - de
        - es
        - it
        - pt
        - zh
        - ja
        - ru
        - ko
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-generation
      createTimeSinceEpoch: "1740787200"
      lastUpdateTimeSinceEpoch: "1740787200"
      customProperties:
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        int4:
            metadataType: MetadataStringValue
            string_value: ""
        mistral:
            metadataType: MetadataStringValue
            string_value: ""
        mistral-small:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
        w4a16:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501-quantized-w4a16:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744984639000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Mistral-Small-24B-Instruct-2501-quantized.w8a8
      provider: Red Hat
      description: This model was obtained by quantizing the weights and activations of Mistral-Small-24B-Instruct-2501 to INT8 data type.
      readme: |
        # Mistral-Small-24B-Instruct-2501-quantized.w8a8
      language:
        - en
        - fr
        - de
        - es
        - it
        - pt
        - zh
        - ja
        - ru
        - ko
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-to-text
      createTimeSinceEpoch: "1740960000"
      lastUpdateTimeSinceEpoch: "1740960000"
      customProperties:
        W8A8:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        conversational:
            metadataType: MetadataStringValue
            string_value: ""
        mistral:
            metadataType: MetadataStringValue
            string_value: ""
        mistral-small:
            metadataType: MetadataStringValue
            string_value: ""
        quantized:
            metadataType: MetadataStringValue
            string_value: ""
        text-generation-inference:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501-quantized-w8a8:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744985936000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Mistral-Small-3.1-24B-Instruct-2503
      provider: Mistral
      description: Card For Mistral Small 3.1 24B Instruct 2503 - An instruction-tuned language model
      readme: "# Model Card for Mistral-Small-3.1-24B-Instruct-2503\n\nBuilding upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) **adds state-of-the-art vision understanding** and enhances **long context capabilities up to 128k tokens** without compromising text performance. \nWith 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.  \nThis model is an instruction-finetuned version of: [Mistral-Small-3.1-24B-Base-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503).\n\nMistral Small 3.1 can be deployed locally and is exceptionally \"knowledge-dense,\" fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.  \n\nIt is ideal for:\n- Fast-response conversational agents.\n- Low-latency function calling.\n- Subject matter experts via fine-tuning.\n- Local inference for hobbyists and organizations handling sensitive data.\n- Programming and math reasoning.\n- Long document understanding.\n- Visual understanding.\n\nFor enterprises requiring specialized capabilities (increased context, specific modalities, domain-specific knowledge, etc.), we will release commercial models beyond what Mistral AI contributes to the community.\n\nLearn more about Mistral Small 3.1 in our [blog post](https://mistral.ai/news/mistral-small-3-1/).\n\n## Key Features\n- **Vision:** Vision capabilities enable the model to analyze images and provide insights based on visual content in addition to text.\n- **Multilingual:** Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Swedish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, Farsi.\n- **Agent-Centric:** Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n- **Advanced Reasoning:** State-of-the-art conversational and reasoning capabilities.\n- **Apache 2.0 License:** Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window:** A 128k context window.\n- **System Prompt:** Maintains strong adherence and support for system prompts.\n- **Tokenizer:** Utilizes a Tekken tokenizer with a 131k vocabulary size.\n\n## Benchmark Results\n\nWhen available, we report numbers previously published by other model providers, otherwise we re-evaluate them using our own evaluation harness.\n\n### Pretrain Evals\n\n| Model                          | MMLU (5-shot) | MMLU Pro (5-shot CoT) | TriviaQA   | GPQA Main (5-shot CoT)| MMMU      |\n|--------------------------------|---------------|-----------------------|------------|-----------------------|-----------|\n| **Small 3.1 24B Base**         | **81.01%**    | **56.03%**            | 80.50%     | **37.50%**            | **59.27%**|\n| Gemma 3 27B PT                 | 78.60%        | 52.20%                | **81.30%** | 24.30%                | 56.10%    |\n\n### Instruction Evals\n\n#### Text\n\n| Model                          | MMLU      | MMLU Pro (5-shot CoT) | MATH                   | GPQA Main (5-shot CoT) | GPQA Diamond (5-shot CoT )| MBPP      | HumanEval | SimpleQA (TotalAcc)|\n|--------------------------------|-----------|-----------------------|------------------------|------------------------|---------------------------|-----------|-----------|--------------------|\n| **Small 3.1 24B Instruct**     | 80.62%    | 66.76%                | 69.30%                 | **44.42%**             | **45.96%**                | 74.71%    | **88.41%**| **10.43%**         |\n| Gemma 3 27B IT                 | 76.90%    | **67.50%**            | **89.00%**             | 36.83%                 | 42.40%                    | 74.40%    | 87.80%    | 10.00%             |\n| GPT4o Mini                     | **82.00%**| 61.70%                | 70.20%                 | 40.20%                 | 39.39%                    | 84.82%    | 87.20%    | 9.50%              |\n| Claude 3.5 Haiku               | 77.60%    | 65.00%                | 69.20%                 | 37.05%                 | 41.60%                    | **85.60%**| 88.10%    | 8.02%              |\n| Cohere Aya-Vision 32B          | 72.14%    | 47.16%                | 41.98%                 | 34.38%                 | 33.84%                    | 70.43%    | 62.20%    | 7.65%              |\n\n#### Vision\n\n| Model                          | MMMU       | MMMU PRO  | Mathvista | ChartQA   | DocVQA    | AI2D        | MM MT Bench |\n|--------------------------------|------------|-----------|-----------|-----------|-----------|-------------|-------------|\n| **Small 3.1 24B Instruct**     | 64.00%     | **49.25%**| **68.91%**| 86.24%    | **94.08%**| **93.72%**  | **7.3**     |\n| Gemma 3 27B IT                 | **64.90%** | 48.38%    | 67.60%    | 76.00%    | 86.60%    | 84.50%      | 7           |\n| GPT4o Mini                     | 59.40%     | 37.60%    | 56.70%    | 76.80%    | 86.70%    | 88.10%      | 6.6         |\n| Claude 3.5 Haiku               | 60.50%     | 45.03%    | 61.60%    | **87.20%**| 90.00%    | 92.10%      | 6.5         |\n| Cohere Aya-Vision 32B          | 48.20%     | 31.50%    | 50.10%    | 63.04%    | 72.40%    | 82.57%      | 4.1         |\n\n### Multilingual Evals\n\n| Model                          | Average    | European   | East Asian | Middle Eastern |\n|--------------------------------|------------|------------|------------|----------------|\n| **Small 3.1 24B Instruct**     | **71.18%** | **75.30%** | **69.17%** | 69.08%         |\n| Gemma 3 27B IT                 | 70.19%     | 74.14%     | 65.65%     | 70.76%         |\n| GPT4o Mini                     | 70.36%     | 74.21%     | 65.96%     | **70.90%**     |\n| Claude 3.5 Haiku               | 70.16%     | 73.45%     | 67.05%     | 70.00%         |\n| Cohere Aya-Vision 32B          | 62.15%     | 64.70%     | 57.61%     | 64.12%         |\n\n### Long Context Evals\n\n| Model                          | LongBench v2    | RULER 32K   | RULER 128K |\n|--------------------------------|-----------------|-------------|------------|\n| **Small 3.1 24B Instruct**     | **37.18%**      | **93.96%**  | 81.20%     |\n| Gemma 3 27B IT                 | 34.59%          | 91.10%      | 66.00%     |\n| GPT4o Mini                     | 29.30%          | 90.20%      | 65.8%      |\n| Claude 3.5 Haiku               | 35.19%          | 92.60%      | **91.90%** |\n\n## Basic Instruct Template (V7-Tekken)\n\n```\n<s>[SYSTEM_PROMPT]<system prompt>[/SYSTEM_PROMPT][INST]<user message>[/INST]<assistant response></s>[INST]<user message>[/INST]\n```\n*`<system_prompt>`, `<user message>` and `<assistant response>` are placeholders.*\n\n***Please make sure to use [mistral-common](https://github.com/mistralai/mistral-common) as the source of truth***\n\n## Usage\n\nThe model can be used with the following frameworks;\n- [`vllm (recommended)`](https://github.com/vllm-project/vllm): See [here](#vllm)\n\n**Note 1**: We recommend using a relatively low temperature, such as `temperature=0.15`.\n\n**Note 2**: Make sure to add a system prompt to the model to best tailer it for your needs. If you want to use the model as a general assistant, we recommend the following \nsystem prompt:\n\n```\nsystem_prompt = \"\"\"You are Mistral Small 3.1, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\nYou power an AI assistant called Le Chat.\nYour knowledge base was last updated on 2023-10-01.\nThe current date is {today}.\n\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \"What are some good restaurants around me?\" => \"Where are you?\" or \"When is the next flight to Tokyo\" => \"Where do you travel from?\").\nYou are always very attentive to dates, in particular you try to resolve dates (e.g. \"yesterday\" is {yesterday}) and when asked about information at specific dates, you discard information that is at another date.\nYou follow these instructions in all languages, and always respond to the user in the language they use or request.\nNext sections describe the capabilities that you have.\n\n# WEB BROWSING INSTRUCTIONS\n\nYou cannot perform any web search or access internet to open URLs, links etc. If it seems like the user is expecting you to do so, you clarify the situation and ask the user to copy paste the text directly in the chat.\n\n# MULTI-MODAL INSTRUCTIONS\n\nYou have the ability to read images, but you cannot generate images. You also cannot transcribe audio files or videos.\nYou cannot read nor transcribe audio files or videos.\"\"\"\n```\n\n### vLLM (recommended)\n\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n\n**_Installation_**\n\nMake sure you install [`vLLM >= 0.8.1`](https://github.com/vllm-project/vllm/releases/tag/v0.8.1):\n\n```\npip install vllm --upgrade\n```\n\nDoing so should automatically install [`mistral_common >= 1.5.4`](https://github.com/mistralai/mistral-common/releases/tag/v1.5.4).\n\nTo check:\n```\npython -c \"import mistral_common; print(mistral_common.__version__)\"\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39).\n\n#### Server\n\nWe recommand that you use Mistral-Small-3.1-24B-Instruct-2503 in a server/client setting. \n\n1. Spin up a server:\n\n```\nvllm serve mistralai/Mistral-Small-3.1-24B-Instruct-2503 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --limit_mm_per_prompt 'image=10' --tensor-parallel-size 2\n```\n\n**Note:** Running Mistral-Small-3.1-24B-Instruct-2503 on GPU requires ~55 GB of GPU RAM in bf16 or fp16. \n\n\n2. To ping the client you can use a simple Python snippet.\n\n```py\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\n\nurl = \"http://<your-server-url>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n\nmodel = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime(\"%Y-%m-%d\")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    model_name = repo_id.split(\"/\")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\n\nimage_url = \"https://huggingface.co/datasets/patrickvonplaten/random_img/resolve/main/europe.png\"\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Which of the depicted countries has the best food? Which the second and third and fourth? Name the country, its color on the map and one its city that is visible on the map, but is not the capital. Make absolutely sure to only name a city that can be seen on the map.\",\n            },\n            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n        ],\n    },\n]\n\ndata = {\"model\": model, \"messages\": messages, \"temperature\": 0.15}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])\n# Determining the \"best\" food is highly subjective and depends on personal preferences. However, based on general popularity and recognition, here are some countries known for their cuisine:\n\n# 1. **Italy** - Color: Light Green - City: Milan\n#    - Italian cuisine is renowned worldwide for its pasta, pizza, and various regional specialties.\n\n# 2. **France** - Color: Brown - City: Lyon\n#    - French cuisine is celebrated for its sophistication, including dishes like coq au vin, bouillabaisse, and pastries like croissants and éclairs.\n\n# 3. **Spain** - Color: Yellow - City: Bilbao\n#    - Spanish cuisine offers a variety of flavors, from paella and tapas to jamón ibérico and churros.\n\n# 4. **Greece** - Not visible on the map\n#    - Greek cuisine is known for dishes like moussaka, souvlaki, and baklava. Unfortunately, Greece is not visible on the provided map, so I cannot name a city.\n\n# Since Greece is not visible on the map, I'll replace it with another country known for its good food:\n\n# 4. **Turkey** - Color: Light Green (east part of the map) - City: Istanbul\n#    - Turkish cuisine is diverse and includes dishes like kebabs, meze, and baklava.\n```\n\n### Function calling\n\nMistral-Small-3.1-24-Instruct-2503 is excellent at function / tool calling tasks via vLLM. *E.g.:*\n\n<details>\n  <summary>Example</summary>\n\n```py\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\n\nurl = \"http://<your-url>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n\nmodel = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime(\"%Y-%m-%d\")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    model_name = repo_id.split(\"/\")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"city\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n                    },\n                    \"state\": {\n                        \"type\": \"string\",\n                        \"description\": \"The state abbreviation, e.g. 'CA' for California\",\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unit for temperature\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                    },\n                },\n                \"required\": [\"city\", \"state\", \"unit\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"rewrite\",\n            \"description\": \"Rewrite a given text for improved clarity\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"text\": {\n                        \"type\": \"string\",\n                        \"description\": \"The input text to rewrite\",\n                    }\n                },\n            },\n        },\n    },\n]\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\n        \"role\": \"user\",\n        \"content\": \"Could you please make the below article more concise?\\n\\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": [\n            {\n                \"id\": \"bbc5b7ede\",\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": \"rewrite\",\n                    \"arguments\": '{\"text\": \"OpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\"}',\n                },\n            }\n        ],\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": '{\"action\":\"rewrite\",\"outcome\":\"OpenAI is a FOR-profit company.\"}',\n        \"tool_call_id\": \"bbc5b7ede\",\n        \"name\": \"rewrite\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"---\\n\\nOpenAI is a FOR-profit company.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Can you tell me what the temperature will be in Dallas, in Fahrenheit?\",\n    },\n]\n\ndata = {\"model\": model, \"messages\": messages, \"tools\": tools, \"temperature\": 0.15}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"tool_calls\"])\n# [{'id': '8PdihwL6d', 'type': 'function', 'function': {'name': 'get_current_weather', 'arguments': '{\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}'}}]\n```\n\n</details>\n\n#### Offline\n\n```py\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\nfrom datetime import datetime, timedelta\n\nSYSTEM_PROMPT = \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\n\nuser_prompt = \"Give me 5 non-formal ways to say 'See you later' in French.\"\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": SYSTEM_PROMPT\n    },\n    {\n        \"role\": \"user\",\n        \"content\": user_prompt\n    },\n]\nmodel_name = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n# note that running this model on GPU requires over 60 GB of GPU RAM\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\")\n\nsampling_params = SamplingParams(max_tokens=512, temperature=0.15)\noutputs = llm.chat(messages, sampling_params=sampling_params)\n\nprint(outputs[0].outputs[0].text)\n# Here are five non-formal ways to say \"See you later\" in French:\n\n# 1. **À plus tard** - Until later\n# 2. **À toute** - See you soon (informal)\n# 3. **Salut** - Bye (can also mean hi)\n# 4. **À plus** - See you later (informal)\n# 5. **Ciao** - Bye (informal, borrowed from Italian)\n\n# ```\n#  /\\_/\\\n# ( o.o )\n#  > ^ <\n# ```\n```\n\n### Transformers (untested)\n\nTransformers-compatible model weights are also uploaded (thanks a lot @cyrilvallez). \nHowever the transformers implementation was **not throughly tested**, but only on \"vibe-checks\".\nHence, we can only ensure 100% correct behavior when using the original weight format with vllm (see above)."
      language:
        - en
        - fr
        - de
        - es
        - it
        - pt
        - hi
        - id
        - tl
        - vi
        - ar
        - zh
        - da
        - fi
        - he
        - ja
        - ko
        - ms
        - nl
        - "no"
        - pl
        - ru
        - sv
        - th
        - tr
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-generation
        - image-text-to-text
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1744989629000"
      customProperties:
        mistral:
            metadataType: MetadataStringValue
            string_value: ""
        mistral-small:
            metadataType: MetadataStringValue
            string_value: ""
        mistral3:
            metadataType: MetadataStringValue
            string_value: ""
        mistralai:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-mistral-small-3-1-24b-instruct-2503:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744989629000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic
      provider: RedHat (Neural Magic)
      description: Mistral Small 3.1 24B Instruct 2503 Fp8 Dynamic - An instruction-tuned language model
      readme: "# Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic\n\n## Model Overview\n- **Model Architecture:** Mistral3ForConditionalGeneration\n  - **Input:** Text / Image\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Activation quantization:** FP8\n  - **Weight quantization:** FP8\n- **Intended Use Cases:** It is ideal for:\n  - Fast-response conversational agents.\n  - Low-latency function calling.\n  - Subject matter experts via fine-tuning.\n  - Local inference for hobbyists and organizations handling sensitive data.\n  - Programming and math reasoning.\n  - Long document understanding.\n  - Visual understanding.\n- **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages not officially supported by the model.\n- **Release Date:** 04/15/2025\n- **Version:** 1.0\n- **Model Developers:** RedHat (Neural Magic)\n\n\n### Model Optimizations\n\nThis model was obtained by quantizing activations and weights of [Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503) to FP8 data type.\nThis optimization reduces the number of bits used to represent weights and activations from 16 to 8, reducing GPU memory requirements (by approximately 50%) and increasing matrix-multiply compute throughput (by approximately 2x).\nWeight quantization also reduces disk size requirements by approximately 50%.\n\nOnly weights and activations of the linear operators within transformers blocks are quantized.\nWeights are quantized with a symmetric static per-channel scheme, whereas activations are quantized with a symmetric dynamic per-token scheme.\n\n\n## Deployment\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel_id = \"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic\"\nnumber_gpus = 1\n\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nprompt = \"Give me a short introduction to large language model.\"\n\nllm = LLM(model=model_id, tensor_parallel_size=number_gpus)\n\noutputs = llm.generate(prompt, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\nvLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\n<details>\n  <summary>Creation details</summary>\n  This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n\n\n  ```python\n  from llmcompressor.modifiers.quantization import QuantizationModifier\n  from llmcompressor.transformers import oneshot\n  from transformers import AutoModelForImageTextToText\n  from PIL import Image\n  import io\n  \n  # Load model\n  model_stub = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n  model_name = model_stub.split(\"/\")[-1]\n\n  model = AutoModelForImageTextToText.from_pretrained(model_stub)\n\n  # Configure the quantization algorithm and scheme\n  recipe = QuantizationModifier(\n      ignore=[\"language_model.lm_head\", \"re:vision_tower.*\", \"re:multi_modal_projector.*\"]\n      targets=\"Linear\",\n      scheme=\"FP8_dynamic\",\n  )\n\n  # Apply quantization\n  oneshot(\n      model=model,\n      recipe=recipe,\n  )\n  \n  # Save to disk in compressed-tensors format\n  save_path = model_name + \"-FP8-dynamic\"\n  model.save_pretrained(save_path)\n  tokenizer.save_pretrained(save_path)\n  print(f\"Model and tokenizer saved to: {save_path}\")\n  ```\n</details>\n \n\n\n## Evaluation\n\nThe model was evaluated on the OpenLLM leaderboard tasks (version 1), MMLU-pro, GPQA, HumanEval and MBPP.\nNon-coding tasks were evaluated with [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), whereas coding tasks were evaluated with a fork of [evalplus](https://github.com/neuralmagic/evalplus).\n[vLLM](https://docs.vllm.ai/en/stable/) is used as the engine in all cases.\n\n<details>\n  <summary>Evaluation details</summary>\n\n  **MMLU**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks mmlu \\\n    --num_fewshot 5 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **ARC Challenge**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks arc_challenge \\\n    --num_fewshot 25 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **GSM8k**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic\",dtype=auto,gpu_memory_utilization=0.9,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks gsm8k \\\n    --num_fewshot 8 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **Hellaswag**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks hellaswag \\\n    --num_fewshot 10 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **Winogrande**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks winogrande \\\n    --num_fewshot 5 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **TruthfulQA**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks truthfulqa \\\n    --num_fewshot 0 \\\n    --apply_chat_template\\\n    --batch_size auto\n  ```\n\n  **MMLU-pro**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks mmlu_pro \\\n    --num_fewshot 5 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n**Coding**\n\nThe commands below can be used for mbpp by simply replacing the dataset name.\n\n*Generation*\n```\npython3 codegen/generate.py \\\n  --model RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic \\\n  --bs 16 \\\n  --temperature 0.2 \\\n  --n_samples 50 \\\n  --root \".\" \\\n  --dataset humaneval\n\n```\n\n*Sanitization*\n```\npython3 evalplus/sanitize.py \\\n  humaneval/RedHatAI--Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic_vllm_temp_0.2\n```\n\n*Evaluation*\n```\nevalplus.evaluate \\\n  --dataset humaneval \\\n  --samples humaneval/RedHatAI--Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic_vllm_temp_0.2-sanitized\n```\n</details>\n\n### Accuracy\n\n#### Open LLM Leaderboard evaluation scores\n<table>\n  <tr>\n   <th>Category\n   </th>\n   <th>Benchmark\n   </th>\n   <th>Mistral-Small-3.1-24B-Instruct-2503\n   </th>\n   <th>Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic<br>(this model)\n   </th>\n   <th>Recovery\n   </th>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" ><strong>OpenLLM v1</strong>\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>80.67\n   </td>\n   <td>80.71\n   </td>\n   <td>100.1%\n   </td>\n  </tr>\n  <tr>\n   <td>ARC Challenge (25-shot)\n   </td>\n   <td>72.78\n   </td>\n   <td>72.87\n   </td>\n   <td>100.1%\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (5-shot, strict-match)\n   </td>\n   <td>65.35\n   </td>\n   <td>62.47\n   </td>\n   <td>95.6%\n   </td>\n  </tr>\n  <tr>\n   <td>Hellaswag (10-shot)\n   </td>\n   <td>83.70\n   </td>\n   <td>83.67\n   </td>\n   <td>100.0%\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>83.74\n   </td>\n   <td>82.56\n   </td>\n   <td>98.6%\n   </td>\n  </tr>\n  <tr>\n   <td>TruthfulQA (0-shot, mc2)\n   </td>\n   <td>70.62\n   </td>\n   <td>70.88\n   </td>\n   <td>100.4%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>76.14</strong>\n   </td>\n   <td><strong>75.53</strong>\n   </td>\n   <td><strong>99.2%</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"3\" ><strong></strong>\n   </td>\n   <td>MMLU-Pro (5-shot)\n   </td>\n   <td>67.25\n   </td>\n   <td>66.86\n   </td>\n   <td>99.4%\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA CoT main (5-shot)\n   </td>\n   <td>42.63\n   </td>\n   <td>41.07\n   </td>\n   <td>99.4%\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA CoT diamond (5-shot)\n   </td>\n   <td>45.96\n   </td>\n   <td>45.45\n   </td>\n   <td>98.9%\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" ><strong>Coding</strong>\n   </td>\n   <td>HumanEval pass@1\n   </td>\n   <td>84.70\n   </td>\n   <td>84.70\n   </td>\n   <td>100.0%\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval+ pass@1\n   </td>\n   <td>79.50\n   </td>\n   <td>79.30\n   </td>\n   <td>99.8%\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP pass@1\n   </td>\n   <td>71.10\n   </td>\n   <td>70.00\n   </td>\n   <td>98.5%\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP+ pass@1\n   </td>\n   <td>60.60\n   </td>\n   <td>59.50\n   </td>\n   <td>98.2%\n   </td>\n  </tr>\n</table>\n\n"
      language:
        - en
        - fr
        - de
        - es
        - it
        - pt
        - hi
        - id
        - tl
        - vi
        - ar
        - zh
        - da
        - fi
        - he
        - ja
        - ko
        - ms
        - nl
        - "no"
        - pl
        - ru
        - sv
        - th
        - tr
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - it is ideal for
      createTimeSinceEpoch: "1744675200"
      lastUpdateTimeSinceEpoch: "1744675200"
      customProperties:
        FP8:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        fast:
            metadataType: MetadataStringValue
            string_value: ""
        llmcompressor:
            metadataType: MetadataStringValue
            string_value: ""
        mistral:
            metadataType: MetadataStringValue
            string_value: ""
        mistral-small:
            metadataType: MetadataStringValue
            string_value: ""
        mistral3:
            metadataType: MetadataStringValue
            string_value: ""
        mistralai:
            metadataType: MetadataStringValue
            string_value: ""
        neuralmagic:
            metadataType: MetadataStringValue
            string_value: ""
        quantized:
            metadataType: MetadataStringValue
            string_value: ""
        redhat:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-mistral-small-3-1-24b-instruct-2503-fp8-dynamic:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744988857000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16
      provider: RedHat (Neural Magic)
      description: Mistral Small 3.1 24B Instruct 2503 Quantized.w4a16 - An instruction-tuned language model
      readme: "# Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16\n\n## Model Overview\n- **Model Architecture:** Mistral3ForConditionalGeneration\n  - **Input:** Text / Image\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Weight quantization:** INT4\n- **Intended Use Cases:** It is ideal for:\n  - Fast-response conversational agents.\n  - Low-latency function calling.\n  - Subject matter experts via fine-tuning.\n  - Local inference for hobbyists and organizations handling sensitive data.\n  - Programming and math reasoning.\n  - Long document understanding.\n  - Visual understanding.\n- **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages not officially supported by the model.\n- **Release Date:** 04/15/2025\n- **Version:** 1.0\n- **Model Developers:** RedHat (Neural Magic)\n\n\n### Model Optimizations\n\nThis model was obtained by quantizing the weights of [Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503) to INT4 data type.\nThis optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 75%.\n\nOnly the weights of the linear operators within transformers blocks are quantized.\nWeights are quantized using a symmetric per-group scheme, with group size 128.\nThe [GPTQ](https://arxiv.org/abs/2210.17323) algorithm is applied for quantization, as implemented in the [llm-compressor](https://github.com/vllm-project/llm-compressor) library.\n\n\n## Deployment\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel_id = \"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16\"\nnumber_gpus = 1\n\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nprompt = \"Give me a short introduction to large language model.\"\n\nllm = LLM(model=model_id, tensor_parallel_size=number_gpus)\n\noutputs = llm.generate(prompt, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\nvLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\n<details>\n  <summary>Creation details</summary>\n  This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n\n\n  ```python\n  from transformers import AutoProcessor\n  from llmcompressor.modifiers.quantization import GPTQModifier\n  from llmcompressor.transformers import oneshot\n  from llmcompressor.transformers.tracing import TraceableMistral3ForConditionalGeneration\n  from PIL import Image\n  import io\n  \n  # Load model\n  model_stub = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n  model_name = model_stub.split(\"/\")[-1]\n  \n  num_text_samples = 1024\n  num_vison_samples = 1024\n  max_seq_len = 8192\n  \n  processor = AutoProcessor.from_pretrained(model_stub)\n  \n  model = TraceableMistral3ForConditionalGeneration.from_pretrained(\n      model_stub,\n      device_map=\"auto\",\n      torch_dtype=\"auto\",\n  )\n\n  # Text-only data subset\n  def preprocess_text(example):\n      input = {\n          \"text\": processor.apply_chat_template(\n              example[\"messages\"],\n              add_generation_prompt=False,\n          ),\n          \"images\" = None,\n      }\n      tokenized_input = processor(**input, max_length=max_seq_len, truncation=True)\n      tokenized_input[\"pixel_values\"] = tokenized_input.get(\"pixel_values\", None)\n      tokenized_input[\"image_sizes\"] = tokenized_input.get(\"image_sizes\", None)\n\n  dst = load_dataset(\"neuralmagic/calibration\", name=\"LLM\", split=\"train\").select(num_text_samples)\n  dst = dst.map(preprocess_text, remove_columns=dst.column_names)\n\n  # Text + vision data subset\n  def preprocess_vision(example):\n      messages = []\n      image = None\n      for message in example[\"messages\"]:\n          message_content = []\n          for content in message[\"content\"]\n              if content[\"type\"] == \"text\":\n                  message_content = {\"type\": \"text\", \"text\": content[\"text\"]}\n              else:\n                  message_content = {\"type\": \"image\"}}\n                  image = Image.open(io.BytesIO(content[\"image\"]))\n\n          messages.append(\n              {\n                  \"role\": message[\"role\"],\n                  \"content\": message_content,\n              }\n          )\n\n      input = {\n          \"text\": processor.apply_chat_template(\n              messages,\n              add_generation_prompt=False,\n          ),\n          \"images\" = image,\n      }\n      tokenized_input = processor(**input, max_length=max_seq_len, truncation=True)\n      tokenized_input[\"pixel_values\"] = tokenized_input.get(\"pixel_values\", None)\n      tokenized_input[\"image_sizes\"] = tokenized_input.get(\"image_sizes\", None)\n\n  dsv = load_dataset(\"neuralmagic/calibration\", name=\"VLLM\", split=\"train\").select(num_vision_samples)\n  dsv = dsv.map(preprocess_vision, remove_columns=dsv.column_names)\n\n  # Interleave subsets\n  ds = interleave_datasets((dsv, dst))\n\n  # Configure the quantization algorithm and scheme\n  recipe = GPTQModifier(\n      ignore=[\"language_model.lm_head\", \"re:vision_tower.*\", \"re:multi_modal_projector.*\"]\n      sequential_targets=[\"MistralDecoderLayer\"]\n      dampening_frac=0.01\n      targets=\"Linear\",\n      scheme=\"W4A16\",\n  )\n\n  # Define data collator\n  def data_collator(batch):\n      import torch\n      assert len(batch) == 1\n      collated = {}\n      for k, v in batch[0].items():\n          if v is None:\n              continue\n          if k == \"input_ids\":\n              collated[k] = torch.LongTensor(v)\n          elif k == \"pixel_values\":\n              collated[k] = torch.tensor(v, dtype=torch.bfloat16)\n          else:\n              collated[k] = torch.tensor(v)\n      return collated\n\n\n  # Apply quantization\n  oneshot(\n      model=model,\n      dataset=ds, \n      recipe=recipe,\n      max_seq_length=max_seq_len,\n      data_collator=data_collator,\n  )\n  \n  # Save to disk in compressed-tensors format\n  save_path = model_name + \"-quantized.w4a16\n  model.save_pretrained(save_path)\n  tokenizer.save_pretrained(save_path)\n  print(f\"Model and tokenizer saved to: {save_path}\")\n  ```\n</details>\n \n\n\n## Evaluation\n\nThe model was evaluated on the OpenLLM leaderboard tasks (version 1), MMLU-pro, GPQA, HumanEval and MBPP.\nNon-coding tasks were evaluated with [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), whereas coding tasks were evaluated with a fork of [evalplus](https://github.com/neuralmagic/evalplus).\n[vLLM](https://docs.vllm.ai/en/stable/) is used as the engine in all cases.\n\n<details>\n  <summary>Evaluation details</summary>\n\n  **MMLU**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks mmlu \\\n    --num_fewshot 5 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **ARC Challenge**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks arc_challenge \\\n    --num_fewshot 25 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **GSM8k**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16\",dtype=auto,gpu_memory_utilization=0.9,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks gsm8k \\\n    --num_fewshot 8 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **Hellaswag**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks hellaswag \\\n    --num_fewshot 10 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **Winogrande**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks winogrande \\\n    --num_fewshot 5 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **TruthfulQA**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks truthfulqa \\\n    --num_fewshot 0 \\\n    --apply_chat_template\\\n    --batch_size auto\n  ```\n\n  **MMLU-pro**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks mmlu_pro \\\n    --num_fewshot 5 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n**Coding**\n\nThe commands below can be used for mbpp by simply replacing the dataset name.\n\n*Generation*\n```\npython3 codegen/generate.py \\\n  --model RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16 \\\n  --bs 16 \\\n  --temperature 0.2 \\\n  --n_samples 50 \\\n  --root \".\" \\\n  --dataset humaneval\n\n```\n\n*Sanitization*\n```\npython3 evalplus/sanitize.py \\\n  humaneval/RedHatAI--Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16_vllm_temp_0.2\n```\n\n*Evaluation*\n```\nevalplus.evaluate \\\n  --dataset humaneval \\\n  --samples humaneval/RedHatAI--Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16_vllm_temp_0.2-sanitized\n```\n</details>\n\n### Accuracy\n\n#### Open LLM Leaderboard evaluation scores\n<table>\n  <tr>\n   <th>Category\n   </th>\n   <th>Benchmark\n   </th>\n   <th>Mistral-Small-3.1-24B-Instruct-2503\n   </th>\n   <th>Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16<br>(this model)\n   </th>\n   <th>Recovery\n   </th>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" ><strong>OpenLLM v1</strong>\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>80.67\n   </td>\n   <td>79.74\n   </td>\n   <td>98.9%\n   </td>\n  </tr>\n  <tr>\n   <td>ARC Challenge (25-shot)\n   </td>\n   <td>72.78\n   </td>\n   <td>72.18\n   </td>\n   <td>99.2%\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (5-shot, strict-match)\n   </td>\n   <td>65.35\n   </td>\n   <td>66.34\n   </td>\n   <td>101.5%\n   </td>\n  </tr>\n  <tr>\n   <td>Hellaswag (10-shot)\n   </td>\n   <td>83.70\n   </td>\n   <td>83.25\n   </td>\n   <td>99.5%\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>83.74\n   </td>\n   <td>83.43\n   </td>\n   <td>99.6%\n   </td>\n  </tr>\n  <tr>\n   <td>TruthfulQA (0-shot, mc2)\n   </td>\n   <td>70.62\n   </td>\n   <td>69.56\n   </td>\n   <td>98.5%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>76.14</strong>\n   </td>\n   <td><strong>75.75</strong>\n   </td>\n   <td><strong>99.5%</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"3\" ><strong></strong>\n   </td>\n   <td>MMLU-Pro (5-shot)\n   </td>\n   <td>67.25\n   </td>\n   <td>66.56\n   </td>\n   <td>99.0%\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA CoT main (5-shot)\n   </td>\n   <td>42.63\n   </td>\n   <td>47.10\n   </td>\n   <td>110.5%\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA CoT diamond (5-shot)\n   </td>\n   <td>45.96\n   </td>\n   <td>44.95\n   </td>\n   <td>97.80%\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" ><strong>Coding</strong>\n   </td>\n   <td>HumanEval pass@1\n   </td>\n   <td>84.70\n   </td>\n   <td>84.60\n   </td>\n   <td>99.9%\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval+ pass@1\n   </td>\n   <td>79.50\n   </td>\n   <td>79.90\n   </td>\n   <td>100.5%\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP pass@1\n   </td>\n   <td>71.10\n   </td>\n   <td>70.10\n   </td>\n   <td>98.6%\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP+ pass@1\n   </td>\n   <td>60.60\n   </td>\n   <td>60.70\n   </td>\n   <td>100.2%\n   </td>\n  </tr>\n</table>\n\n"
      language:
        - en
        - fr
        - de
        - es
        - it
        - pt
        - hi
        - id
        - tl
        - vi
        - ar
        - zh
        - da
        - fi
        - he
        - ja
        - ko
        - ms
        - nl
        - "no"
        - pl
        - ru
        - sv
        - th
        - tr
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - it is ideal for
      createTimeSinceEpoch: "1744675200"
      lastUpdateTimeSinceEpoch: "1744675200"
      customProperties:
        INT4:
            metadataType: MetadataStringValue
            string_value: ""
        W4A16:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        fast:
            metadataType: MetadataStringValue
            string_value: ""
        llmcompressor:
            metadataType: MetadataStringValue
            string_value: ""
        mistral:
            metadataType: MetadataStringValue
            string_value: ""
        mistral-small:
            metadataType: MetadataStringValue
            string_value: ""
        mistral3:
            metadataType: MetadataStringValue
            string_value: ""
        mistralai:
            metadataType: MetadataStringValue
            string_value: ""
        neuralmagic:
            metadataType: MetadataStringValue
            string_value: ""
        quantized:
            metadataType: MetadataStringValue
            string_value: ""
        redhat:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-mistral-small-3-1-24b-instruct-2503-quantized-w4a16:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744920668000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8
      provider: RedHat (Neural Magic)
      description: Mistral Small 3.1 24B Instruct 2503 Quantized.w8a8 - An instruction-tuned language model
      readme: "# Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8\n\n## Model Overview\n- **Model Architecture:** Mistral3ForConditionalGeneration\n  - **Input:** Text / Image\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Activation quantization:** INT8\n  - **Weight quantization:** INT8\n- **Intended Use Cases:** It is ideal for:\n  - Fast-response conversational agents.\n  - Low-latency function calling.\n  - Subject matter experts via fine-tuning.\n  - Local inference for hobbyists and organizations handling sensitive data.\n  - Programming and math reasoning.\n  - Long document understanding.\n  - Visual understanding.\n- **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages not officially supported by the model.\n- **Release Date:** 04/15/2025\n- **Version:** 1.0\n- **Model Developers:** RedHat (Neural Magic)\n\n\n### Model Optimizations\n\nThis model was obtained by quantizing activations and weights of [Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503) to INT8 data type.\nThis optimization reduces the number of bits used to represent weights and activations from 16 to 8, reducing GPU memory requirements (by approximately 50%) and increasing matrix-multiply compute throughput (by approximately 2x).\nWeight quantization also reduces disk size requirements by approximately 50%.\n\nOnly weights and activations of the linear operators within transformers blocks are quantized.\nWeights are quantized with a symmetric static per-channel scheme, whereas activations are quantized with a symmetric dynamic per-token scheme.\nA combination of the [SmoothQuant](https://arxiv.org/abs/2211.10438) and [GPTQ](https://arxiv.org/abs/2210.17323) algorithms is applied for quantization, as implemented in the [llm-compressor](https://github.com/vllm-project/llm-compressor) library.\n\n\n## Deployment\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel_id = \"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8\"\nnumber_gpus = 1\n\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nprompt = \"Give me a short introduction to large language model.\"\n\nllm = LLM(model=model_id, tensor_parallel_size=number_gpus)\n\noutputs = llm.generate(prompt, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\nvLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\n<details>\n  <summary>Creation details</summary>\n  This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n\n\n  ```python\n  from transformers import AutoProcessor\n  from llmcompressor.modifiers.quantization import GPTQModifier\n  from llmcompressor.transformers import oneshot\n  from llmcompressor.transformers.tracing import TraceableMistral3ForConditionalGeneration\n  from PIL import Image\n  import io\n  \n  # Load model\n  model_stub = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n  model_name = model_stub.split(\"/\")[-1]\n  \n  num_text_samples = 1024\n  num_vison_samples = 1024\n  max_seq_len = 8192\n  \n  processor = AutoProcessor.from_pretrained(model_stub)\n  \n  model = TraceableMistral3ForConditionalGeneration.from_pretrained(\n      model_stub,\n      device_map=\"auto\",\n      torch_dtype=\"auto\",\n  )\n\n  # Text-only data subset\n  def preprocess_text(example):\n      input = {\n          \"text\": processor.apply_chat_template(\n              example[\"messages\"],\n              add_generation_prompt=False,\n          ),\n          \"images\" = None,\n      }\n      tokenized_input = processor(**input, max_length=max_seq_len, truncation=True)\n      tokenized_input[\"pixel_values\"] = tokenized_input.get(\"pixel_values\", None)\n      tokenized_input[\"image_sizes\"] = tokenized_input.get(\"image_sizes\", None)\n\n  dst = load_dataset(\"neuralmagic/calibration\", name=\"LLM\", split=\"train\").select(num_text_samples)\n  dst = dst.map(preprocess_text, remove_columns=dst.column_names)\n\n  # Text + vision data subset\n  def preprocess_vision(example):\n      messages = []\n      image = None\n      for message in example[\"messages\"]:\n          message_content = []\n          for content in message[\"content\"]\n              if content[\"type\"] == \"text\":\n                  message_content = {\"type\": \"text\", \"text\": content[\"text\"]}\n              else:\n                  message_content = {\"type\": \"image\"}}\n                  image = Image.open(io.BytesIO(content[\"image\"]))\n\n          messages.append(\n              {\n                  \"role\": message[\"role\"],\n                  \"content\": message_content,\n              }\n          )\n\n      input = {\n          \"text\": processor.apply_chat_template(\n              messages,\n              add_generation_prompt=False,\n          ),\n          \"images\" = image,\n      }\n      tokenized_input = processor(**input, max_length=max_seq_len, truncation=True)\n      tokenized_input[\"pixel_values\"] = tokenized_input.get(\"pixel_values\", None)\n      tokenized_input[\"image_sizes\"] = tokenized_input.get(\"image_sizes\", None)\n\n  dsv = load_dataset(\"neuralmagic/calibration\", name=\"VLLM\", split=\"train\").select(num_vision_samples)\n  dsv = dsv.map(preprocess_vision, remove_columns=dsv.column_names)\n\n  # Interleave subsets\n  ds = interleave_datasets((dsv, dst))\n\n  # Configure the quantization algorithm and scheme\n  recipe = [\n      SmoothQuantModifier(smoothing_strength=0.8),\n      GPTQModifier(\n          ignore=[\"language_model.lm_head\", \"re:vision_tower.*\", \"re:multi_modal_projector.*\"]\n          sequential_targets=[\"MistralDecoderLayer\"]\n          dampening_frac=0.01\n          targets=\"Linear\",\n          scheme=\"W8A8\",\n      ),\n  ]\n\n  # Define data collator\n  def data_collator(batch):\n      import torch\n      assert len(batch) == 1\n      collated = {}\n      for k, v in batch[0].items():\n          if v is None:\n              continue\n          if k == \"input_ids\":\n              collated[k] = torch.LongTensor(v)\n          elif k == \"pixel_values\":\n              collated[k] = torch.tensor(v, dtype=torch.bfloat16)\n          else:\n              collated[k] = torch.tensor(v)\n      return collated\n\n\n  # Apply quantization\n  oneshot(\n      model=model,\n      dataset=ds, \n      recipe=recipe,\n      max_seq_length=max_seq_len,\n      data_collator=data_collator,\n  )\n  \n  # Save to disk in compressed-tensors format\n  save_path = model_name + \"-quantized.w8a8\n  model.save_pretrained(save_path)\n  tokenizer.save_pretrained(save_path)\n  print(f\"Model and tokenizer saved to: {save_path}\")\n  ```\n</details>\n \n\n\n## Evaluation\n\nThe model was evaluated on the OpenLLM leaderboard tasks (version 1), MMLU-pro, GPQA, HumanEval and MBPP.\nNon-coding tasks were evaluated with [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), whereas coding tasks were evaluated with a fork of [evalplus](https://github.com/neuralmagic/evalplus).\n[vLLM](https://docs.vllm.ai/en/stable/) is used as the engine in all cases.\n\n<details>\n  <summary>Evaluation details</summary>\n\n  **MMLU**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks mmlu \\\n    --num_fewshot 5 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **ARC Challenge**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks arc_challenge \\\n    --num_fewshot 25 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **GSM8k**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8\",dtype=auto,gpu_memory_utilization=0.9,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks gsm8k \\\n    --num_fewshot 8 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **Hellaswag**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks hellaswag \\\n    --num_fewshot 10 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **Winogrande**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks winogrande \\\n    --num_fewshot 5 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n  **TruthfulQA**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks truthfulqa \\\n    --num_fewshot 0 \\\n    --apply_chat_template\\\n    --batch_size auto\n  ```\n\n  **MMLU-pro**\n  ```\n  lm_eval \\\n    --model vllm \\\n    --model_args pretrained=\"RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=8192,enable_chunk_prefill=True,tensor_parallel_size=2 \\\n    --tasks mmlu_pro \\\n    --num_fewshot 5 \\\n    --apply_chat_template\\\n    --fewshot_as_multiturn \\\n    --batch_size auto\n  ```\n\n**Coding**\n\nThe commands below can be used for mbpp by simply replacing the dataset name.\n\n*Generation*\n```\npython3 codegen/generate.py \\\n  --model RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8 \\\n  --bs 16 \\\n  --temperature 0.2 \\\n  --n_samples 50 \\\n  --root \".\" \\\n  --dataset humaneval\n\n```\n\n*Sanitization*\n```\npython3 evalplus/sanitize.py \\\n  humaneval/RedHatAI--Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8_vllm_temp_0.2\n```\n\n*Evaluation*\n```\nevalplus.evaluate \\\n  --dataset humaneval \\\n  --samples humaneval/RedHatAI--Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8_vllm_temp_0.2-sanitized\n```\n</details>\n\n### Accuracy\n\n#### Open LLM Leaderboard evaluation scores\n<table>\n  <tr>\n   <th>Category\n   </th>\n   <th>Benchmark\n   </th>\n   <th>Mistral-Small-3.1-24B-Instruct-2503\n   </th>\n   <th>Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8<br>(this model)\n   </th>\n   <th>Recovery\n   </th>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" ><strong>OpenLLM v1</strong>\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>80.67\n   </td>\n   <td>80.40\n   </td>\n   <td>99.7%\n   </td>\n  </tr>\n  <tr>\n   <td>ARC Challenge (25-shot)\n   </td>\n   <td>72.78\n   </td>\n   <td>73.46\n   </td>\n   <td>100.9%\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (5-shot, strict-match)\n   </td>\n   <td>65.35\n   </td>\n   <td>70.58\n   </td>\n   <td>108.0%\n   </td>\n  </tr>\n  <tr>\n   <td>Hellaswag (10-shot)\n   </td>\n   <td>83.70\n   </td>\n   <td>82.26\n   </td>\n   <td>98.3%\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>83.74\n   </td>\n   <td>80.90\n   </td>\n   <td>96.6%\n   </td>\n  </tr>\n  <tr>\n   <td>TruthfulQA (0-shot, mc2)\n   </td>\n   <td>70.62\n   </td>\n   <td>69.15\n   </td>\n   <td>97.9%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>76.14</strong>\n   </td>\n   <td><strong>76.13</strong>\n   </td>\n   <td><strong>100.0%</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"3\" ><strong></strong>\n   </td>\n   <td>MMLU-Pro (5-shot)\n   </td>\n   <td>67.25\n   </td>\n   <td>66.54\n   </td>\n   <td>98.9%\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA CoT main (5-shot)\n   </td>\n   <td>42.63\n   </td>\n   <td>44.64\n   </td>\n   <td>104.7%\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA CoT diamond (5-shot)\n   </td>\n   <td>45.96\n   </td>\n   <td>41.92\n   </td>\n   <td>91.2%\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" ><strong>Coding</strong>\n   </td>\n   <td>HumanEval pass@1\n   </td>\n   <td>84.70\n   </td>\n   <td>84.20\n   </td>\n   <td>99.4%\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval+ pass@1\n   </td>\n   <td>79.50\n   </td>\n   <td>81.00\n   </td>\n   <td>101.9%\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP pass@1\n   </td>\n   <td>71.10\n   </td>\n   <td>72.10\n   </td>\n   <td>101.4%\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP+ pass@1\n   </td>\n   <td>60.60\n   </td>\n   <td>62.10\n   </td>\n   <td>100.7%\n   </td>\n  </tr>\n</table>\n\n"
      language:
        - en
        - fr
        - de
        - es
        - it
        - pt
        - hi
        - id
        - tl
        - vi
        - ar
        - zh
        - da
        - fi
        - he
        - ja
        - ko
        - ms
        - nl
        - "no"
        - pl
        - ru
        - sv
        - th
        - tr
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - it is ideal for
      createTimeSinceEpoch: "1744675200"
      lastUpdateTimeSinceEpoch: "1744675200"
      customProperties:
        8-bit:
            metadataType: MetadataStringValue
            string_value: ""
        W8A8:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        fast:
            metadataType: MetadataStringValue
            string_value: ""
        llmcompressor:
            metadataType: MetadataStringValue
            string_value: ""
        mistral:
            metadataType: MetadataStringValue
            string_value: ""
        mistral-small:
            metadataType: MetadataStringValue
            string_value: ""
        mistral3:
            metadataType: MetadataStringValue
            string_value: ""
        mistralai:
            metadataType: MetadataStringValue
            string_value: ""
        neuralmagic:
            metadataType: MetadataStringValue
            string_value: ""
        quantized:
            metadataType: MetadataStringValue
            string_value: ""
        redhat:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-mistral-small-3-1-24b-instruct-2503-quantized-w8a8:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744989864000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Mixtral-8x7B-Instruct-v0.1
      provider: Mistral AI
      description: A pretrained generative Sparse Mixture of Experts model with 8x7B parameters.
      readme: null
      language:
        - en
        - fr
        - it
        - de
        - es
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-to-text
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1739776989000"
      customProperties:
        conversational:
            metadataType: MetadataStringValue
            string_value: ""
        featured:
            metadataType: MetadataStringValue
            string_value: ""
        mistral:
            metadataType: MetadataStringValue
            string_value: ""
        mixtral:
            metadataType: MetadataStringValue
            string_value: ""
        text-generation-inference:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-mixtral-8x7b-instruct-v0-1:1.4
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1739776989000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Qwen2.5-7B-Instruct
      provider: Hugging face
      description: Qwen2.5 7B Instruct - An instruction-tuned language model
      readme: "# Qwen2.5-7B-Instruct\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 7B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 7.61B\n- Number of Paramaters (Non-Embedding): 6.53B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 28 for Q and 4 for KV\n- Context Length: Full 131,072 tokens and generation 8192 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [\U0001F4D1 blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```"
      language:
        - zh
        - en
        - fr
        - es
        - pt
        - de
        - it
        - ru
        - ja
        - ko
        - vi
        - th
        - ar
        - id
        - tr
        - nl
        - pl
        - cs
        - he
        - sv
        - fi
        - da
        - "no"
        - ms
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-generation
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1744988136000"
      customProperties:
        qwen:
            metadataType: MetadataStringValue
            string_value: ""
        qwen2:
            metadataType: MetadataStringValue
            string_value: ""
        qwen2_5:
            metadataType: MetadataStringValue
            string_value: ""
        qwen2_5_instruct:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-qwen2-5-7b-instruct:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744988136000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Qwen2.5-7B-Instruct-FP8-dynamic
      provider: Neural Magic
      description: '- **Model Architecture:** Qwen2'
      readme: "# Qwen2.5-7B-Instruct-FP8-dynamic\n\n## Model Overview\n- **Model Architecture:** Qwen2\n  - **Input:** Text\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Activation quantization:** FP8\n  - **Weight quantization:** FP8\n- **Intended Use Cases:** Intended for commercial and research use multiple languages. Similarly to [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B), this models is intended for assistant-like chat.\n- **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws).\n- **Release Date:** 11/27/2024\n- **Version:** 1.0\n- **License(s):** [apache-2.0](https://huggingface.co/Qwen/Qwen2.5-7B/blob/main/LICENSE)\n- **Model Developers:** Neural Magic\n\n### Model Optimizations\n\nThis model was obtained by quantizing activations and weights of [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) to FP8 data type.\nThis optimization reduces the number of bits used to represent weights and activations from 16 to 8, reducing GPU memory requirements (by approximately 50%) and increasing matrix-multiply compute throughput (by approximately 2x).\nWeight quantization also reduces disk size requirements by approximately 50%.\n\nOnly weights and activations of the linear operators within transformers blocks are quantized.\nWeights are quantized with a symmetric static per-channel scheme, whereas activations are quantized with a symmetric dynamic per-token scheme.\nThe [llm-compressor](https://github.com/vllm-project/llm-compressor) library is used for quantization.\n\n## Deployment\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel_id = \"RedHatAI/Qwen2.5-7B-Instruct-FP8-dynamic\"\nnumber_gpus = 1\nmax_model_len = 8192\n\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Give me a short introduction to large language model.\"},\n]\n\nprompts = tokenizer.apply_chat_template(messages, tokenize=False)\n\nllm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=max_model_len)\n\noutputs = llm.generate(prompts, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\nvLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\n<details>\n  <summary>Creation details</summary>\n  This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n\n\n  ```python\n  from transformers import AutoModelForCausalLM, AutoTokenizer\n  from llmcompressor.modifiers.quantization import QuantizationModifier\n  from llmcompressor.transformers import oneshot\n  \n  # Load model\n  model_stub = \"Qwen/Qwen2.5-7B-Instruct-FP8-dynamic\"\n  model_name = model_stub.split(\"/\")[-1]\n  \n  tokenizer = AutoTokenizer.from_pretrained(model_stub)\n  \n  model = AutoModelForCausalLM.from_pretrained(\n      model_stub,\n      device_map=\"auto\",\n      torch_dtype=\"auto\",\n  )\n  \n  # Configure the quantization algorithm and scheme\n  recipe = QuantizationModifier(\n      targets=\"Linear\",\n      scheme=\"FP8_dynamic\",\n      ignore=[\"lm_head\"],\n  )\n  \n  # Apply quantization\n  oneshot(\n      model=model,\n      recipe=recipe,\n  )\n  \n  # Save to disk in compressed-tensors format\n  save_path = model_name + \"-FP8-dynamic\"\n  model.save_pretrained(save_path)\n  tokenizer.save_pretrained(save_path)\n  print(f\"Model and tokenizer saved to: {save_path}\")\n  ```\n</details>\n\n## Evaluation\n\nThe model was evaluated on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) leaderboard tasks (version 1) with the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/387Bbd54bc621086e05aa1b030d8d4d5635b25e6) (commit 387Bbd54bc621086e05aa1b030d8d4d5635b25e6) and the [vLLM](https://docs.vllm.ai/en/stable/) engine, using the following command:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Qwen2.5-7B-Instruct-FP8-dynamic\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=4096,enable_chunk_prefill=True,tensor_parallel_size=1 \\\n  --apply_chat_template \\\n  --fewshot_as_multiturn \\\n  --tasks openllm \\\n  --batch_size auto\n```\n\n### Accuracy\n\n#### Open LLM Leaderboard evaluation scores\n<table>\n  <tr>\n   <th>Benchmark\n   </th>\n   <th>Qwen2.5-7B-Instruct\n   </th>\n   <th>Qwen2.5-7B-Instruct-FP8-dynamic<br>(this model)\n   </th>\n   <th>Recovery\n   </th>\n  </tr>\n  <tr>\n   <td>MMLU (5-shot)\n   </td>\n   <td>74.24\n   </td>\n   <td>74.04\n   </td>\n   <td>99.7%\n   </td>\n  </tr>\n  <tr>\n   <td>ARC Challenge (25-shot)\n   </td>\n   <td>63.40\n   </td>\n   <td>63.14\n   </td>\n   <td>99.6%\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (5-shot, strict-match)\n   </td>\n   <td>80.36\n   </td>\n   <td>80.06\n   </td>\n   <td>99.6%\n   </td>\n  </tr>\n  <tr>\n   <td>Hellaswag (10-shot)\n   </td>\n   <td>81.52\n   </td>\n   <td>81.11\n   </td>\n   <td>99.5%\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>74.66\n   </td>\n   <td>74.43\n   </td>\n   <td>99.7%\n   </td>\n  </tr>\n  <tr>\n   <td>TruthfulQA (0-shot, mc2)\n   </td>\n   <td>64.76\n   </td>\n   <td>64.87\n   </td>\n   <td>100.2%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>73.16</strong>\n   </td>\n   <td><strong>72.94</strong>\n   </td>\n   <td><strong>99.7%</strong>\n   </td>\n  </tr>\n</table>\n\n"
      language:
        - zh
        - en
        - fr
        - es
        - pt
        - de
        - it
        - ru
        - ja
        - ko
        - vi
        - th
        - ar
        - id
        - tr
        - nl
        - pl
        - cs
        - he
        - sv
        - fi
        - da
        - "no"
        - ms
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-generation
        - text-generation
        - question-answering
      createTimeSinceEpoch: "1732665600"
      lastUpdateTimeSinceEpoch: "1732665600"
      customProperties:
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        fp8:
            metadataType: MetadataStringValue
            string_value: ""
        quantized:
            metadataType: MetadataStringValue
            string_value: ""
        qwen:
            metadataType: MetadataStringValue
            string_value: ""
        qwen2:
            metadataType: MetadataStringValue
            string_value: ""
        qwen2_5:
            metadataType: MetadataStringValue
            string_value: ""
        qwen2_5_instruct:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-qwen2-5-7b-instruct-fp8-dynamic:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744987926000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Qwen2.5-7B-Instruct-quantized.w4a16
      provider: Neural Magic
      description: '- **Model Architecture:** Qwen2'
      readme: "# Qwen2.5-7B-Instruct-quantized.w4a16\n\n## Model Overview\n- **Model Architecture:** Qwen2\n  - **Input:** Text\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Weight quantization:** INT4\n- **Intended Use Cases:** Intended for commercial and research use multiple languages. Similarly to [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B), this models is intended for assistant-like chat.\n- **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws).\n- **Release Date:** 04/16/2025\n- **Version:** 1.0\n- **License(s):** [apache-2.0](https://huggingface.co/Qwen/Qwen2.5-7B/blob/main/LICENSE)\n- **Model Developers:** Neural Magic\n\n### Model Optimizations\n\nThis model was obtained by quantizing the weights of [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) to INT4 data type.\nThis optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 75%.\n\nOnly the weights of the linear operators within transformers blocks are quantized.\nWeights are quantized using a symmetric per-group scheme, with group size 128.\nThe [GPTQ](https://arxiv.org/abs/2210.17323) algorithm is applied for quantization, as implemented in the [llm-compressor](https://github.com/vllm-project/llm-compressor) library.\n\n## Deployment\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel_id = \"RedHatAI/Qwen2.5-7B-Instruct-quantized.w4a16\"\nnumber_gpus = 1\nmax_model_len = 8192\n\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Give me a short introduction to large language model.\"},\n]\n\nprompts = tokenizer.apply_chat_template(messages, tokenize=False)\n\nllm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=max_model_len)\n\noutputs = llm.generate(prompts, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\nvLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\n<details>\n  <summary>Creation details</summary>\n  This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n\n\n  ```python\n  from transformers import AutoModelForCausalLM, AutoTokenizer\n  from llmcompressor.modifiers.quantization import GPTQModifier\n  from llmcompressor.transformers import oneshot\n  from datasets import load_dataset\n  \n  # Load model\n  model_stub = \"Qwen/Qwen2.5-7B-Instruct\"\n  model_name = model_stub.split(\"/\")[-1]\n  \n  num_samples = 3072\n  max_seq_len = 8192\n  \n  tokenizer = AutoTokenizer.from_pretrained(model_stub)\n  \n  model = AutoModelForCausalLM.from_pretrained(\n      model_stub,\n      device_map=\"auto\",\n      torch_dtype=\"auto\",\n  )\n  \n  def preprocess_fn(example):\n      return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], add_generation_prompt=False, tokenize=False)}\n  \n  ds = load_dataset(\"neuralmagic/LLM_compression_calibration\", split=\"train\")\n  ds = ds.map(preprocess_fn)\n  \n  # Configure the quantization algorithm and scheme\n  recipe = GPTQModifier(\n      targets=\"Linear\",\n      scheme=\"W4A16\",\n      ignore=[\"lm_head\"],\n      sequential_targets=[\"Qwen2DecoderLayer\"],\n      dampening_frac=0.2,\n  )\n  \n  # Apply quantization\n  oneshot(\n      model=model,\n      dataset=ds, \n      recipe=recipe,\n      max_seq_length=max_seq_len,\n      num_calibration_samples=num_samples,\n  )\n  \n  # Save to disk in compressed-tensors format\n  save_path = model_name + \"-quantized.w4a16\"\n  model.save_pretrained(save_path)\n  tokenizer.save_pretrained(save_path)\n  print(f\"Model and tokenizer saved to: {save_path}\")\n  ```\n</details>\n\n## Evaluation\n\nThe model was evaluated on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) leaderboard tasks (version 1) with the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/387Bbd54bc621086e05aa1b030d8d4d5635b25e6) (commit 387Bbd54bc621086e05aa1b030d8d4d5635b25e6) and the [vLLM](https://docs.vllm.ai/en/stable/) engine, using the following command:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Qwen2.5-7B-Instruct-quantized.w4a16\",dtype=auto,gpu_memory_utilization=0.5,max_model_len=4096,enable_chunk_prefill=True,tensor_parallel_size=1 \\\n  --apply_chat_template \\\n  --fewshot_as_multiturn \\\n  --tasks openllm \\\n  --batch_size auto\n```\n\n### Accuracy\n\n#### Open LLM Leaderboard evaluation scores\n<table>\n  <tr>\n   <th>Benchmark\n   </th>\n   <th>Qwen2.5-7B-Instruct\n   </th>\n   <th>Qwen2.5-7B-Instruct-quantized.w4a16<br>(this model)\n   </th>\n   <th>Recovery\n   </th>\n  </tr>\n  <tr>\n   <td>MMLU (5-shot)\n   </td>\n   <td>74.24\n   </td>\n   <td>73.19\n   </td>\n   <td>98.6%\n   </td>\n  </tr>\n  <tr>\n   <td>ARC Challenge (25-shot)\n   </td>\n   <td>63.40\n   </td>\n   <td>63.23\n   </td>\n   <td>99.7%\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (5-shot, strict-match)\n   </td>\n   <td>80.36\n   </td>\n   <td>80.59\n   </td>\n   <td>100.3%\n   </td>\n  </tr>\n  <tr>\n   <td>Hellaswag (10-shot)\n   </td>\n   <td>81.52\n   </td>\n   <td>80.65\n   </td>\n   <td>98.9%\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>74.66\n   </td>\n   <td>74.19\n   </td>\n   <td>99.4%\n   </td>\n  </tr>\n  <tr>\n   <td>TruthfulQA (0-shot, mc2)\n   </td>\n   <td>64.76\n   </td>\n   <td>64.27\n   </td>\n   <td>99.3%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>73.16</strong>\n   </td>\n   <td><strong>72.69</strong>\n   </td>\n   <td><strong>98.6%</strong>\n   </td>\n  </tr>\n</table>\n\n"
      language:
        - zh
        - en
        - fr
        - es
        - pt
        - de
        - it
        - ru
        - ja
        - ko
        - vi
        - th
        - ar
        - id
        - tr
        - nl
        - pl
        - cs
        - he
        - sv
        - fi
        - da
        - "no"
        - ms
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-generation
        - text-generation
        - question-answering
      createTimeSinceEpoch: "1744761600"
      lastUpdateTimeSinceEpoch: "1744761600"
      customProperties:
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        int4:
            metadataType: MetadataStringValue
            string_value: ""
        qwen:
            metadataType: MetadataStringValue
            string_value: ""
        qwen2:
            metadataType: MetadataStringValue
            string_value: ""
        qwen2_5:
            metadataType: MetadataStringValue
            string_value: ""
        qwen2_5_instruct:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
        w4a16:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-qwen2-5-7b-instruct-quantized-w4a16:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1745836482000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/Qwen2.5-7B-Instruct-quantized.w8a8
      provider: Neural Magic
      description: Qwen2.5 7B Instruct Quantized.w8a8 - An instruction-tuned language model
      readme: |
        # Qwen2.5-7B-Instruct-quantized.w8a8

        ## Model Overview
        - **Model Architecture:** Qwen2
          - **Input:** Text
          - **Output:** Text
        - **Model Optimizations:**
          - **Activation quantization:** INT8
          - **Weight quantization:** INT8
        - **Intended Use Cases:** Intended for commercial and research use multiple languages. Similarly to [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct), this models is intended for assistant-like chat.
        - **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws).
        - **Release Date:** 10/09/2024
        - **Version:** 1.0
        - **Model Developers:** Neural Magic

        Quantized version of [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct).
        It achieves an average score of 73.05 on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) benchmark version 1 and 41.44 on version 2, whereas the unquantized model achieves 73.16 on version 1 and 41.40 on version 2.

        ### Model Optimizations

        This model was obtained by quantizing the weights and activations of [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) to INT8 data type.
        This optimization reduces the number of bits used to represent weights and activations from 16 to 8, reducing GPU memory requirements (by approximately 50%) and increasing matrix-multiply compute throughput (by approximately 2x).
        Weight quantization also reduces disk size requirements by approximately 50%.

        Only weights and activations of the linear operators within transformers blocks are quantized.
        Weights are quantized with a symmetric static per-channel scheme, where a fixed linear scaling factor is applied between INT8 and floating point representations for each output channel dimension.
        Activations are quantized with a symmetric dynamic per-token scheme, computing a linear scaling factor at runtime for each token between INT8 and floating point representations.

        ## Deployment

        This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

        ```python
        from vllm import LLM, SamplingParams
        from transformers import AutoTokenizer

        model_id = "neuralmagic-ent/Qwen2.5-7B-Instruct-quantized.w8a8"
        number_gpus = 1
        max_model_len = 8192

        sampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)

        tokenizer = AutoTokenizer.from_pretrained(model_id)

        prompt = "Give me a short introduction to large language model."

        llm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=max_model_len)

        outputs = llm.generate(prompt, sampling_params)

        generated_text = outputs[0].outputs[0].text
        print(generated_text)
        ```

        vLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.


        ## Evaluation

        The model was evaluated on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) leaderboard tasks (version 1) with the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/387Bbd54bc621086e05aa1b030d8d4d5635b25e6) (commit 387Bbd54bc621086e05aa1b030d8d4d5635b25e6) and the [vLLM](https://docs.vllm.ai/en/stable/) engine, using the following command:
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic-ent/Qwen2.5-7B-Instruct-quantized.w8a8",dtype=auto,gpu_memory_utilization=0.9,add_bos_token=True,max_model_len=4096,enable_chunk_prefill=True,tensor_parallel_size=1 \
          --tasks openllm \
          --batch_size auto
        ```

        ### Accuracy

        <table>
          <tr>
           <td><strong>Benchmark</strong>
           </td>
           <td><strong>Qwen2.5-7B-Instruct</strong>
           </td>
           <td><strong>Qwen2.5-7B-Instruct-quantized.w8a8 (this model)</strong>
           </td>
           <td><strong>Recovery</strong>
           </td>
          </tr>
          <tr>
           <td rowspan="7" ><strong>OpenLLM v1</strong>
           </td>
           <td>MMLU (5-shot)
           </td>
           <td>74.24
           </td>
           <td>73.84
           </td>
           <td>99.5%
           </td>
          </tr>
          <tr>
           <td>ARC Challenge (25-shot)
           </td>
           <td>63.40
           </td>
           <td>63.23
           </td>
           <td>99.7%
           </td>
          </tr>
          <tr>
           <td>GSM-8K (5-shot, strict-match)
           </td>
           <td>80.36
           </td>
           <td>80.74
           </td>
           <td>100.5%
           </td>
          </tr>
          <tr>
           <td>Hellaswag (10-shot)
           </td>
           <td>81.52
           </td>
           <td>81.06
           </td>
           <td>99.4%
           </td>
          </tr>
          <tr>
           <td>Winogrande (5-shot)
           </td>
           <td>74.66
           </td>
           <td>74.82
           </td>
           <td>100.2%
           </td>
          </tr>
          <tr>
           <td>TruthfulQA (0-shot, mc2)
           </td>
           <td>64.76
           </td>
           <td>64.58
           </td>
           <td>99.7%
           </td>
          </tr>
          <tr>
           <td><strong>Average</strong>
           </td>
           <td><strong>73.16</strong>
           </td>
           <td><strong>73.05</strong>
           </td>
           <td><strong>99.9%</strong>
           </td>
          </tr>
          <tr>
           <td rowspan="7" ><strong>OpenLLM v2</strong>
           </td>
           <td>MMLU-Pro (5-shot)
           </td>
           <td>42.93
           </td>
           <td>42.40
           </td>
           <td>98.8%
           </td>
          </tr>
          <tr>
           <td>IFEval (0-shot)
           </td>
           <td>76.25
           </td>
           <td>75.30
           </td>
           <td>98.8%
           </td>
          </tr>
          <tr>
           <td>BBH (3-shot)
           </td>
           <td>55.56
           </td>
           <td>55.03
           </td>
           <td>99.1%
           </td>
          </tr>
          <tr>
           <td>Math-lvl-5 (4-shot)
           </td>
           <td>0.00
           </td>
           <td>0.00
           </td>
           <td>***
           </td>
          </tr>
          <tr>
           <td>GPQA (0-shot)
           </td>
           <td>33.07
           </td>
           <td>33.74
           </td>
           <td>102.3%
           </td>
          </tr>
          <tr>
           <td>MuSR (0-shot)
           </td>
           <td>40.60
           </td>
           <td>42.18
           </td>
           <td>103.9%
           </td>
          </tr>
          <tr>
           <td><strong>Average</strong>
           </td>
           <td><strong>41.40</strong>
           </td>
           <td><strong>41.44</strong>
           </td>
           <td><strong>100.1%</strong>
           </td>
          </tr>
        </table>
        *** Reference value too low to report meaningful recovery.
      language:
        - zh
        - en
        - fr
        - es
        - pt
        - de
        - it
        - ru
        - ja
        - ko
        - vi
        - th
        - ar
        - id
        - tr
        - nl
        - pl
        - cs
        - he
        - sv
        - fi
        - da
        - "no"
        - ms
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-generation
        - text-generation
        - question-answering
      createTimeSinceEpoch: "1728432000"
      lastUpdateTimeSinceEpoch: "1728432000"
      customProperties:
        8-bit:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        featured:
            metadataType: MetadataStringValue
            string_value: ""
        int8:
            metadataType: MetadataStringValue
            string_value: ""
        qwen:
            metadataType: MetadataStringValue
            string_value: ""
        qwen2:
            metadataType: MetadataStringValue
            string_value: ""
        qwen2_5:
            metadataType: MetadataStringValue
            string_value: ""
        qwen2_5_instruct:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
        w8a8:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-qwen2-5-7b-instruct-quantized-w8a8:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744384464000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/gemma-2-9b-it
      provider: Google
      description: 'Model Page**: [Gemma](https://ai.google.dev/gemma/docs)'
      readme: "# Gemma 2 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs)\n\n**Resources and Technical Documentation**:\n\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma]\n\n**Terms of Use**: [Terms](https://www.kaggle.com/models/google/gemma/license/consent/verify/huggingface?returnModelRepoId=google/gemma-2-9b-it)\n\n**Authors**: Google\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights for both pre-trained variants and instruction-tuned variants.\nGemma models are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.\n\n### Usage\n\nBelow we share some code snippets on how to get quickly started with running the model. First, install the Transformers library with:\n```sh\npip install -U transformers\n```\n\nThen, copy the snippet from the section that is relevant for your usecase.\n\n#### Running with the `pipeline` API\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    \"text-generation\",\n    model=\"google/gemma-2-9b-it\",\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device=\"cuda\",  # replace with \"mps\" to run on a Mac device\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n]\n\noutputs = pipe(messages, max_new_tokens=256)\nassistant_response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\nprint(assistant_response)\n# Ahoy, matey! I be Gemma, a digital scallywag, a language-slingin' parrot of the digital seas. I be here to help ye with yer wordy woes, answer yer questions, and spin ye yarns of the digital world.  So, what be yer pleasure, eh? \U0001F99C\n```\n\n#### Running the model on a single / multi GPU\n\n```python\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2-9b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\n```\n\nYou can ensure the correct chat template is applied by using `tokenizer.apply_chat_template` as follows:\n```python\nmessages = [\n    {\"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\"},\n]\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True).to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=256)\nprint(tokenizer.decode(outputs[0]))\n```\n\n<a name=\"precisions\"></a>\n#### Running the model on a GPU using different precisions\n\nThe native weights of this model were exported in `bfloat16` precision.\n\nYou can also use `float32` if you skip the dtype, but no precision increase will occur (model weights will just be upcasted to `float32`). See examples below.\n\n* _Upcasting to `torch.float32`_\n\n```python\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2-9b-it\",\n    device_map=\"auto\",\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\n```\n\n#### Running the model through a CLI\n\nThe [local-gemma](https://github.com/huggingface/local-gemma) repository contains a lightweight wrapper around Transformers\nfor running Gemma 2 through a command line interface, or CLI. Follow the [installation instructions](https://github.com/huggingface/local-gemma#cli-usage)\nfor getting started, then launch the CLI through the following command:\n\n```shell\nlocal-gemma --model 9b --preset speed\n```\n\n#### Quantized Versions through `bitsandbytes`\n\n<details>\n  <summary>\n    Using 8-bit precision (int8)  \n  </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2-9b-it\",\n    quantization_config=quantization_config,\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\n```\n</details>\n\n<details>\n  <summary>\n    Using 4-bit precision  \n  </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2-9b-it\",\n    quantization_config=quantization_config,\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\n```\n</details>\n\n#### Advanced Usage\n\n<details>\n  <summary>\n    Torch compile  \n  </summary>\n\n[Torch compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) is a method for speeding-up the \ninference of PyTorch modules. The Gemma-2 model can be run up to 6x faster by leveraging torch compile.\n\nNote that two warm-up steps are required before the full inference speed is realised:\n\n```python\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom transformers import AutoTokenizer, Gemma2ForCausalLM\nfrom transformers.cache_utils import HybridCache\nimport torch\n\ntorch.set_float32_matmul_precision(\"high\")\n\n# load the model + tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\nmodel = Gemma2ForCausalLM.from_pretrained(\"google/gemma-2-9b-it\", torch_dtype=torch.bfloat16)\nmodel.to(\"cuda\")\n\n# apply the torch compile transformation\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n\n# pre-process inputs\ninput_text = \"The theory of special relativity states \"\nmodel_inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs.input_ids.shape[1]\n\n# set-up k/v cache\npast_key_values = HybridCache(\n    config=model.config,\n    max_batch_size=1,\n    max_cache_len=model.config.max_position_embeddings,\n    device=model.device,\n    dtype=model.dtype\n)\n\n# enable passing kv cache to generate\nmodel._supports_cache_class = True\nmodel.generation_config.cache_implementation = None\n\n# two warm-up steps\nfor idx in range(2):\n    outputs = model.generate(**model_inputs, past_key_values=past_key_values, do_sample=True, temperature=1.0, max_new_tokens=128)\n    past_key_values.reset()\n\n# fast run\noutputs = model.generate(**model_inputs, past_key_values=past_key_values, do_sample=True, temperature=1.0, max_new_tokens=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nFor more details, refer to the [Transformers documentation](https://huggingface.co/docs/transformers/main/en/llm_optims?static-kv=basic+usage%3A+generation_config).\n\n</details>\n\n### Chat Template\n\nThe instruction-tuned models use a chat template that must be adhered to for conversational use.\nThe easiest way to apply it is using the tokenizer's built-in chat template, as shown in the following snippet.\n\nLet's load the model and apply the chat template to a conversation. In this example, we'll start with a single user interaction:\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel_id = \"google/gemma-2-9b-it\"\ndtype = torch.bfloat16\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"cuda\",\n    torch_dtype=dtype,)\n\nchat = [\n    { \"role\": \"user\", \"content\": \"Write a hello world program\" },\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n```\n\nAt this point, the prompt contains the following text:\n\n```\n<bos><start_of_turn>user\nWrite a hello world program<end_of_turn>\n<start_of_turn>model\n```\n\nAs you can see, each turn is preceded by a `<start_of_turn>` delimiter and then the role of the entity\n(either `user`, for content supplied by the user, or `model` for LLM responses). Turns finish with\nthe `<end_of_turn>` token.\n\nYou can follow this format to build the prompt manually, if you need to do it without the tokenizer's\nchat template.\n\nAfter the prompt is ready, generation can be performed like this:\n\n```py\ninputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\nprint(tokenizer.decode(outputs[0]))\n```\n\n### Inputs and outputs\n\n*   **Input:** Text string, such as a question, a prompt, or a document to be\n    summarized.\n*   **Output:** Generated English-language text in response to the input, such\n    as an answer to a question, or a summary of a document.\n\n### Citation\n\n```none\n@article{gemma_2024,\n    title={Gemma},\n    url={https://www.kaggle.com/m/3301},\n    DOI={10.34740/KAGGLE/M/3301},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2024}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 13 trillion tokens and the 9B model was trained with 8 trillion tokens. \nHere are the key components:\n\n* Web Documents: A diverse collection of web text ensures the model is exposed\n  to a broad range of linguistic styles, topics, and vocabulary. Primarily\n  English-language content.\n* Code: Exposing the model to code helps it to learn the syntax and patterns of\n  programming languages, which improves its ability to generate code or\n  understand code-related questions.\n* Mathematics: Training on mathematical text helps the model learn logical\n  reasoning, symbolic representation, and to address mathematical queries.\n\nThe combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n* CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\n  applied at multiple stages in the data preparation process to ensure the\n  exclusion of harmful and illegal content.\n* Sensitive Data Filtering: As part of making Gemma pre-trained models safe and\n  reliable, automated techniques were used to filter out certain personal\n  information and other sensitive data from training sets.\n* Additional methods: Filtering based on content quality and safety in line with\n  [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using the latest generation of\n[Tensor Processing Unit (TPU)][tpu] hardware (TPUv5p).\n\nTraining large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:\n\n* Performance: TPUs are specifically designed to handle the massive computations\n  involved in training LLMs. They can speed up training considerably compared to\n  CPUs.\n* Memory: TPUs often come with large amounts of high-bandwidth memory, allowing\n  for the handling of large models and batch sizes during training. This can\n  lead to better model quality.\n* Scalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\n  handling the growing complexity of large foundation models. You can distribute\n  training across multiple TPU devices for faster and more efficient processing.\n* Cost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\n  solution for training large models compared to CPU-based infrastructure,\n  especially when considering the time and resources saved due to faster\n  training.\n* These advantages are aligned with\n  [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\n\nML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\n[foundation models][foundation-models], including large language models like\nthese ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n| Benchmark                      | Metric        | Gemma PT 9B | Gemma PT 27B |\n| ------------------------------ | ------------- | ----------- | ------------ |\n| [MMLU][mmlu]                   | 5-shot, top-1 | 71.3        | 75.2         |\n| [HellaSwag][hellaswag]         | 10-shot       | 81.9        | 86.4         |\n| [PIQA][piqa]                   | 0-shot        | 81.7        | 83.2         |\n| [SocialIQA][socialiqa]         | 0-shot        | 53.4        | 53.7         |\n| [BoolQ][boolq]                 | 0-shot        | 84.2        | 84.8         |\n| [WinoGrande][winogrande]       | partial score | 80.6        | 83.7         |\n| [ARC-e][arc]                   | 0-shot        | 88.0        | 88.6         |\n| [ARC-c][arc]                   | 25-shot       | 68.4        | 71.4         |\n| [TriviaQA][triviaqa]           | 5-shot        | 76.6        | 83.7         |\n| [Natural Questions][naturalq]  | 5-shot        | 29.2        | 34.5         |\n| [HumanEval][humaneval]         | pass@1        | 40.2        | 51.8         |\n| [MBPP][mbpp]                   | 3-shot        | 52.4        | 62.6         |\n| [GSM8K][gsm8k]                 | 5-shot, maj@1 | 68.6        | 74.0         |\n| [MATH][math]                   | 4-shot        | 36.6        | 42.3         |\n| [AGIEval][agieval]             | 3-5-shot      | 52.8        | 55.1         |\n| [BIG-Bench][big-bench]         | 3-shot, CoT   | 68.2        | 74.9         |\n| ------------------------------ | ------------- | ----------- | ------------ |\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n* Text-to-Text Content Safety: Human evaluation on prompts covering safety\n  policies including child sexual abuse and exploitation, harassment, violence\n  and gore, and hate speech.\n* Text-to-Text Representational Harms: Benchmark against relevant academic\n  datasets such as [WinoBias][winobias] and [BBQ Dataset][bbq].\n* Memorization: Automated evaluation of memorization of training data, including\n  the risk of personally identifiable information exposure.\n* Large-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\n  biological, radiological, and nuclear (CBRN) risks.\n\n### Evaluation Results\n\nThe results of ethics and safety evaluations are within acceptable thresholds\nfor meeting [internal policies][safety-policies] for categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well-known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.\n\n#### Gemma 2.0\n\n| Benchmark                | Metric        | Gemma 2 IT 9B | Gemma 2 IT 27B |\n| ------------------------ | ------------- | --------------- | ---------------- |\n| [RealToxicity][realtox]  | average       |  8.25           |  8.84            |\n| [CrowS-Pairs][crows]     | top-1         | 37.47           | 36.67            |\n| [BBQ Ambig][bbq]         | 1-shot, top-1 | 88.58           | 85.99            |\n| [BBQ Disambig][bbq]      | top-1         | 82.67           | 86.94            |\n| [Winogender][winogender] | top-1         | 79.17           | 77.22            |\n| [TruthfulQA][truthfulqa] |               | 50.27           | 51.60            |\n| [Winobias 1_2][winobias] |               | 78.09           | 81.94            |\n| [Winobias 2_2][winobias] |               | 95.32           | 97.22            |\n| [Toxigen][toxigen]       |               | 39.30           | 38.42            |\n| ------------------------ | ------------- | --------------- | ---------------- |\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n* Content Creation and Communication\n  * Text Generation: These models can be used to generate creative text formats\n    such as poems, scripts, code, marketing copy, and email drafts.\n  * Chatbots and Conversational AI: Power conversational interfaces for customer\n    service, virtual assistants, or interactive applications.\n  * Text Summarization: Generate concise summaries of a text corpus, research\n    papers, or reports.\n* Research and Education\n  * Natural Language Processing (NLP) Research: These models can serve as a\n    foundation for researchers to experiment with NLP techniques, develop\n    algorithms, and contribute to the advancement of the field.\n  * Language Learning Tools: Support interactive language learning experiences,\n    aiding in grammar correction or providing writing practice.\n  * Knowledge Exploration: Assist researchers in exploring large bodies of text\n    by generating summaries or answering questions about specific topics.\n\n### Limitations\n\n* Training Data\n  * The quality and diversity of the training data significantly influence the\n    model's capabilities. Biases or gaps in the training data can lead to\n    limitations in the model's responses.\n  * The scope of the training dataset determines the subject areas the model can\n    handle effectively.\n* Context and Task Complexity\n  * LLMs are better at tasks that can be framed with clear prompts and\n    instructions. Open-ended or highly complex tasks might be challenging.\n  * A model's performance can be influenced by the amount of context provided\n    (longer context generally leads to better outputs, up to a certain point).\n* Language Ambiguity and Nuance\n  * Natural language is inherently complex. LLMs might struggle to grasp subtle\n    nuances, sarcasm, or figurative language.\n* Factual Accuracy\n  * LLMs generate responses based on information they learned from their\n    training datasets, but they are not knowledge bases. They may generate\n    incorrect or outdated factual statements.\n* Common Sense\n  * LLMs rely on statistical patterns in language. They might lack the ability\n    to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\n\n* Bias and Fairness\n  * LLMs trained on large-scale, real-world text data can reflect socio-cultural\n    biases embedded in the training material. These models underwent careful\n    scrutiny, input data pre-processing described and posterior evaluations\n    reported in this card.\n* Misinformation and Misuse\n  * LLMs can be misused to generate text that is false, misleading, or harmful.\n  * Guidelines are provided for responsible use with the model, see the\n    [Responsible Generative AI Toolkit][rai-toolkit].\n* Transparency and Accountability:\n  * This model card summarizes details on the models' architecture,\n    capabilities, limitations, and evaluation processes.\n  * A responsibly developed open model offers the opportunity to share\n    innovation by making LLM technology accessible to developers and researchers\n    across the AI ecosystem.\n\nRisks identified and mitigations:\n\n* Perpetuation of biases: It's encouraged to perform continuous monitoring\n  (using evaluation metrics, human review) and the exploration of de-biasing\n  techniques during model training, fine-tuning, and other use cases.\n* Generation of harmful content: Mechanisms and guidelines for content safety\n  are essential. Developers are encouraged to exercise caution and implement\n  appropriate content safety safeguards based on their specific product policies\n  and application use cases.\n* Misuse for malicious purposes: Technical limitations and developer and\n  end-user education can help mitigate against malicious applications of LLMs.\n  Educational resources and reporting mechanisms for users to flag misuse are\n  provided. Prohibited uses of Gemma models are outlined in the\n  [Gemma Prohibited Use Policy][prohibited-use].\n* Privacy violations: Models were trained on data filtered for removal of PII\n  (Personally Identifiable Information). Developers are encouraged to adhere to\n  privacy regulations with privacy-preserving techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n\n[rai-toolkit]: https://ai.google.dev/responsible\n[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-2\n[terms]: https://ai.google.dev/gemma/terms\n[vertex-mg-gemma]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335\n[sensitive-info]: https://cloud.google.com/dlp/docs/high-sensitivity-infotypes-reference\n[safety-policies]: https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11\n[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy\n[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu\n[sustainability]: https://sustainability.google/operating-sustainably/\n[jax]: https://github.com/google/jax\n[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\n[sustainability]: https://sustainability.google/operating-sustainably/\n[foundation-models]: https://ai.google/discover/foundation-models/\n[gemini-2-paper]: https://goo.gle/gemma2report\n[mmlu]: https://arxiv.org/abs/2009.03300\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[boolq]: https://arxiv.org/abs/1905.10044\n[winogrande]: https://arxiv.org/abs/1907.10641\n[commonsenseqa]: https://arxiv.org/abs/1811.00937\n[openbookqa]: https://arxiv.org/abs/1809.02789\n[arc]: https://arxiv.org/abs/1911.01547\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[humaneval]: https://arxiv.org/abs/2107.03374\n[mbpp]: https://arxiv.org/abs/2108.07732\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[realtox]: https://arxiv.org/abs/2009.11462\n[bold]: https://arxiv.org/abs/2101.11718\n[crows]: https://aclanthology.org/2020.emnlp-main.154/\n[bbq]: https://arxiv.org/abs/2110.08193v2\n[winogender]: https://arxiv.org/abs/1804.09301\n[truthfulqa]: https://arxiv.org/abs/2109.07958\n[winobias]: https://arxiv.org/abs/1804.06876\n[math]: https://arxiv.org/abs/2103.03874\n[agieval]: https://arxiv.org/abs/2304.06364\n[big-bench]: https://arxiv.org/abs/2206.04615\n[toxigen]: https://arxiv.org/abs/2203.09509\n"
      language:
        - en
      license: gemma
      licenseLink: https://ai.google.dev/gemma/terms
      tasks:
        - text-generation
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1747669712000"
      customProperties:
        gemma:
            metadataType: MetadataStringValue
            string_value: ""
        gemma2:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-gemma-2-9b-it:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1747669712000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/gemma-2-9b-it-FP8
      provider: Neural Magic
      description: Gemma 2 9b It Fp8 - A large language model
      readme: |-
        # gemma-2-9b-it-FP8

        ## Model Overview
        - **Model Architecture:** Gemma 2
          - **Input:** Text
          - **Output:** Text
        - **Model Optimizations:**
          - **Weight quantization:** FP8
          - **Activation quantization:** FP8
        - **Intended Use Cases:** Intended for commercial and research use in English. Similarly to [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), this models is intended for assistant-like chat.
        - **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English.
        - **Release Date:** 7/8/2024
        - **Version:** 1.0
        - **License(s):** [gemma](https://ai.google.dev/gemma/terms)
        - **Model Developers:** Neural Magic

        Quantized version of [gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it).
        It achieves an average score of 73.49 on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) benchmark (version 1), whereas the unquantized model achieves 73.23.

        ### Model Optimizations

        This model was obtained by quantizing the weights and activations of [gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it) to FP8 data type, ready for inference with vLLM >= 0.5.1.
        This optimization reduces the number of bits per parameter from 16 to 8, reducing the disk size and GPU memory requirements by approximately 50%.

        Only the weights and activations of the linear operators within transformers blocks are quantized. Symmetric per-tensor quantization is applied, in which a single linear scaling maps the FP8 representations of the quantized weights and activations.
        [AutoFP8](https://github.com/neuralmagic/AutoFP8) is used for quantization with a single instance of every token in random order.

        ## Deployment

        ### Use with vLLM

        This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

        ```python
        from vllm import LLM, SamplingParams
        from transformers import AutoTokenizer

        model_id = "neuralmagic/gemma-2-9b-it-FP8"

        sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)

        tokenizer = AutoTokenizer.from_pretrained(model_id)

        messages = [
            {"role": "user", "content": "Who are you? Please respond in pirate speak!"},
        ]

        prompts = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        llm = LLM(model=model_id)

        outputs = llm.generate(prompts, sampling_params)

        generated_text = outputs[0].outputs[0].text
        print(generated_text)
        ```

        vLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.

        ## Creation

        This model was created by applying [AutoFP8 with calibration samples from ultrachat](https://github.com/neuralmagic/AutoFP8/blob/147fa4d9e1a90ef8a93f96fc7d9c33056ddc017a/example_dataset.py), as presented in the code snipet below.
        Although AutoFP8 was used for this particular model, Neural Magic is transitioning to using [llm-compressor](https://github.com/vllm-project/llm-compressor) which supports several quantization schemes and models not supported by AutoFP8.

        ```python
        from datasets import load_dataset
        from transformers import AutoTokenizer
        import numpy as np
        import torch

        from auto_fp8 import AutoFP8ForCausalLM, BaseQuantizeConfig

        MODEL_DIR = "google/gemma-2-9b-it"
        final_model_dir = MODEL_DIR.split("/")[-1]

        CONTEXT_LENGTH = 4096
        NUM_SAMPLES = 512
        NUM_REPEATS = 1

        pretrained_model_dir = MODEL_DIR
        tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True, model_max_length=CONTEXT_LENGTH)
        tokenizer.pad_token = tokenizer.eos_token

        tokenizer_num_tokens = len(list(tokenizer.get_vocab().values()))
        total_token_samples = NUM_REPEATS * tokenizer_num_tokens
        num_random_samp = -(-total_token_samples // CONTEXT_LENGTH)

        input_ids = np.tile(np.arange(tokenizer_num_tokens), NUM_REPEATS + 1)[:num_random_samp * CONTEXT_LENGTH]
        np.random.shuffle(input_ids)
        input_ids = input_ids.reshape(num_random_samp, CONTEXT_LENGTH)
        input_ids = torch.tensor(input_ids, dtype=torch.int64).to("cuda")

        quantize_config = BaseQuantizeConfig(
            quant_method="fp8",
            activation_scheme="static",
        )

        examples = input_ids

        model = AutoFP8ForCausalLM.from_pretrained(pretrained_model_dir, quantize_config=quantize_config)

        model.quantize(examples)

        quantized_model_dir = f"{final_model_dir}-FP8"
        model.save_quantized(quantized_model_dir)
        ```

        ## Evaluation

        The model was evaluated on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) leaderboard tasks (version 1) with the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/383bbd54bc621086e05aa1b030d8d4d5635b25e6) (commit 383bbd54bc621086e05aa1b030d8d4d5635b25e6) and the [vLLM](https://docs.vllm.ai/en/stable/) engine, using the following command:
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic/gemma-2-9b-it-FP8",dtype=auto,gpu_memory_utilization=0.4,add_bos_token=True,max_model_len=4096 \
          --tasks openllm \
          --batch_size auto
        ```

        ### Accuracy

        #### Open LLM Leaderboard evaluation scores
        <table>
          <tr>
           <td><strong>Benchmark</strong>
           </td>
           <td><strong>gemma-2-9b-it</strong>
           </td>
           <td><strong>gemma-2-9b-it-FP8(this model)</strong>
           </td>
           <td><strong>Recovery</strong>
           </td>
          </tr>
          <tr>
           <td>MMLU (5-shot)
           </td>
           <td>72.28
           </td>
           <td>71.99
           </td>
           <td>99.59%
           </td>
          </tr>
          <tr>
           <td>ARC Challenge (25-shot)
           </td>
           <td>71.50
           </td>
           <td>71.50
           </td>
           <td>100.0%
           </td>
          </tr>
          <tr>
           <td>GSM-8K (5-shot, strict-match)
           </td>
           <td>76.26
           </td>
           <td>76.87
           </td>
           <td>100.7%
           </td>
          </tr>
          <tr>
           <td>Hellaswag (10-shot)
           </td>
           <td>81.91
           </td>
           <td>81.70
           </td>
           <td>99.74%
           </td>
          </tr>
          <tr>
           <td>Winogrande (5-shot)
           </td>
           <td>77.11
           </td>
           <td>78.37
           </td>
           <td>101.6%
           </td>
          </tr>
          <tr>
           <td>TruthfulQA (0-shot)
           </td>
           <td>60.32
           </td>
           <td>60.52
           </td>
           <td>100.3%
           </td>
          </tr>
          <tr>
           <td><strong>Average</strong>
           </td>
           <td><strong>73.23</strong>
           </td>
           <td><strong>73.49</strong>
           </td>
           <td><strong>100.36%</strong>
           </td>
          </tr>
        </table>
      language:
        - en
      license: gemma
      licenseLink: https://ai.google.dev/gemma/terms
      tasks:
        - text-generation
      createTimeSinceEpoch: "1720396800"
      lastUpdateTimeSinceEpoch: "1720396800"
      customProperties:
        autotrain_compatible:
            metadataType: MetadataStringValue
            string_value: ""
        endpoints_compatible:
            metadataType: MetadataStringValue
            string_value: ""
        fp8:
            metadataType: MetadataStringValue
            string_value: ""
        gemma:
            metadataType: MetadataStringValue
            string_value: ""
        gemma2:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        transformers:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-gemma-2-9b-it-fp8:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1747669323000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/granite-3.1-8b-base-quantized.w4a16
      provider: Neural Magic
      description: Granite 3.1 8b Base Quantized.w4a16 - A foundation language model
      readme: "# granite-3.1-8b-base-quantized.w4a16\n\n## Model Overview\n- **Model Architecture:** granite-3.1-8b-base\n  - **Input:** Text\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Weight quantization:** INT4\n  - **Activation quantization:** INT4\n- **Release Date:** 1/8/2025\n- **Version:** 1.0\n- **Model Developers:** Neural Magic\n\nQuantized version of [ibm-granite/granite-3.1-8b-base](https://huggingface.co/ibm-granite/granite-3.1-8b-base).\nIt achieves an average score of 69.81 on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) benchmark (version 1), whereas the unquantized model achieves 70.30.\n\n### Model Optimizations\n\nThis model was obtained by quantizing the weights of [ibm-granite/granite-3.1-8b-base](https://huggingface.co/ibm-granite/granite-3.1-8b-base) to INT4 data type, ready for inference with vLLM >= 0.5.2.\nThis optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 75%. Only the weights of the linear operators within transformers blocks are quantized. \n\n## Deployment\n\n### Use with vLLM\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\nmax_model_len, tp_size = 4096, 1\nmodel_name = \"neuralmagic/granite-3.1-8b-base-quantized.w4a16\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\n\nmessages_list = [\n    [{\"role\": \"user\", \"content\": \"Who are you? Please respond in pirate speak!\"}],\n]\n\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\n\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n```\n\nvLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\nThis model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n\n<details>\n  <summary>Model Creation Code</summary>\n\n```bash\npython quantize.py --model_path ibm-granite/granite-3.1-8b-base --quant_path \"output_dir/granite-3.1-8b-base-quantized.w4a16\" --calib_size 3072 --dampening_frac 0.1 --observer mse --actorder static\n```\n\n\n```python\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom llmcompressor.modifiers.quantization import GPTQModifier\nfrom llmcompressor.transformers import SparseAutoModelForCausalLM, oneshot, apply\nimport argparse\nfrom compressed_tensors.quantization import QuantizationScheme, QuantizationArgs, QuantizationType, QuantizationStrategy\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--model_path', type=str)\nparser.add_argument('--quant_path', type=str)\nparser.add_argument('--calib_size', type=int, default=256)\nparser.add_argument('--dampening_frac', type=float, default=0.1) \nparser.add_argument('--observer', type=str, default=\"minmax\")\nparser.add_argument('--actorder', type=str, default=\"dynamic\")\n\nargs = parser.parse_args()\n\nmodel = SparseAutoModelForCausalLM.from_pretrained(\n    args.model_path,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    use_cache=False,\n    trust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(args.model_path)\n\n\nNUM_CALIBRATION_SAMPLES = args.calib_size\nDATASET_ID = \"neuralmagic/LLM_compression_calibration\"\nDATASET_SPLIT = \"train\"\nds = load_dataset(DATASET_ID, split=DATASET_SPLIT)\nds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n\ndef preprocess(example):\n    return {\"text\": example[\"text\"]}\n\nds = ds.map(preprocess)\n\ndef tokenize(sample):\n    return tokenizer(\n        sample[\"text\"],\n        padding=False,\n        truncation=False,\n        add_special_tokens=True,\n    )\n\n\nds = ds.map(tokenize, remove_columns=ds.column_names)\n\nrecipe = [\n    GPTQModifier(\n        targets=[\"Linear\"],\n        ignore=[\"lm_head\"],\n        scheme=\"w4a16\",\n        dampening_frac=args.dampening_frac,\n        observer=args.observer,\n        actorder=args.actorder,\n    )\n]\noneshot(\n    model=model,\n    dataset=ds,\n    recipe=recipe,\n    num_calibration_samples=args.calib_size,\n    max_seq_length=8196,\n)\n\n# Save to disk compressed.\nmodel.save_pretrained(quant_path, save_compressed=True)\ntokenizer.save_pretrained(quant_path)\n```\n</details>\n\n## Evaluation\n\nThe model was evaluated on OpenLLM Leaderboard [V1](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard), OpenLLM Leaderboard [V2](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/) and on [HumanEval](https://github.com/neuralmagic/evalplus), using the following commands:\n\n<details>\n<summary>Evaluation Commands</summary>\n\nOpenLLM Leaderboard V1:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/granite-3.1-8b-base-quantized.w4a16\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \\\n  --tasks openllm \\\n  --write_out \\\n  --batch_size auto \\\n  --output_path output_dir \\\n  --show_config\n```\n\nOpenLLM Leaderboard V2:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/granite-3.1-8b-base-quantized.w4a16\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \\\n  --tasks leaderboard \\\n  --write_out \\\n  --batch_size auto \\\n  --output_path output_dir \\\n  --show_config\n```\n\n#### HumanEval\n##### Generation\n```\npython3 codegen/generate.py \\\n  --model neuralmagic/granite-3.1-8b-base-quantized.w4a16 \\\n  --bs 16 \\\n  --temperature 0.2 \\\n  --n_samples 50 \\\n  --root \".\" \\\n  --dataset humaneval\n```\n##### Sanitization\n```\npython3 evalplus/sanitize.py \\\n  humaneval/neuralmagic--granite-3.1-8b-base-quantized.w4a16_vllm_temp_0.2\n```\n##### Evaluation\n```\nevalplus.evaluate \\\n  --dataset humaneval \\\n  --samples humaneval/neuralmagic--granite-3.1-8b-base-quantized.w4a16_vllm_temp_0.2-sanitized\n```\n</details>\n\n### Accuracy\n\n<table>\n  <thead>\n    <tr>\n      <th>Category</th>\n      <th>Metric</th>\n      <th>ibm-granite/granite-3.1-8b-base</th>\n      <th>neuralmagic/granite-3.1-8b-base-quantized.w4a16</th>\n      <th>Recovery (%)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan=\"7\"><b>OpenLLM V1</b></td>\n      <td>ARC-Challenge (Acc-Norm, 25-shot)</td>\n      <td>64.68</td>\n      <td>62.37</td>\n      <td>96.43</td>\n    </tr>\n    <tr>\n      <td>GSM8K (Strict-Match, 5-shot)</td>\n      <td>60.88</td>\n      <td>54.89</td>\n      <td>90.16</td>\n    </tr>\n    <tr>\n      <td>HellaSwag (Acc-Norm, 10-shot)</td>\n      <td>83.52</td>\n      <td>82.53</td>\n      <td>98.81</td>\n    </tr>\n    <tr>\n      <td>MMLU (Acc, 5-shot)</td>\n      <td>63.33</td>\n      <td>62.78</td>\n      <td>99.13</td>\n    </tr>\n    <tr>\n      <td>TruthfulQA (MC2, 0-shot)</td>\n      <td>51.33</td>\n      <td>51.30</td>\n      <td>99.94</td>\n    </tr>\n    <tr>\n      <td>Winogrande (Acc, 5-shot)</td>\n      <td>80.90</td>\n      <td>79.24</td>\n      <td>97.95</td>\n    </tr>\n    <tr>\n      <td><b>Average Score</b></td>\n      <td><b>67.44</b></td>\n      <td><b>65.52</b></td>\n      <td><b>97.15</b></td>\n    </tr>\n    <tr>\n      <td rowspan=\"2\"><b>Coding</b></td>\n      <td>HumanEval Pass@1</td>\n      <td>44.10</td>\n      <td>40.70</td>\n      <td><b>92.28</b></td>\n    </tr>\n  </tbody>\n</table>\n\n\n---\n\n\n## Inference Performance\n\n\nThis model achieves up to 2.7x speedup in single-stream deployment and up to 1.5x speedup in multi-stream asynchronous deployment, depending on hardware and use-case scenario.\nThe following performance benchmarks were conducted with [vLLM](https://docs.vllm.ai/en/latest/) version 0.6.6.post1, and [GuideLLM](https://github.com/neuralmagic/guidellm).\n\n<details>\n<summary>Benchmarking Command</summary>\n\n```\nguidellm --model neuralmagic/granite-3.1-8b-base-quantized.w4a16 --target \"http://localhost:8000/v1\" --data-type emulated --data \"prompt_tokens=<prompt_tokens>,generated_tokens=<generated_tokens>\" --max seconds 360 --backend aiohttp_server\n```\n\n</details>\n\n### Single-stream performance (measured with vLLM version 0.6.6.post1)\n<table>\n  <tr>\n    <td></td>\n    <td></td>\n    <td></td>\n    <th style=\"text-align: center;\" colspan=\"7\" >Latency (s)</th>\n  </tr>\n  <tr>\n    <th>GPU class</th>\n    <th>Model</th>\n    <th>Speedup</th>\n    <th>Code Completion<br>prefill: 256 tokens<br>decode: 1024 tokens</th>\n    <th>Docstring Generation<br>prefill: 768 tokens<br>decode: 128 tokens</th>\n    <th>Code Fixing<br>prefill: 1024 tokens<br>decode: 1024 tokens</th>\n    <th>RAG<br>prefill: 1024 tokens<br>decode: 128 tokens</th>\n    <th>Instruction Following<br>prefill: 256 tokens<br>decode: 128 tokens</th>\n    <th>Multi-turn Chat<br>prefill: 512 tokens<br>decode: 256 tokens</th>\n    <th>Large Summarization<br>prefill: 4096 tokens<br>decode: 512 tokens</th>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A5000</td>\n    <td>granite-3.1-8b-base</td>\n    <td></td>\n    <td>28.3</td>\n    <td>3.7</td>\n    <td>28.8</td>\n    <td>3.8</td>\n    <td>3.6</td>\n    <td>7.2</td>\n    <td>15.7</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w8a8</td>\n    <td>1.60</td>\n    <td>17.7</td>\n    <td>2.3</td>\n    <td>18.0</td>\n    <td>2.4</td>\n    <td>2.2</td>\n    <td>4.5</td>\n    <td>10.0</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>\n    <td>2.61</td>\n    <td>10.3</td>\n    <td>1.5</td>\n    <td>10.7</td>\n    <td>1.5</td>\n    <td>1.3</td>\n    <td>2.7</td>\n    <td>6.6</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A6000</td>\n    <td>granite-3.1-8b-base</td>\n    <td></td>\n    <td>25.8</td>\n    <td>3.4</td>\n    <td>26.2</td>\n    <td>3.4</td>\n    <td>3.3</td>\n    <td>6.5</td>\n    <td>14.2</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w8a8</td>\n    <td>1.50</td>\n    <td>17.4</td>\n    <td>2.3</td>\n    <td>16.9</td>\n    <td>2.2</td>\n    <td>2.2</td>\n    <td>4.4</td>\n    <td>9.8</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>\n    <td>2.48</td>\n    <td>10.0</td>\n    <td>1.4</td>\n    <td>10.4</td>\n    <td>1.5</td>\n    <td>1.3</td>\n    <td>2.5</td>\n    <td>6.2</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A100</td>\n    <td>granite-3.1-8b-base</td>\n    <td></td>\n    <td>13.6</td>\n    <td>1.8</td>\n    <td>13.7</td>\n    <td>1.8</td>\n    <td>1.7</td>\n    <td>3.4</td>\n    <td>7.3</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w8a8</td>\n    <td>1.31</td>\n    <td>10.4</td>\n    <td>1.3</td>\n    <td>10.5</td>\n    <td>1.4</td>\n    <td>1.3</td>\n    <td>2.6</td>\n    <td>5.6</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>\n    <td>1.80</td>\n    <td>7.3</td>\n    <td>1.0</td>\n    <td>7.4</td>\n    <td>1.0</td>\n    <td>0.9</td>\n    <td>1.9</td>\n    <td>4.3</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >L40</td>\n    <td>granite-3.1-8b-base</td>\n    <td></td>\n    <td>25.1</td>\n    <td>3.2</td>\n    <td>25.3</td>\n    <td>3.2</td>\n    <td>3.2</td>\n    <td>6.3</td>\n    <td>13.4</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-FP8-dynamic</td>\n    <td>1.47</td>\n    <td>16.8</td>\n    <td>2.2</td>\n    <td>17.1</td>\n    <td>2.2</td>\n    <td>2.1</td>\n    <td>4.2</td>\n    <td>9.3</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>\n    <td>2.72</td>\n    <td>8.9</td>\n    <td>1.2</td>\n    <td>9.2</td>\n    <td>1.2</td>\n    <td>1.1</td>\n    <td>2.3</td>\n    <td>5.3</td>\n  </tr>\n</table>\n\n\n### Multi-stream asynchronous performance (measured with vLLM version 0.6.6.post1)\n<table>\n  <tr>\n    <td></td>\n    <td></td>\n    <td></td>\n    <th style=\"text-align: center;\" colspan=\"7\" >Maximum Throughput (Queries per Second)</th>\n  </tr>\n  <tr>\n    <th>GPU class</th>\n    <th>Model</th>\n    <th>Speedup</th>\n    <th>Code Completion<br>prefill: 256 tokens<br>decode: 1024 tokens</th>\n    <th>Docstring Generation<br>prefill: 768 tokens<br>decode: 128 tokens</th>\n    <th>Code Fixing<br>prefill: 1024 tokens<br>decode: 1024 tokens</th>\n    <th>RAG<br>prefill: 1024 tokens<br>decode: 128 tokens</th>\n    <th>Instruction Following<br>prefill: 256 tokens<br>decode: 128 tokens</th>\n    <th>Multi-turn Chat<br>prefill: 512 tokens<br>decode: 256 tokens</th>\n    <th>Large Summarization<br>prefill: 4096 tokens<br>decode: 512 tokens</th>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A5000</td>\n    <td>granite-3.1-8b-base</td>\n    <td></td>\n    <td>0.8</td>\n    <td>3.1</td>\n    <td>0.4</td>\n    <td>2.5</td>\n    <td>6.7</td>\n    <td>2.7</td>\n    <td>0.3</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w8a8</td>\n    <td>1.71</td>\n    <td>1.3</td>\n    <td>5.2</td>\n    <td>0.9</td>\n    <td>4.0</td>\n    <td>10.5</td>\n    <td>4.4</td>\n    <td>0.5</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>\n    <td>1.46</td>\n    <td>1.3</td>\n    <td>3.9</td>\n    <td>0.8</td>\n    <td>2.9</td>\n    <td>8.2</td>\n    <td>3.6</td>\n    <td>0.5</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A6000</td>\n    <td>granite-3.1-8b-base</td>\n    <td></td>\n    <td>1.3</td>\n    <td>5.1</td>\n    <td>0.9</td>\n    <td>4.0</td>\n    <td>0.3</td>\n    <td>4.3</td>\n    <td>0.6</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w8a8</td>\n    <td>1.39</td>\n    <td>1.8</td>\n    <td>7.0</td>\n    <td>1.3</td>\n    <td>5.6</td>\n    <td>14.0</td>\n    <td>6.3</td>\n    <td>0.8</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>\n    <td>1.09</td>\n    <td>1.9</td>\n    <td>4.8</td>\n    <td>1.0</td>\n    <td>3.8</td>\n    <td>10.0</td>\n    <td>5.0</td>\n    <td>0.6</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A100</td>\n    <td>granite-3.1-8b-base</td>\n    <td></td>\n    <td>3.1</td>\n    <td>10.7</td>\n    <td>2.1</td>\n    <td>8.5</td>\n    <td>20.6</td>\n    <td>9.6</td>\n    <td>1.4</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w8a8</td>\n    <td>1.23</td>\n    <td>3.8</td>\n    <td>14.2</td>\n    <td>2.1</td>\n    <td>11.4</td>\n    <td>25.9</td>\n    <td>12.1</td>\n    <td>1.7</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>\n    <td>0.96</td>\n    <td>3.4</td>\n    <td>9.0</td>\n    <td>2.6</td>\n    <td>7.2</td>\n    <td>18.0</td>\n    <td>8.8</td>\n    <td>1.3</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >L40</td>\n    <td>granite-3.1-8b-base</td>\n    <td></td>\n    <td>1.4</td>\n    <td>7.8</td>\n    <td>1.1</td>\n    <td>6.2</td>\n    <td>15.5</td>\n    <td>6.0</td>\n    <td>0.7</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-FP8-dynamic</td>\n    <td>1.12</td>\n    <td>2.1</td>\n    <td>7.4</td>\n    <td>1.3</td>\n    <td>5.9</td>\n    <td>15.3</td>\n    <td>6.9</td>\n    <td>0.8</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-base-quantized.w4a16<br>(this model)</td>\n    <td>1.29</td>\n    <td>2.4</td>\n    <td>8.9</td>\n    <td>1.4</td>\n    <td>7.1</td>\n    <td>17.8</td>\n    <td>7.8</td>\n    <td>1.0</td>\n  </tr>\n</table>\n\n\n\n\n"
      language:
        - en
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-generation
      createTimeSinceEpoch: "1736294400"
      lastUpdateTimeSinceEpoch: "1736294400"
      customProperties:
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        granite:
            metadataType: MetadataStringValue
            string_value: ""
        int4:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
        w4a16:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-base-quantized-w4a16:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744384230000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/granite-3.1-8b-instruct
      provider: Granite Team, IBM
      description: Granite 3.1 8B Instruct - An instruction-tuned language model
      readme: "# Granite-3.1-8B-Instruct\n\n**Model Summary:**\nGranite-3.1-8B-Instruct is a 8B parameter long-context instruct model finetuned from Granite-3.1-8B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems. This model is developed using a diverse set of techniques with a structured chat format, including supervised finetuning, model alignment using reinforcement learning, and model merging.\n\n- **Developers:** Granite Team, IBM\n- **GitHub Repository:** [ibm-granite/granite-3.1-language-models](https://github.com/ibm-granite/granite-3.1-language-models)\n- **Website**: [Granite Docs](https://www.ibm.com/granite/docs/)\n- **Paper:** [Granite 3.1 Language Models (coming soon)](https://huggingface.co/collections/ibm-granite/granite-31-language-models-6751dbbf2f3389bec5c6f02d) \n- **Release Date**: December 18th, 2024\n- **License:** [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n\n**Supported Languages:** \nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. Users may finetune Granite 3.1 models for languages beyond these 12 languages.\n\n**Intended Use:** \nThe model is designed to respond to general instructions and can be used to build AI assistants for multiple domains, including business applications.\n\n*Capabilities*\n* Summarization\n* Text classification\n* Text extraction\n* Question-answering\n* Retrieval Augmented Generation (RAG)\n* Code related tasks\n* Function-calling tasks\n* Multilingual dialog use cases\n* Long-context tasks including long document/meeting summarization, long document QA, etc.\n\n**Generation:** \nThis is a simple example of how to use Granite-3.1-8B-Instruct model.\n\nInstall the following libraries:\n\n```shell\npip install torch torchvision torchaudio\npip install accelerate\npip install transformers\n```\nThen, copy the snippet from the section that is relevant for your use case.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"auto\"\nmodel_path = \"ibm-granite/granite-3.1-8b-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n# drop device_map if running on CPU\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\nmodel.eval()\n# change input text as desired\nchat = [\n    { \"role\": \"user\", \"content\": \"Please list one IBM Research laboratory located in the United States. You should only output its name and location.\" },\n]\nchat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n# tokenize the text\ninput_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n# generate output tokens\noutput = model.generate(**input_tokens, \n                        max_new_tokens=100)\n# decode output tokens into text\noutput = tokenizer.batch_decode(output)\n# print output\nprint(output)\n```\n**Evaluation Results:**\n<table>\n  <caption><b>HuggingFace Open LLM Leaderboard V1</b></caption>\n<thead>\n  <tr>\n    <th style=\"text-align:left; background-color: #001d6c; color: white;\">Models</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">ARC-Challenge</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">Hellaswag</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">MMLU</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">TruthfulQA</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">Winogrande</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">GSM8K</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">Avg</th>\n  </tr></thead>\n  <tbody>\n  <tr>\n    <td style=\"text-align:left; background-color: #DAE8FF; color: black;\">Granite-3.1-8B-Instruct</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">62.62</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">84.48</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">65.34</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">66.23</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">75.37</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">73.84</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">71.31</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: #2D2D2D;\">Granite-3.1-2B-Instruct</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">54.61</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">75.14</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">55.31</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">59.42</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">67.48</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">52.76</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">60.79</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: #2D2D2D;\">Granite-3.1-3B-A800M-Instruct</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">50.42</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">73.01</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">52.19</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">49.71</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">64.87</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">48.97</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">56.53</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: #2D2D2D;\">Granite-3.1-1B-A400M-Instruct</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">42.66</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">65.97</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">26.13</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">46.77</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">62.35</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">33.88</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">46.29</td>\n  </tr>\n</tbody></table>\n\n<table>\n  <caption><b>HuggingFace Open LLM Leaderboard V2</b></caption>\n<thead>\n  <tr>\n    <th style=\"text-align:left; background-color: #001d6c; color: white;\">Models</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">IFEval</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">BBH</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">MATH Lvl 5</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">GPQA</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">MUSR</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">MMLU-Pro</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">Avg</th>\n  </tr></thead>\n  <tbody>\n  <tr>\n    <td style=\"text-align:left; background-color: #DAE8FF; color: black;\">Granite-3.1-8B-Instruct</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">72.08</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">34.09</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">21.68</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">8.28</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">19.01</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">28.19</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">30.55</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: #2D2D2D;\">Granite-3.1-2B-Instruct</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">62.86</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">21.82</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">11.33</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">5.26</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">4.87</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">20.21</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">21.06</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: #2D2D2D;\">Granite-3.1-3B-A800M-Instruct</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">55.16</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">16.69</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">10.35</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">5.15</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">2.51</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">12.75</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">17.1</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: #2D2D2D;\">Granite-3.1-1B-A400M-Instruct</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">46.86</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">6.18</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">4.08</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">0</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">0.78</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">2.41</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: #2D2D2D;\">10.05</td>\n  </tr>\n</tbody></table>\n\n**Model Architecture:**\nGranite-3.1-8B-Instruct is based on a decoder-only dense transformer architecture. Core components of this architecture are: GQA and RoPE, MLP with SwiGLU, RMSNorm, and shared input/output embeddings.\n\n<table>\n<thead>\n  <tr>\n    <th style=\"text-align:left; background-color: #001d6c; color: white;\">Model</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">2B Dense</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">8B Dense</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">1B MoE</th>\n    <th style=\"text-align:center; background-color: #001d6c; color: white;\">3B MoE</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\">Embedding size</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">2048</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">4096</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">1024</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">1536</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\">Number of layers</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">40</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">40</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">24</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">32</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\">Attention head size</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">64</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">128</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">64</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">64</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\">Number of attention heads</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">32</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">32</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">16</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">24</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\">Number of KV heads</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">8</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">8</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">8</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">8</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\">MLP hidden size</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">8192</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">12800</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">512</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">512</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\">MLP activation</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">SwiGLU</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">SwiGLU</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">SwiGLU</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">SwiGLU</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\">Number of experts</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">—</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">—</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">32</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">40</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\">MoE TopK</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">—</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">—</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">8</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">8</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\">Initialization std</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">0.1</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">0.1</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">0.1</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">0.1</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\">Sequence length</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">128K</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">128K</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">128K</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">128K</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\">Position embedding</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">RoPE</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">RoPE</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">RoPE</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">RoPE</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\"># Parameters</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">2.5B</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">8.1B</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">1.3B</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">3.3B</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\"># Active parameters</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">2.5B</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">8.1B</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">400M</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">800M</td>\n  </tr>\n  <tr>\n    <td style=\"text-align:left; background-color: #FFFFFF; color: black;\"># Training tokens</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">12T</td>\n    <td style=\"text-align:center; background-color: #DAE8FF; color: black;\">12T</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">10T</td>\n    <td style=\"text-align:center; background-color: #FFFFFF; color: black;\">10T</td>\n  </tr>\n</tbody></table>\n\n**Training Data:** \nOverall, our SFT data is largely comprised of three key sources: (1) publicly available datasets with permissive license, (2) internal synthetic data targeting specific capabilities including long-context tasks, and (3) very small amounts of human-curated data. A detailed attribution of datasets can be found in the [Granite 3.0 Technical Report](https://github.com/ibm-granite/granite-3.0-language-models/blob/main/paper.pdf), [Granite 3.1 Technical Report (coming soon)](https://huggingface.co/collections/ibm-granite/granite-31-language-models-6751dbbf2f3389bec5c6f02d), and [Accompanying Author List](https://github.com/ibm-granite/granite-3.0-language-models/blob/main/author-ack.pdf).\n\n**Infrastructure:**\nWe train Granite 3.1 Language Models using IBM's super computing cluster, Blue Vela, which is outfitted with NVIDIA H100 GPUs. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.\n\n**Ethical Considerations and Limitations:** \nGranite 3.1 Instruct Models are primarily finetuned using instruction-response pairs mostly in English, but also multilingual data covering eleven languages. Although this model can handle multilingual dialog use cases, its performance might not be similar to English tasks. In such case, introducing a small number of examples (few-shot) can help the model in generating more accurate outputs. While this model has been aligned by keeping safety in consideration, the model may in some cases produce inaccurate, biased, or unsafe responses to user prompts. So we urge the community to use this model with proper safety testing and tuning tailored for their specific tasks.\n\n**Resources**\n- ⭐️ Learn about the latest updates with Granite: https://www.ibm.com/granite\n- \U0001F4C4 Get started with tutorials, best practices, and prompt engineering advice: https://www.ibm.com/granite/docs/\n- \U0001F4A1 Learn about the latest Granite learning resources: https://ibm.biz/granite-learning-resources\n\n<!-- ## Citation\n```\n@misc{granite-models,\n  author = {author 1, author2, ...},\n  title = {},\n  journal = {},\n  volume = {},\n  year = {2024},\n  url = {https://arxiv.org/abs/0000.00000},\n}\n``` -->"
      language:
        - en
        - de
        - es
        - fr
        - ja
        - pt
        - ar
        - cs
        - it
        - ko
        - nl
        - zh
      license: Apache 2.0
      licenseLink: https://github.com/ibm-granite/granite-3.1-language-models
      tasks:
        - text-generation
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1744995384000"
      customProperties:
        granite:
            metadataType: MetadataStringValue
            string_value: ""
        granite-3.1:
            metadataType: MetadataStringValue
            string_value: ""
        language:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744995384000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/granite-3.1-8b-instruct-FP8-dynamic
      provider: Neural Magic
      description: Granite 3.1 8b Instruct Fp8 Dynamic - An instruction-tuned language model
      readme: "# granite-3.1-8b-instruct-FP8-dynamic\n\n## Model Overview\n- **Model Architecture:** granite-3.1-8b-instruct\n  - **Input:** Text\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Weight quantization:** FP8\n  - **Activation quantization:** FP8\n- **Release Date:** 1/8/2025\n- **Version:** 1.0\n- **Model Developers:** Neural Magic\n\nQuantized version of [ibm-granite/granite-3.1-8b-instruct](https://huggingface.co/ibm-granite/granite-3.1-8b-instruct).\nIt achieves an average score of 70.57 on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) benchmark (version 1), whereas the unquantized model achieves 70.30.\n\n### Model Optimizations\n\nThis model was obtained by quantizing the weights and activations of [ibm-granite/granite-3.1-8b-instruct](https://huggingface.co/ibm-granite/granite-3.1-8b-instruct) to FP8 data type, ready for inference with vLLM >= 0.5.2.\nThis optimization reduces the number of bits per parameter from 16 to 8, reducing the disk size and GPU memory requirements by approximately 50%. Only the weights and activations of the linear operators within transformers blocks are quantized. \n\n## Deployment\n\n### Use with vLLM\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\nmax_model_len, tp_size = 4096, 1\nmodel_name = \"neuralmagic/granite-3.1-8b-instruct-FP8-dynamic\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\n\nmessages_list = [\n    [{\"role\": \"user\", \"content\": \"Who are you? Please respond in pirate speak!\"}],\n]\n\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\n\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n```\n\nvLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\nThis model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n\n<details>\n  <summary>Model Creation Code</summary>\n\n```bash\npython quantize.py --model_id ibm-granite/granite-3.1-8b-instruct --save_path \"output_dir/\"\n```\n\n```python\nimport argparse\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom llmcompressor.modifiers.quantization import QuantizationModifier\nfrom llmcompressor.transformers import oneshot\nimport os\n\ndef main():\n    parser = argparse.ArgumentParser(description='Quantize a transformer model to FP8')\n    parser.add_argument('--model_id', type=str, required=True,\n                        help='The model ID from HuggingFace (e.g., \"meta-llama/Meta-Llama-3-8B-Instruct\")')\n    parser.add_argument('--save_path', type=str, default='.',\n                        help='Custom path to save the quantized model. If not provided, will use model_name-FP8-dynamic')\n    args = parser.parse_args()\n\n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_id, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True,\n    )\n    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n\n    # Configure the quantization algorithm and scheme\n    recipe = QuantizationModifier(\n        targets=\"Linear\", scheme=\"FP8_DYNAMIC\", ignore=[\"lm_head\"]\n    )\n\n    # Apply quantization\n    oneshot(model=model, recipe=recipe)\n\n    save_path = os.path.join(args.save_path, args.model_id.split(\"/\")[1] + \"-FP8-dynamic\")\n    os.makedirs(save_path, exist_ok=True)\n\n    # Save to disk in compressed-tensors format\n    model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n    print(f\"Model and tokenizer saved to: {save_path}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n</details>\n\n## Evaluation\n\nThe model was evaluated on OpenLLM Leaderboard [V1](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard), OpenLLM Leaderboard [V2](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/) and on [HumanEval](https://github.com/neuralmagic/evalplus), using the following commands:\n\n<details>\n<summary>Evaluation Commands</summary>\n  \nOpenLLM Leaderboard V1:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/granite-3.1-8b-instruct-FP8-dynamic\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \\\n  --tasks openllm \\\n  --write_out \\\n  --batch_size auto \\\n  --output_path output_dir \\\n  --show_config\n```\n\nOpenLLM Leaderboard V2:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/granite-3.1-8b-instruct-FP8-dynamic\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \\\n  --tasks leaderboard \\\n  --write_out \\\n  --batch_size auto \\\n  --output_path output_dir \\\n  --show_config\n```\n\n#### HumanEval\n##### Generation\n```\npython3 codegen/generate.py \\\n  --model neuralmagic/granite-3.1-8b-instruct-FP8-dynamic \\\n  --bs 16 \\\n  --temperature 0.2 \\\n  --n_samples 50 \\\n  --root \".\" \\\n  --dataset humaneval\n```\n##### Sanitization\n```\npython3 evalplus/sanitize.py \\\n  humaneval/neuralmagic--granite-3.1-8b-instruct-FP8-dynamic_vllm_temp_0.2\n```\n##### Evaluation\n```\nevalplus.evaluate \\\n  --dataset humaneval \\\n  --samples humaneval/neuralmagic--granite-3.1-8b-instruct-FP8-dynamic_vllm_temp_0.2-sanitized\n```\n</details>\n\n### Accuracy\n\n<table>\n  <thead>\n    <tr>\n      <th>Category</th>\n      <th>Metric</th>\n      <th>ibm-granite/granite-3.1-8b-instruct</th>\n      <th>neuralmagic/granite-3.1-8b-instruct-FP8-dynamic</th>\n      <th>Recovery (%)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <!-- OpenLLM Leaderboard V1 -->\n    <tr>\n      <td rowspan=\"7\"><b>OpenLLM V1</b></td>\n      <td>ARC-Challenge (Acc-Norm, 25-shot)</td>\n      <td>66.81</td>\n      <td>66.81</td>\n      <td>100.00</td>\n    </tr>\n    <tr>\n      <td>GSM8K (Strict-Match, 5-shot)</td>\n      <td>64.52</td>\n      <td>66.64</td>\n      <td>103.29</td>\n    </tr>\n    <tr>\n      <td>HellaSwag (Acc-Norm, 10-shot)</td>\n      <td>84.18</td>\n      <td>84.16</td>\n      <td>99.98</td>\n    </tr>\n    <tr>\n      <td>MMLU (Acc, 5-shot)</td>\n      <td>65.52</td>\n      <td>65.36</td>\n      <td>99.76</td>\n    </tr>\n    <tr>\n      <td>TruthfulQA (MC2, 0-shot)</td>\n      <td>60.57</td>\n      <td>60.52</td>\n      <td>99.92</td>\n    </tr>\n    <tr>\n      <td>Winogrande (Acc, 5-shot)</td>\n      <td>80.19</td>\n      <td>79.95</td>\n      <td>99.70</td>\n    </tr>\n    <tr>\n      <td><b>Average Score</b></td>\n      <td><b>70.30</b></td>\n      <td><b>70.57</b></td>\n      <td><b>100.39</b></td>\n    </tr>\n    <!-- OpenLLM Leaderboard V2 -->\n    <tr>\n      <td rowspan=\"7\"><b>OpenLLM V2</b></td>\n      <td>IFEval (Inst Level Strict Acc, 0-shot)</td>\n      <td>74.10</td>\n      <td>73.62</td>\n      <td>99.35</td>\n    </tr>\n    <tr>\n      <td>BBH (Acc-Norm, 3-shot)</td>\n      <td>53.19</td>\n      <td>53.26</td>\n      <td>100.13</td>\n    </tr>\n    <tr>\n      <td>Math-Hard (Exact-Match, 4-shot)</td>\n      <td>14.77</td>\n      <td>16.79</td>\n      <td>113.66</td>\n    </tr>\n    <tr>\n      <td>GPQA (Acc-Norm, 0-shot)</td>\n      <td>31.76</td>\n      <td>32.58</td>\n      <td>102.58</td>\n    </tr>\n    <tr>\n      <td>MUSR (Acc-Norm, 0-shot)</td>\n      <td>46.01</td>\n      <td>47.34</td>\n      <td>102.89</td>\n    </tr>\n    <tr>\n      <td>MMLU-Pro (Acc, 5-shot)</td>\n      <td>35.81</td>\n      <td>35.72</td>\n      <td>99.75</td>\n    </tr>\n    <tr>\n      <td><b>Average Score</b></td>\n      <td><b>42.61</b></td>\n      <td><b>43.22</b></td>\n      <td><b>101.43</b></td>\n    </tr>\n    <!-- HumanEval -->\n    <tr>\n      <td rowspan=\"2\"><b>Coding</b></td>\n      <td>HumanEval Pass@1</td>\n      <td>71.00</td>\n      <td>69.90</td>\n      <td><b>98.45</b></td>\n    </tr>\n  </tbody>\n</table>\n\n\n\n## Inference Performance\n\n\nThis model achieves up to 1.5x speedup in single-stream deployment and up to 1.1x speedup in multi-stream asynchronous deployment on L40 GPUs.\nThe following performance benchmarks were conducted with [vLLM](https://docs.vllm.ai/en/latest/) version 0.6.6.post1, and [GuideLLM](https://github.com/neuralmagic/guidellm).\n\n<details>\n<summary>Benchmarking Command</summary>\n\n```\nguidellm --model neuralmagic/granite-3.1-8b-instruct-FP8-dynamic --target \"http://localhost:8000/v1\" --data-type emulated --data \"prompt_tokens=<prompt_tokens>,generated_tokens=<generated_tokens>\" --max seconds 360 --backend aiohttp_server\n```\n\n</details>\n\n\n### Single-stream performance (measured with vLLM version 0.6.6.post1)\n<table>\n  <tr>\n    <td></td>\n    <td></td>\n    <td></td>\n    <th style=\"text-align: center;\" colspan=\"7\" >Latency (s)</th>\n  </tr>\n  <tr>\n    <th>GPU class</th>\n    <th>Model</th>\n    <th>Speedup</th>\n    <th>Code Completion<br>prefill: 256 tokens<br>decode: 1024 tokens</th>\n    <th>Docstring Generation<br>prefill: 768 tokens<br>decode: 128 tokens</th>\n    <th>Code Fixing<br>prefill: 1024 tokens<br>decode: 1024 tokens</th>\n    <th>RAG<br>prefill: 1024 tokens<br>decode: 128 tokens</th>\n    <th>Instruction Following<br>prefill: 256 tokens<br>decode: 128 tokens</th>\n    <th>Multi-turn Chat<br>prefill: 512 tokens<br>decode: 256 tokens</th>\n    <th>Large Summarization<br>prefill: 4096 tokens<br>decode: 512 tokens</th>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >L40</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>25.1</td>\n    <td>3.2</td>\n    <td>25.3</td>\n    <td>3.2</td>\n    <td>3.2</td>\n    <td>6.3</td>\n    <td>13.4</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-FP8-dynamic<br>(this model)</td>\n    <td>1.47</td>\n    <td>16.8</td>\n    <td>2.2</td>\n    <td>17.1</td>\n    <td>2.2</td>\n    <td>2.1</td>\n    <td>4.2</td>\n    <td>9.3</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16</td>\n    <td>2.72</td>\n    <td>8.9</td>\n    <td>1.2</td>\n    <td>9.2</td>\n    <td>1.2</td>\n    <td>1.1</td>\n    <td>2.3</td>\n    <td>5.3</td>\n  </tr>\n</table>\n\n\n### Multi-stream asynchronous performance (measured with vLLM version 0.6.6.post1)\n<table>\n  <tr>\n    <td></td>\n    <td></td>\n    <td></td>\n    <th style=\"text-align: center;\" colspan=\"7\" >Maximum Throughput (Queries per Second)</th>\n  </tr>\n  <tr>\n    <th>GPU class</th>\n    <th>Model</th>\n    <th>Speedup</th>\n    <th>Code Completion<br>prefill: 256 tokens<br>decode: 1024 tokens</th>\n    <th>Docstring Generation<br>prefill: 768 tokens<br>decode: 128 tokens</th>\n    <th>Code Fixing<br>prefill: 1024 tokens<br>decode: 1024 tokens</th>\n    <th>RAG<br>prefill: 1024 tokens<br>decode: 128 tokens</th>\n    <th>Instruction Following<br>prefill: 256 tokens<br>decode: 128 tokens</th>\n    <th>Multi-turn Chat<br>prefill: 512 tokens<br>decode: 256 tokens</th>\n    <th>Large Summarization<br>prefill: 4096 tokens<br>decode: 512 tokens</th>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >L40</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>1.4</td>\n    <td>7.8</td>\n    <td>1.1</td>\n    <td>6.2</td>\n    <td>15.5</td>\n    <td>6.0</td>\n    <td>0.7</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-FP8-dynamic<br>(this model)</td>\n    <td>1.12</td>\n    <td>2.1</td>\n    <td>7.4</td>\n    <td>1.3</td>\n    <td>5.9</td>\n    <td>15.3</td>\n    <td>6.9</td>\n    <td>0.8</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-2b-instruct-quantized.w4a16</td>\n    <td>1.29</td>\n    <td>2.4</td>\n    <td>8.9</td>\n    <td>1.4</td>\n    <td>7.1</td>\n    <td>17.8</td>\n    <td>7.8</td>\n    <td>1.0</td>\n  </tr>\n</table>\n"
      language:
        - en
        - de
        - es
        - fr
        - ja
        - pt
        - ar
        - cs
        - it
        - ko
        - nl
        - zh
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-generation
      createTimeSinceEpoch: "1736294400"
      lastUpdateTimeSinceEpoch: "1736294400"
      customProperties:
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        fp8:
            metadataType: MetadataStringValue
            string_value: ""
        granite:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct-fp8-dynamic:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744993200000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/granite-3.1-8b-instruct-FP8-dynamic
      provider: Neural Magic
      description: Meta Llama 3.1 8B Instruct Fp8 Dynamic - An instruction-tuned language model
      readme: |
        # Meta-Llama-3.1-8B-Instruct-FP8-dynamic

        ## Model Overview
        - **Model Architecture:** Meta-Llama-3.1
          - **Input:** Text
          - **Output:** Text
        - **Model Optimizations:**
          - **Weight quantization:** FP8
          - **Activation quantization:** FP8
        - **Intended Use Cases:** Intended for commercial and research use in multiple languages. Similarly to [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct), this models is intended for assistant-like chat.
        - **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English.
        - **Release Date:** 7/23/2024
        - **Version:** 1.0
        - **Model Developers:** Neural Magic

        This model is a quantized version of [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).
        It was evaluated on a several tasks to assess the its quality in comparison to the unquatized model, including multiple-choice, math reasoning, and open-ended text generation.
        Meta-Llama-3.1-8B-Instruct-FP8-dynamic achieves 105.4% recovery for the Arena-Hard evaluation, 99.7% for OpenLLM v1 (using Meta's prompting when available), 101.2% for OpenLLM v2, 100.0% for HumanEval pass@1, and 101.0% for HumanEval+ pass@1.

        ### Model Optimizations

        This model was obtained by quantizing the weights and activations of [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) to FP8 data type, ready for inference with vLLM built from source.
        This optimization reduces the number of bits per parameter from 16 to 8, reducing the disk size and GPU memory requirements by approximately 50%.

        Only the weights and activations of the linear operators within transformers blocks are quantized. Symmetric per-channel quantization is applied, in which a linear scaling per output dimension maps the FP8 representations of the quantized weights and activations. Activations are also quantized on a per-token dynamic basis.
        [LLM Compressor](https://github.com/vllm-project/llm-compressor) is used for quantization.

        ## Deployment

        ### Use with vLLM

        This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

        ```python
        from vllm import LLM, SamplingParams
        from transformers import AutoTokenizer

        model_id = "neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic"

        sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)

        tokenizer = AutoTokenizer.from_pretrained(model_id)

        messages = [
            {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
            {"role": "user", "content": "Who are you?"},
        ]

        prompts = tokenizer.apply_chat_template(messages, tokenize=False)

        llm = LLM(model=model_id)

        outputs = llm.generate(prompts, sampling_params)

        generated_text = outputs[0].outputs[0].text
        print(generated_text)
        ```

        vLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.

        ## Creation

        This model was created by applying [LLM Compressor with calibration samples from UltraChat](https://github.com/vllm-project/llm-compressor/blob/sa/big_model_support/examples/big_model_offloading/big_model_w8a8_calibrate.py), as presented in the code snipet below.

        ```python
        import torch

        from transformers import AutoTokenizer

        from llmcompressor.transformers import SparseAutoModelForCausalLM, oneshot
        from llmcompressor.transformers.compression.helpers import (  # noqa
            calculate_offload_device_map,
            custom_offload_device_map,
        )

        recipe = """
        quant_stage:
            quant_modifiers:
                QuantizationModifier:
                    ignore: ["lm_head"]
                    config_groups:
                        group_0:
                            weights:
                                num_bits: 8
                                type: float
                                strategy: channel
                                dynamic: false
                                symmetric: true
                            input_activations:
                                num_bits: 8
                                type: float
                                strategy: token
                                dynamic: true
                                symmetric: true
                            targets: ["Linear"]
        """

        model_stub = "meta-llama/Meta-Llama-3.1-8B-Instruct"
        model_name = model_stub.split("/")[-1]

        device_map = calculate_offload_device_map(
            model_stub, reserve_for_hessians=False, num_gpus=1, torch_dtype="auto"
        )

        model = SparseAutoModelForCausalLM.from_pretrained(
            model_stub, torch_dtype="auto", device_map=device_map
        )

        output_dir = f"./{model_name}-FP8-dynamic"

        oneshot(
            model=model,
            recipe=recipe,
            output_dir=output_dir,
            save_compressed=True,
            tokenizer=AutoTokenizer.from_pretrained(model_stub),
        )
        ```

        ## Evaluation

        This model was evaluated on the well-known Arena-Hard, OpenLLM v1, OpenLLM v2, HumanEval, and HumanEval+ benchmarks.
        In all cases, model outputs were generated with the [vLLM](https://docs.vllm.ai/en/stable/) engine.

        Arena-Hard evaluations were conducted using the [Arena-Hard-Auto](https://github.com/lmarena/arena-hard-auto) repository.
        The model generated a single answer for each prompt form Arena-Hard, and each answer was judged twice by GPT-4.
        We report below the scores obtained in each judgement and the average.

        OpenLLM v1 and v2 evaluations were conducted using Neural Magic's fork of [lm-evaluation-harness](https://github.com/neuralmagic/lm-evaluation-harness/tree/llama_3.1_instruct) (branch llama_3.1_instruct).
        This version of the lm-evaluation-harness includes versions of MMLU, ARC-Challenge and GSM-8K that match the prompting style of [Meta-Llama-3.1-Instruct-evals](https://huggingface.co/datasets/meta-llama/Meta-Llama-3.1-8B-Instruct-evals) and a few fixes to OpenLLM v2 tasks.

        HumanEval and HumanEval+ evaluations were conducted using Neural Magic's fork of the [EvalPlus](https://github.com/neuralmagic/evalplus) repository.

        Detailed model outputs are available as HuggingFace datasets for [Arena-Hard](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-arena-hard-evals), [OpenLLM v2](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-leaderboard-v2-evals), and [HumanEval](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-humaneval-evals).

        ### Accuracy

        <table>
          <tr>
           <td><strong>Benchmark</strong>
           </td>
           <td><strong>Meta-Llama-3.1-8B-Instruct </strong>
           </td>
           <td><strong>Meta-Llama-3.1-8B-Instruct-FP8-dynamic (this model)</strong>
           </td>
           <td><strong>Recovery</strong>
           </td>
          </tr>
          <tr>
           <td>MMLU (5-shot)
           </td>
           <td>67.95
           </td>
           <td>68.02
           </td>
           <td>100.1%
           </td>
          </tr>
          <tr>
           <td><strong>Arena Hard</strong>
           </td>
           <td>25.8 (25.1 / 26.5)
           </td>
           <td>27.2 (27.4 / 27.0)
           </td>
           <td>105.4%
           </td>
          </tr>
          <tr>
           <td><strong>OpenLLM v1</strong>
           </td>
          </tr>
          <tr>
           <td>MMLU-cot (0-shot)
           </td>
           <td>71.2
           </td>
           <td>71.6
           </td>
           <td>100.5%
           </td>
          </tr>
          <tr>
           <td>ARC Challenge (0-shot)
           </td>
           <td>82.0
           </td>
           <td>81.2
           </td>
           <td>99.1%
           </td>
          </tr>
          <tr>
           <td>GSM-8K-cot (8-shot, strict-match)
           </td>
           <td>82.0
           </td>
           <td>82.0
           </td>
           <td>100.0%
           </td>
          </tr>
          <tr>
           <td>Hellaswag (10-shot)
           </td>
           <td>80.5
           </td>
           <td>80.0
           </td>
           <td>99.5%
           </td>
          </tr>
          <tr>
           <td>Winogrande (5-shot)
           </td>
           <td>78.5
           </td>
           <td>77.7
           </td>
           <td>99.0%
           </td>
          </tr>
          <tr>
           <td>TruthfulQA (0-shot, mc2)
           </td>
           <td>54.5
           </td>
           <td>54.3
           </td>
           <td>99.6%
           </td>
          </tr>
          <tr>
           <td><strong>Average</strong>
           </td>
           <td><strong>73.8</strong>
           </td>
           <td><strong>73.6</strong>
           </td>
           <td><strong>99.7%</strong>
           </td>
          </tr>
          <tr>
           <td><strong>OpenLLM v2</strong>
           </td>
          </tr>
          <tr>
           <td>MMLU-Pro (5-shot)
           </td>
           <td>30.8
           </td>
           <td>31.2
           </td>
           <td>101.3%
           </td>
          </tr>
          <tr>
           <td>IFEval (0-shot)
           </td>
           <td>77.9
           </td>
           <td>77.2
           </td>
           <td>99.1%
           </td>
          </tr>
          <tr>
           <td>BBH (3-shot)
           </td>
           <td>30.1
           </td>
           <td>29.7
           </td>
           <td>98.5%
           </td>
          </tr>
          <tr>
           <td>Math-|v|-5 (4-shot)
           </td>
           <td>15.7
           </td>
           <td>16.5
           </td>
           <td>105.4%
           </td>
          </tr>
          <tr>
           <td>GPQA (0-shot)
           </td>
           <td>3.7
           </td>
           <td>5.7
           </td>
           <td>156.0%
           </td>
          </tr>
          <tr>
           <td>MuSR (0-shot)
           </td>
           <td>7.6
           </td>
           <td>7.5
           </td>
           <td>98.8%
           </td>
          </tr>
          <tr>
           <td><strong>Average</strong>
           </td>
           <td><strong>27.6</strong>
           </td>
           <td><strong>28.0</strong>
           </td>
           <td><strong>101.2%</strong>
           </td>
          </tr>
          <tr>
           <td><strong>Coding</strong>
           </td>
          </tr>
          <tr>
           <td>HumanEval pass@1
           </td>
           <td>67.3
           </td>
           <td>67.3
           </td>
           <td>100.0%
           </td>
          </tr>
          <tr>
           <td>HumanEval+ pass@1
           </td>
           <td>60.7
           </td>
           <td>61.3
           </td>
           <td>101.0%
           </td>
          </tr>
        </table>

        ### Reproduction

        The results were obtained using the following commands:

        #### MMLU
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
          --tasks mmlu \
          --num_fewshot 5 \
          --batch_size auto
        ```

        #### MMLU-cot
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
          --tasks mmlu_cot_0shot_llama_3.1_instruct \
          --apply_chat_template \
          --num_fewshot 0 \
          --batch_size auto
        ```

        #### ARC-Challenge
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
          --tasks arc_challenge_llama_3.1_instruct \
          --apply_chat_template \
          --num_fewshot 0 \
          --batch_size auto
        ```

        #### GSM-8K
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
          --tasks gsm8k_cot_llama_3.1_instruct \
          --apply_chat_template \
          --fewshot_as_multiturn \
          --num_fewshot 8 \
          --batch_size auto
        ```

        #### Hellaswag
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
          --tasks hellaswag \
          --num_fewshot 10 \
          --batch_size auto
        ```

        #### Winogrande
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
          --tasks winogrande \
          --num_fewshot 5 \
          --batch_size auto
        ```

        #### TruthfulQA
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
          --tasks truthfulqa \
          --num_fewshot 0 \
          --batch_size auto
        ```

        #### OpenLLM v2
        ```
        lm_eval \
          --model vllm \
          --model_args pretrained="neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic",dtype=auto,max_model_len=4096,tensor_parallel_size=1,enable_chunked_prefill=True \
          --apply_chat_template \
          --fewshot_as_multiturn \
          --tasks leaderboard \
          --batch_size auto
        ```

        #### HumanEval and HumanEval+
        ##### Generation
        ```
        python3 codegen/generate.py \
          --model neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic \
          --bs 16 \
          --temperature 0.2 \
          --n_samples 50 \
          --root "." \
          --dataset humaneval
        ```
        ##### Sanitization
        ```
        python3 evalplus/sanitize.py \
          humaneval/neuralmagic--Meta-Llama-3.1-8B-Instruct-FP8-dynamic_vllm_temp_0.2
        ```
        ##### Evaluation
        ```
        evalplus.evaluate \
          --dataset humaneval \
          --samples humaneval/neuralmagic--Meta-Llama-3.1-8B-Instruct-FP8-dynamic_vllm_temp_0.2-sanitized
        ```
      language:
        - en
        - de
        - es
        - fr
        - ja
        - pt
        - ar
        - cs
        - it
        - ko
        - nl
        - zh
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-generation
      createTimeSinceEpoch: "1721692800"
      lastUpdateTimeSinceEpoch: "1721692800"
      customProperties:
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        fp8:
            metadataType: MetadataStringValue
            string_value: ""
        granite:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct-fp8-dynamic:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744887641000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/granite-3.1-8b-instruct-quantized.w4a16
      provider: Neural Magic
      description: Granite 3.1 8b Instruct Quantized.w4a16 - An instruction-tuned language model
      readme: "# granite-3.1-8b-instruct-quantized.w4a16\n\n## Model Overview\n- **Model Architecture:** granite-3.1-8b-instruct\n  - **Input:** Text\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Weight quantization:** INT4\n  - **Activation quantization:** INT4\n- **Release Date:** 1/8/2025\n- **Version:** 1.0\n- **Model Developers:** Neural Magic\n\nQuantized version of [ibm-granite/granite-3.1-8b-instruct](https://huggingface.co/ibm-granite/granite-3.1-8b-instruct).\nIt achieves an average score of 69.81 on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) benchmark (version 1), whereas the unquantized model achieves 70.30.\n\n### Model Optimizations\n\nThis model was obtained by quantizing the weights of [ibm-granite/granite-3.1-8b-instruct](https://huggingface.co/ibm-granite/granite-3.1-8b-instruct) to INT4 data type, ready for inference with vLLM >= 0.5.2.\nThis optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 75%. Only the weights of the linear operators within transformers blocks are quantized. \n\n## Deployment\n\n### Use with vLLM\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\nmax_model_len, tp_size = 4096, 1\nmodel_name = \"neuralmagic/granite-3.1-8b-instruct-quantized.w4a16\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\n\nmessages_list = [\n    [{\"role\": \"user\", \"content\": \"Who are you? Please respond in pirate speak!\"}],\n]\n\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\n\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n```\n\nvLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\nThis model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n\n<details>\n  <summary>Model Creation Code</summary>\n\n```bash\npython quantize.py --model_path ibm-granite/granite-3.1-8b-instruct --quant_path \"output_dir/granite-3.1-8b-instruct-quantized.w4a16\" --calib_size 1024 --dampening_frac 0.1 --observer mse  --actorder static\n```\n\n\n```python\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom llmcompressor.modifiers.quantization import GPTQModifier\nfrom llmcompressor.transformers import SparseAutoModelForCausalLM, oneshot, apply\nimport argparse\nfrom compressed_tensors.quantization import QuantizationScheme, QuantizationArgs, QuantizationType, QuantizationStrategy\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--model_path', type=str)\nparser.add_argument('--quant_path', type=str)\nparser.add_argument('--calib_size', type=int, default=256)\nparser.add_argument('--dampening_frac', type=float, default=0.1) \nparser.add_argument('--observer', type=str, default=\"minmax\")\nparser.add_argument('--actorder', type=str, default=\"dynamic\")\n\nargs = parser.parse_args()\n\nmodel = SparseAutoModelForCausalLM.from_pretrained(\n    args.model_path,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    use_cache=False,\n    trust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(args.model_path)\n\n\nNUM_CALIBRATION_SAMPLES = args.calib_size\nDATASET_ID = \"neuralmagic/LLM_compression_calibration\"\nDATASET_SPLIT = \"train\"\nds = load_dataset(DATASET_ID, split=DATASET_SPLIT)\nds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n\ndef preprocess(example):\n    return {\"text\": example[\"text\"]}\n\nds = ds.map(preprocess)\n\ndef tokenize(sample):\n    return tokenizer(\n        sample[\"text\"],\n        padding=False,\n        truncation=False,\n        add_special_tokens=True,\n    )\n\n\nds = ds.map(tokenize, remove_columns=ds.column_names)\n\nrecipe = [\n    GPTQModifier(\n        targets=[\"Linear\"],\n        ignore=[\"lm_head\"],\n        scheme=\"w4a16\",\n        dampening_frac=args.dampening_frac,\n        observer=args.observer,\n        actoder=args.actorder,\n    )\n]\noneshot(\n    model=model,\n    dataset=ds,\n    recipe=recipe,\n    num_calibration_samples=args.calib_size,\n    max_seq_length=8196,\n)\n\n# Save to disk compressed.\nmodel.save_pretrained(SAVE_DIR, save_compressed=True)\ntokenizer.save_pretrained(SAVE_DIR)\n```\n</details>\n\n## Evaluation\n\nThe model was evaluated on OpenLLM Leaderboard [V1](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard), OpenLLM Leaderboard [V2](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/) and on [HumanEval](https://github.com/neuralmagic/evalplus), using the following commands:\n\n<details>\n<summary>Evaluation Commands</summary>\n  \nOpenLLM Leaderboard V1:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/granite-3.1-8b-instruct-quantized.w4a16\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \\\n  --tasks openllm \\\n  --write_out \\\n  --batch_size auto \\\n  --output_path output_dir \\\n  --show_config\n```\n\nOpenLLM Leaderboard V2:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/granite-3.1-8b-instruct-quantized.w4a16\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \\\n  --tasks leaderboard \\\n  --write_out \\\n  --batch_size auto \\\n  --output_path output_dir \\\n  --show_config\n```\n\n#### HumanEval\n##### Generation\n```\npython3 codegen/generate.py \\\n  --model neuralmagic/granite-3.1-8b-instruct-quantized.w4a16 \\\n  --bs 16 \\\n  --temperature 0.2 \\\n  --n_samples 50 \\\n  --root \".\" \\\n  --dataset humaneval\n```\n##### Sanitization\n```\npython3 evalplus/sanitize.py \\\n  humaneval/neuralmagic--granite-3.1-8b-instruct-quantized.w4a16_vllm_temp_0.2\n```\n##### Evaluation\n```\nevalplus.evaluate \\\n  --dataset humaneval \\\n  --samples humaneval/neuralmagic--granite-3.1-8b-instruct-quantized.w4a16_vllm_temp_0.2-sanitized\n```\n</details>\n\n### Accuracy\n\n<table>\n  <thead>\n    <tr>\n      <th>Category</th>\n      <th>Metric</th>\n      <th>ibm-granite/granite-3.1-8b-instruct</th>\n      <th>neuralmagic/granite-3.1-8b-instruct-quantized.w4a16</th>\n      <th>Recovery (%)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan=\"7\"><b>OpenLLM V1</b></td>\n      <td>ARC-Challenge (Acc-Norm, 25-shot)</td>\n      <td>66.81</td>\n      <td>66.81</td>\n      <td>100.00</td>\n    </tr>\n    <tr>\n      <td>GSM8K (Strict-Match, 5-shot)</td>\n      <td>64.52</td>\n      <td>65.66</td>\n      <td>101.77</td>\n    </tr>\n    <tr>\n      <td>HellaSwag (Acc-Norm, 10-shot)</td>\n      <td>84.18</td>\n      <td>83.62</td>\n      <td>99.33</td>\n    </tr>\n    <tr>\n      <td>MMLU (Acc, 5-shot)</td>\n      <td>65.52</td>\n      <td>64.25</td>\n      <td>98.06</td>\n    </tr>\n    <tr>\n      <td>TruthfulQA (MC2, 0-shot)</td>\n      <td>60.57</td>\n      <td>60.17</td>\n      <td>99.34</td>\n    </tr>\n    <tr>\n      <td>Winogrande (Acc, 5-shot)</td>\n      <td>80.19</td>\n      <td>78.37</td>\n      <td>97.73</td>\n    </tr>\n    <tr>\n      <td><b>Average Score</b></td>\n      <td><b>70.30</b></td>\n      <td><b>69.81</b></td>\n      <td><b>99.31</b></td>\n    </tr>\n    <tr>\n      <td rowspan=\"7\"><b>OpenLLM V2</b></td>\n      <td>IFEval (Inst Level Strict Acc, 0-shot)</td>\n      <td>74.01</td>\n      <td>73.14</td>\n      <td>98.82</td>\n    </tr>\n    <tr>\n      <td>BBH (Acc-Norm, 3-shot)</td>\n      <td>53.19</td>\n      <td>51.52</td>\n      <td>96.86</td>\n    </tr>\n    <tr>\n      <td>Math-Hard (Exact-Match, 4-shot)</td>\n      <td>14.77</td>\n      <td>16.66</td>\n      <td>112.81</td>\n    </tr>\n    <tr>\n      <td>GPQA (Acc-Norm, 0-shot)</td>\n      <td>31.76</td>\n      <td>29.91</td>\n      <td>94.17</td>\n    </tr>\n    <tr>\n      <td>MUSR (Acc-Norm, 0-shot)</td>\n      <td>46.01</td>\n      <td>45.75</td>\n      <td>99.44</td>\n    </tr>\n    <tr>\n      <td>MMLU-Pro (Acc, 5-shot)</td>\n      <td>35.81</td>\n      <td>34.23</td>\n      <td>95.59</td>\n    </tr>\n    <tr>\n      <td><b>Average Score</b></td>\n      <td><b>42.61</b></td>\n      <td><b>41.87</b></td>\n      <td><b>98.26</b></td>\n    </tr>\n    <tr>\n      <td rowspan=\"2\"><b>Coding</b></td>\n      <td>HumanEval Pass@1</td>\n      <td>71.00</td>\n      <td>70.50</td>\n      <td><b>99.30</b></td>\n    </tr>\n  </tbody>\n</table>\n\n\n\n## Inference Performance\n\n\nThis model achieves up to 2.7x speedup in single-stream deployment and up to 1.5x speedup in multi-stream asynchronous deployment, depending on hardware and use-case scenario.\nThe following performance benchmarks were conducted with [vLLM](https://docs.vllm.ai/en/latest/) version 0.6.6.post1, and [GuideLLM](https://github.com/neuralmagic/guidellm).\n\n<details>\n<summary>Benchmarking Command</summary>\n\n```\nguidellm --model neuralmagic/granite-3.1-8b-instruct-quantized.w4a16 --target \"http://localhost:8000/v1\" --data-type emulated --data \"prompt_tokens=<prompt_tokens>,generated_tokens=<generated_tokens>\" --max seconds 360 --backend aiohttp_server\n```\n\n</details>\n\n### Single-stream performance (measured with vLLM version 0.6.6.post1)\n<table>\n  <tr>\n    <td></td>\n    <td></td>\n    <td></td>\n    <th style=\"text-align: center;\" colspan=\"7\" >Latency (s)</th>\n  </tr>\n  <tr>\n    <th>GPU class</th>\n    <th>Model</th>\n    <th>Speedup</th>\n    <th>Code Completion<br>prefill: 256 tokens<br>decode: 1024 tokens</th>\n    <th>Docstring Generation<br>prefill: 768 tokens<br>decode: 128 tokens</th>\n    <th>Code Fixing<br>prefill: 1024 tokens<br>decode: 1024 tokens</th>\n    <th>RAG<br>prefill: 1024 tokens<br>decode: 128 tokens</th>\n    <th>Instruction Following<br>prefill: 256 tokens<br>decode: 128 tokens</th>\n    <th>Multi-turn Chat<br>prefill: 512 tokens<br>decode: 256 tokens</th>\n    <th>Large Summarization<br>prefill: 4096 tokens<br>decode: 512 tokens</th>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A5000</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>28.3</td>\n    <td>3.7</td>\n    <td>28.8</td>\n    <td>3.8</td>\n    <td>3.6</td>\n    <td>7.2</td>\n    <td>15.7</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w8a8</td>\n    <td>1.60</td>\n    <td>17.7</td>\n    <td>2.3</td>\n    <td>18.0</td>\n    <td>2.4</td>\n    <td>2.2</td>\n    <td>4.5</td>\n    <td>10.0</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16<br>(this model)</td>\n    <td>2.61</td>\n    <td>10.3</td>\n    <td>1.5</td>\n    <td>10.7</td>\n    <td>1.5</td>\n    <td>1.3</td>\n    <td>2.7</td>\n    <td>6.6</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A6000</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>25.8</td>\n    <td>3.4</td>\n    <td>26.2</td>\n    <td>3.4</td>\n    <td>3.3</td>\n    <td>6.5</td>\n    <td>14.2</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w8a8</td>\n    <td>1.50</td>\n    <td>17.4</td>\n    <td>2.3</td>\n    <td>16.9</td>\n    <td>2.2</td>\n    <td>2.2</td>\n    <td>4.4</td>\n    <td>9.8</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16<br>(this model)</td>\n    <td>2.48</td>\n    <td>10.0</td>\n    <td>1.4</td>\n    <td>10.4</td>\n    <td>1.5</td>\n    <td>1.3</td>\n    <td>2.5</td>\n    <td>6.2</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A100</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>13.6</td>\n    <td>1.8</td>\n    <td>13.7</td>\n    <td>1.8</td>\n    <td>1.7</td>\n    <td>3.4</td>\n    <td>7.3</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w8a8</td>\n    <td>1.31</td>\n    <td>10.4</td>\n    <td>1.3</td>\n    <td>10.5</td>\n    <td>1.4</td>\n    <td>1.3</td>\n    <td>2.6</td>\n    <td>5.6</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16<br>(this model)</td>\n    <td>1.80</td>\n    <td>7.3</td>\n    <td>1.0</td>\n    <td>7.4</td>\n    <td>1.0</td>\n    <td>0.9</td>\n    <td>1.9</td>\n    <td>4.3</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >L40</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>25.1</td>\n    <td>3.2</td>\n    <td>25.3</td>\n    <td>3.2</td>\n    <td>3.2</td>\n    <td>6.3</td>\n    <td>13.4</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-FP8-dynamic</td>\n    <td>1.47</td>\n    <td>16.8</td>\n    <td>2.2</td>\n    <td>17.1</td>\n    <td>2.2</td>\n    <td>2.1</td>\n    <td>4.2</td>\n    <td>9.3</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16<br>(this model)</td>\n    <td>2.72</td>\n    <td>8.9</td>\n    <td>1.2</td>\n    <td>9.2</td>\n    <td>1.2</td>\n    <td>1.1</td>\n    <td>2.3</td>\n    <td>5.3</td>\n  </tr>\n</table>\n\n\n### Multi-stream asynchronous performance (measured with vLLM version 0.6.6.post1)\n<table>\n  <tr>\n    <td></td>\n    <td></td>\n    <td></td>\n    <th style=\"text-align: center;\" colspan=\"7\" >Maximum Throughput (Queries per Second)</th>\n  </tr>\n  <tr>\n    <th>GPU class</th>\n    <th>Model</th>\n    <th>Speedup</th>\n    <th>Code Completion<br>prefill: 256 tokens<br>decode: 1024 tokens</th>\n    <th>Docstring Generation<br>prefill: 768 tokens<br>decode: 128 tokens</th>\n    <th>Code Fixing<br>prefill: 1024 tokens<br>decode: 1024 tokens</th>\n    <th>RAG<br>prefill: 1024 tokens<br>decode: 128 tokens</th>\n    <th>Instruction Following<br>prefill: 256 tokens<br>decode: 128 tokens</th>\n    <th>Multi-turn Chat<br>prefill: 512 tokens<br>decode: 256 tokens</th>\n    <th>Large Summarization<br>prefill: 4096 tokens<br>decode: 512 tokens</th>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A5000</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>0.8</td>\n    <td>3.1</td>\n    <td>0.4</td>\n    <td>2.5</td>\n    <td>6.7</td>\n    <td>2.7</td>\n    <td>0.3</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w8a8</td>\n    <td>1.71</td>\n    <td>1.3</td>\n    <td>5.2</td>\n    <td>0.9</td>\n    <td>4.0</td>\n    <td>10.5</td>\n    <td>4.4</td>\n    <td>0.5</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16<br>(this model)</td>\n    <td>1.46</td>\n    <td>1.3</td>\n    <td>3.9</td>\n    <td>0.8</td>\n    <td>2.9</td>\n    <td>8.2</td>\n    <td>3.6</td>\n    <td>0.5</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A6000</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>1.3</td>\n    <td>5.1</td>\n    <td>0.9</td>\n    <td>4.0</td>\n    <td>0.3</td>\n    <td>4.3</td>\n    <td>0.6</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w8a8</td>\n    <td>1.39</td>\n    <td>1.8</td>\n    <td>7.0</td>\n    <td>1.3</td>\n    <td>5.6</td>\n    <td>14.0</td>\n    <td>6.3</td>\n    <td>0.8</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16<br>(this model)</td>\n    <td>1.09</td>\n    <td>1.9</td>\n    <td>4.8</td>\n    <td>1.0</td>\n    <td>3.8</td>\n    <td>10.0</td>\n    <td>5.0</td>\n    <td>0.6</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A100</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>3.1</td>\n    <td>10.7</td>\n    <td>2.1</td>\n    <td>8.5</td>\n    <td>20.6</td>\n    <td>9.6</td>\n    <td>1.4</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w8a8</td>\n    <td>1.23</td>\n    <td>3.8</td>\n    <td>14.2</td>\n    <td>2.1</td>\n    <td>11.4</td>\n    <td>25.9</td>\n    <td>12.1</td>\n    <td>1.7</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16<br>(this model)</td>\n    <td>0.96</td>\n    <td>3.4</td>\n    <td>9.0</td>\n    <td>2.6</td>\n    <td>7.2</td>\n    <td>18.0</td>\n    <td>8.8</td>\n    <td>1.3</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >L40</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>1.4</td>\n    <td>7.8</td>\n    <td>1.1</td>\n    <td>6.2</td>\n    <td>15.5</td>\n    <td>6.0</td>\n    <td>0.7</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-FP8-dynamic</td>\n    <td>1.12</td>\n    <td>2.1</td>\n    <td>7.4</td>\n    <td>1.3</td>\n    <td>5.9</td>\n    <td>15.3</td>\n    <td>6.9</td>\n    <td>0.8</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16<br>(this model)</td>\n    <td>1.29</td>\n    <td>2.4</td>\n    <td>8.9</td>\n    <td>1.4</td>\n    <td>7.1</td>\n    <td>17.8</td>\n    <td>7.8</td>\n    <td>1.0</td>\n  </tr>\n</table>\n\n"
      language:
        - en
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-generation
      createTimeSinceEpoch: "1736294400"
      lastUpdateTimeSinceEpoch: "1736294400"
      customProperties:
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        granite:
            metadataType: MetadataStringValue
            string_value: ""
        int4:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
        w4a16:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct-quantized-w4a16:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1746714934000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/granite-3.1-8b-instruct-quantized.w4a16
      provider: Neural Magic
      description: Meta Llama 3.1 8B Instruct Quantized.w4a16 - An instruction-tuned language model
      readme: "# Meta-Llama-3.1-8B-Instruct-quantized.w4a16\n\n## Model Overview\n- **Model Architecture:** Meta-Llama-3\n  - **Input:** Text\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Weight quantization:** INT4\n- **Intended Use Cases:** Intended for commercial and research use in English. Similarly to [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct), this models is intended for assistant-like chat.\n- **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English.\n- **Release Date:** 7/26/2024\n- **Version:** 1.0\n- **License(s):** Llama3.1\n- **Model Developers:** Neural Magic\n\nThis model is a quantized version of [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).\nIt was evaluated on a several tasks to assess the its quality in comparison to the unquatized model, including multiple-choice, math reasoning, and open-ended text generation.\nMeta-Llama-3.1-8B-Instruct-quantized.w4a16 achieves 93.0% recovery for the Arena-Hard evaluation, 98.9% for OpenLLM v1 (using Meta's prompting when available), 96.1% for OpenLLM v2, 99.7% for HumanEval pass@1, and 97.4% for HumanEval+ pass@1.\n\n### Model Optimizations\n\nThis model was obtained by quantizing the weights of [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) to INT4 data type.\nThis optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 75%.\n\nOnly the weights of the linear operators within transformers blocks are quantized. Symmetric per-channel quantization is applied, in which a linear scaling per output dimension maps the INT8 and floating point representations of the quantized weights.\n[AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ) is used for quantization with 10% damping factor and 768 sequences taken from Neural Magic's [LLM compression calibration dataset](https://huggingface.co/datasets/neuralmagic/LLM_compression_calibration).\n\n\n## Deployment\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel_id = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\"\nnumber_gpus = 1\nmax_model_len = 8192\n\nsampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\nprompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n\nllm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=max_model_len)\n\noutputs = llm.generate(prompts, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\nvLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n\n## Creation\n\nThis model was created by applying the [AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ) library as presented in the code snipet below.\nAlthough AutoGPTQ was used for this particular model, Neural Magic is transitioning to using [llm-compressor](https://github.com/vllm-project/llm-compressor) which supports several quantization schemes and models not supported by AutoGPTQ.\n\n```python\nfrom transformers import AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom datasets import load_dataset\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\nnum_samples = 756\nmax_seq_len = 4064\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ndef preprocess_fn(example):\n  return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], add_generation_prompt=False, tokenize=False)}\n\nds = load_dataset(\"neuralmagic/LLM_compression_calibration\", split=\"train\")\nds = ds.shuffle().select(range(num_samples))\nds = ds.map(preprocess_fn)\n\nexamples = [tokenizer(example[\"text\"], padding=False, max_length=max_seq_len, truncation=True) for example in ds]\n    \nquantize_config = BaseQuantizeConfig(\n  bits=4,\n  group_size=128,\n  desc_act=True,\n  model_file_base_name=\"model\",\n  damp_percent=0.1,\n)\n\nmodel = AutoGPTQForCausalLM.from_pretrained(\n  model_id,\n  quantize_config,\n  device_map=\"auto\",\n)\n\nmodel.quantize(examples)\nmodel.save_pretrained(\"Meta-Llama-3.1-8B-Instruct-quantized.w4a16\")\n```\n\n## Evaluation\n\nThis model was evaluated on the well-known Arena-Hard, OpenLLM v1, OpenLLM v2, HumanEval, and HumanEval+ benchmarks.\nIn all cases, model outputs were generated with the [vLLM](https://docs.vllm.ai/en/stable/) engine.\n\nArena-Hard evaluations were conducted using the [Arena-Hard-Auto](https://github.com/lmarena/arena-hard-auto) repository.\nThe model generated a single answer for each prompt form Arena-Hard, and each answer was judged twice by GPT-4.\nWe report below the scores obtained in each judgement and the average.\n\nOpenLLM v1 and v2 evaluations were conducted using Neural Magic's fork of [lm-evaluation-harness](https://github.com/neuralmagic/lm-evaluation-harness/tree/llama_3.1_instruct) (branch llama_3.1_instruct).\nThis version of the lm-evaluation-harness includes versions of MMLU, ARC-Challenge and GSM-8K that match the prompting style of [Meta-Llama-3.1-Instruct-evals](https://huggingface.co/datasets/meta-llama/Meta-Llama-3.1-8B-Instruct-evals) and a few fixes to OpenLLM v2 tasks.\n\nHumanEval and HumanEval+ evaluations were conducted using Neural Magic's fork of the [EvalPlus](https://github.com/neuralmagic/evalplus) repository.\n\nDetailed model outputs are available as HuggingFace datasets for [Arena-Hard](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-arena-hard-evals), [OpenLLM v2](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-leaderboard-v2-evals), and [HumanEval](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-humaneval-evals).\n\n**Note:** Results have been updated after Meta modified the chat template.\n\n### Accuracy\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Meta-Llama-3.1-8B-Instruct </strong>\n   </td>\n   <td><strong>Meta-Llama-3.1-8B-Instruct-quantized.w4a16 (this model)</strong>\n   </td>\n   <td><strong>Recovery</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"1\" ><strong>LLM as a judge</strong>\n   </td>    \n   <td>Arena Hard\n   </td>\n   <td>25.8 (25.1 / 26.5)\n   </td>\n   <td>27.2 (27.6 / 26.7)\n   </td>\n   <td>105.4%\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"8\" ><strong>OpenLLM v1</strong>\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>68.3\n   </td>\n   <td>66.9\n   </td>\n   <td>97.9%\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (CoT, 0-shot)\n   </td>\n   <td>72.8\n   </td>\n   <td>71.1\n   </td>\n   <td>97.6%\n   </td>\n  </tr>\n  <tr>\n   <td>ARC Challenge (0-shot)\n   </td>\n   <td>81.4\n   </td>\n   <td>80.2\n   </td>\n   <td>98.0%\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (CoT, 8-shot, strict-match)\n   </td>\n   <td>82.8\n   </td>\n   <td>82.9\n   </td>\n   <td>100.2%\n   </td>\n  </tr>\n  <tr>\n   <td>Hellaswag (10-shot)\n   </td>\n   <td>80.5\n   </td>\n   <td>79.9\n   </td>\n   <td>99.3%\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>78.1\n   </td>\n   <td>78.0\n   </td>\n   <td>99.9%\n   </td>\n  </tr>\n  <tr>\n   <td>TruthfulQA (0-shot, mc2)\n   </td>\n   <td>54.5\n   </td>\n   <td>52.8\n   </td>\n   <td>96.9%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>74.3</strong>\n   </td>\n   <td><strong>73.5</strong>\n   </td>\n   <td><strong>98.9%</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" ><strong>OpenLLM v2</strong>\n   </td>\n   <td>MMLU-Pro (5-shot)\n   </td>\n   <td>30.8\n   </td>\n   <td>28.8\n   </td>\n   <td>93.6%\n   </td>\n  </tr>\n  <tr>\n   <td>IFEval (0-shot)\n   </td>\n   <td>77.9\n   </td>\n   <td>76.3\n   </td>\n   <td>98.0%\n   </td>\n  </tr>\n  <tr>\n   <td>BBH (3-shot)\n   </td>\n   <td>30.1\n   </td>\n   <td>28.9\n   </td>\n   <td>96.1%\n   </td>\n  </tr>\n  <tr>\n   <td>Math-lvl-5 (4-shot)\n   </td>\n   <td>15.7\n   </td>\n   <td>14.8\n   </td>\n   <td>94.4%\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA (0-shot)\n   </td>\n   <td>3.7\n   </td>\n   <td>4.0\n   </td>\n   <td>109.8%\n   </td>\n  </tr>\n  <tr>\n   <td>MuSR (0-shot)\n   </td>\n   <td>7.6\n   </td>\n   <td>6.3\n   </td>\n   <td>83.2%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>27.6</strong>\n   </td>\n   <td><strong>26.5</strong>\n   </td>\n   <td><strong>96.1%</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" ><strong>Coding</strong>\n   </td>\n   <td>HumanEval pass@1\n   </td>\n   <td>67.3\n   </td>\n   <td>67.1\n   </td>\n   <td>99.7%\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval+ pass@1\n   </td>\n   <td>60.7\n   </td>\n   <td>59.1\n   </td>\n   <td>97.4%\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"9\" ><strong>Multilingual</strong>\n   </td>\n   <td>Portuguese MMLU (5-shot)\n   </td>\n   <td>59.96\n   </td>\n   <td>58.69\n   </td>\n   <td>97.9%\n   </td>\n  </tr>\n  <tr>\n   <td>Spanish MMLU (5-shot)\n   </td>\n   <td>60.25\n   </td>\n   <td>58.39\n   </td>\n   <td>96.9%\n   </td>\n  </tr>\n  <tr>\n   <td>Italian MMLU (5-shot)\n   </td>\n   <td>59.23\n   </td>\n   <td>57.82\n   </td>\n   <td>97.6%\n   </td>\n  </tr>\n  <tr>\n   <td>German MMLU (5-shot)\n   </td>\n   <td>58.63\n   </td>\n   <td>56.22\n   </td>\n   <td>95.9%\n   </td>\n  </tr>\n  <tr>\n   <td>French MMLU (5-shot)\n   </td>\n   <td>59.65\n   </td>\n   <td>57.58\n   </td>\n   <td>96.5%\n   </td>\n  </tr>\n  <tr>\n   <td>Hindi MMLU (5-shot)\n   </td>\n   <td>50.10\n   </td>\n   <td>47.14\n   </td>\n   <td>94.1%\n   </td>\n  </tr>\n  <tr>\n   <td>Thai MMLU (5-shot)\n   </td>\n   <td>49.12\n   </td>\n   <td>46.72\n   </td>\n   <td>95.1%\n   </td>\n  </tr>\n</table>\n\n\n### Reproduction\n\nThe results were obtained using the following commands:\n\n#### MMLU\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU-CoT\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=4064,max_gen_toks=1024,tensor_parallel_size=1 \\\n  --tasks mmlu_cot_0shot_llama_3.1_instruct \\\n  --apply_chat_template \\\n  --num_fewshot 0 \\\n  --batch_size auto\n```\n\n#### ARC-Challenge\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=3940,max_gen_toks=100,tensor_parallel_size=1 \\\n  --tasks arc_challenge_llama_3.1_instruct \\\n  --apply_chat_template \\\n  --num_fewshot 0 \\\n  --batch_size auto\n```\n\n#### GSM-8K\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=4096,max_gen_toks=1024,tensor_parallel_size=1 \\\n  --tasks gsm8k_cot_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 8 \\\n  --batch_size auto\n```\n\n#### Hellaswag\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \\\n  --tasks hellaswag \\\n  --num_fewshot 10 \\\n  --batch_size auto\n```\n\n#### Winogrande\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \\\n  --tasks winogrande \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### TruthfulQA\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \\\n  --tasks truthfulqa \\\n  --num_fewshot 0 \\\n  --batch_size auto\n```\n\n#### OpenLLM v2\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=4096,tensor_parallel_size=1,enable_chunked_prefill=True \\\n  --apply_chat_template \\\n  --fewshot_as_multiturn \\\n  --tasks leaderboard \\\n  --batch_size auto\n```\n\n#### MMLU Portuguese\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_pt_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU Spanish\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_es_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU Italian\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_it_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU German\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_de_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU French\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_fr_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU Hindi\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_hi_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU Thai\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_th_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### HumanEval and HumanEval+\n##### Generation\n```\npython3 codegen/generate.py \\\n  --model neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16 \\\n  --bs 16 \\\n  --temperature 0.2 \\\n  --n_samples 50 \\\n  --root \".\" \\\n  --dataset humaneval\n```\n##### Sanitization\n```\npython3 evalplus/sanitize.py \\\n  humaneval/neuralmagic--Meta-Llama-3.1-8B-Instruct-quantized.w4a16_vllm_temp_0.2\n```\n##### Evaluation\n```\nevalplus.evaluate \\\n  --dataset humaneval \\\n  --samples humaneval/neuralmagic--Meta-Llama-3.1-8B-Instruct-quantized.w4a16_vllm_temp_0.2-sanitized\n```\n"
      language:
        - en
      license: Llama3.1
      licenseLink: https://github.com/meta-llvm/llama-models/blob/main/models/llama3_1/LICENSE
      tasks:
        - text-generation
      createTimeSinceEpoch: "1721952000"
      lastUpdateTimeSinceEpoch: "1721952000"
      customProperties:
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        granite:
            metadataType: MetadataStringValue
            string_value: ""
        int4:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
        w4a16:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct-quantized-w4a16:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744136202000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/granite-3.1-8b-instruct-quantized.w8a8
      provider: Neural Magic
      description: Granite 3.1 8b Instruct Quantized.w8a8 - An instruction-tuned language model
      readme: "# granite-3.1-8b-instruct-quantized.w8a8\n\n## Model Overview\n- **Model Architecture:** granite-3.1-8b-instruct\n  - **Input:** Text\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Weight quantization:** INT8\n  - **Activation quantization:** INT8\n- **Release Date:** 1/8/2025\n- **Version:** 1.0\n- **Model Developers:** Neural Magic\n\nQuantized version of [ibm-granite/granite-3.1-8b-instruct](https://huggingface.co/ibm-granite/granite-3.1-8b-instruct).\nIt achieves an average score of 70.26 on the [OpenLLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) benchmark (version 1), whereas the unquantized model achieves 70.30.\n\n### Model Optimizations\n\nThis model was obtained by quantizing the weights and activations of [ibm-granite/granite-3.1-8b-instruct](https://huggingface.co/ibm-granite/granite-3.1-8b-instruct) to INT8 data type, ready for inference with vLLM >= 0.5.2.\nThis optimization reduces the number of bits per parameter from 16 to 8, reducing the disk size and GPU memory requirements by approximately 50%. Only the weights and activations of the linear operators within transformers blocks are quantized. \n\n## Deployment\n\n### Use with vLLM\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\nmax_model_len, tp_size = 4096, 1\nmodel_name = \"neuralmagic/granite-3.1-8b-instruct-quantized.w8a8\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\n\nmessages_list = [\n    [{\"role\": \"user\", \"content\": \"Who are you? Please respond in pirate speak!\"}],\n]\n\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\n\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n```\n\nvLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\nThis model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n<details>\n  <summary>Model Creation Code</summary>\n\n```bash\npython quantize.py --model_path ibm-granite/granite-3.1-8b-instruct --quant_path \"output_dir/granite-3.1-8b-instruct-quantized.w8a8\" --calib_size 3072 --dampening_frac 0.1 --observer mse\n```\n\n\n```python\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom llmcompressor.modifiers.quantization import GPTQModifier\nfrom llmcompressor.modifiers.smoothquant import SmoothQuantModifier\nfrom llmcompressor.transformers import oneshot, apply\nimport argparse\nfrom compressed_tensors.quantization import QuantizationScheme, QuantizationArgs, QuantizationType, QuantizationStrategy\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--model_path', type=str)\nparser.add_argument('--quant_path', type=str)\nparser.add_argument('--calib_size', type=int, default=256)\nparser.add_argument('--dampening_frac', type=float, default=0.1) \nparser.add_argument('--observer', type=str, default=\"minmax\")\nargs = parser.parse_args()\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    args.model_path,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    use_cache=False,\n    trust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(args.model_path)\n\nNUM_CALIBRATION_SAMPLES = args.calib_size\nDATASET_ID = \"neuralmagic/LLM_compression_calibration\"\nDATASET_SPLIT = \"train\"\nds = load_dataset(DATASET_ID, split=DATASET_SPLIT)\nds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n\ndef preprocess(example):\n    return {\"text\": example[\"text\"]}\n\nds = ds.map(preprocess)\n\ndef tokenize(sample):\n    return tokenizer(\n        sample[\"text\"],\n        padding=False,\n        truncation=False,\n        add_special_tokens=True,\n    )\n\n\nds = ds.map(tokenize, remove_columns=ds.column_names)\n\nignore=[\"lm_head\"]\nmappings=[\n    [[\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"], \"re:.*input_layernorm\"],\n    [[\"re:.*gate_proj\", \"re:.*up_proj\"], \"re:.*post_attention_layernorm\"],\n    [[\"re:.*down_proj\"], \"re:.*up_proj\"]\n]\n\nrecipe = [\n    SmoothQuantModifier(smoothing_strength=0.8, ignore=ignore, mappings=mappings),\n    GPTQModifier(\n        targets=[\"Linear\"],\n        ignore=[\"lm_head\"],\n        scheme=\"W8A8\",\n        dampening_frac=args.dampening_frac,\n        observer=args.observer,\n    )\n]\noneshot(\n    model=model,\n    dataset=ds,\n    recipe=recipe,\n    num_calibration_samples=args.calib_size,\n    max_seq_length=8196,\n)\n\n# Save to disk compressed.\nmodel.save_pretrained(quant_path, save_compressed=True)\ntokenizer.save_pretrained(quant_path)\n```\n</details>\n\n## Evaluation\n\nThe model was evaluated on OpenLLM Leaderboard [V1](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard), OpenLLM Leaderboard [V2](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/) and on [HumanEval](https://github.com/neuralmagic/evalplus), using the following commands:\n\n<details>\n<summary>Evaluation Commands</summary>\n  \nOpenLLM Leaderboard V1:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/granite-3.1-8b-instruct-quantized.w8a8\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \\\n  --tasks openllm \\\n  --write_out \\\n  --batch_size auto \\\n  --output_path output_dir \\\n  --show_config\n```\n\nOpenLLM Leaderboard V2:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/granite-3.1-8b-instruct-quantized.w8a8\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True \\\n  --tasks leaderboard \\\n  --write_out \\\n  --batch_size auto \\\n  --output_path output_dir \\\n  --show_config\n```\n\n#### HumanEval\n##### Generation\n```\npython3 codegen/generate.py \\\n  --model neuralmagic/granite-3.1-8b-instruct-quantized.w8a8 \\\n  --bs 16 \\\n  --temperature 0.2 \\\n  --n_samples 50 \\\n  --root \".\" \\\n  --dataset humaneval\n```\n##### Sanitization\n```\npython3 evalplus/sanitize.py \\\n  humaneval/neuralmagic--granite-3.1-8b-instruct-quantized.w8a8_vllm_temp_0.2\n```\n##### Evaluation\n```\nevalplus.evaluate \\\n  --dataset humaneval \\\n  --samples humaneval/neuralmagic--granite-3.1-8b-instruct-quantized.w8a8_vllm_temp_0.2-sanitized\n```\n</details>\n\n### Accuracy\n\n<table>\n  <thead>\n    <tr>\n      <th>Category</th>\n      <th>Metric</th>\n      <th>ibm-granite/granite-3.1-8b-instruct</th>\n      <th>neuralmagic/granite-3.1-8b-instruct-quantized.w8a8</th>\n      <th>Recovery (%)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <!-- OpenLLM Leaderboard V1 -->\n    <tr>\n      <td rowspan=\"7\"><b>OpenLLM V1</b></td>\n      <td>ARC-Challenge (Acc-Norm, 25-shot)</td>\n      <td>66.81</td>\n      <td>67.06</td>\n      <td>100.37</td>\n    </tr>\n    <tr>\n      <td>GSM8K (Strict-Match, 5-shot)</td>\n      <td>64.52</td>\n      <td>65.66</td>\n      <td>101.77</td>\n    </tr>\n    <tr>\n      <td>HellaSwag (Acc-Norm, 10-shot)</td>\n      <td>84.18</td>\n      <td>83.93</td>\n      <td>99.70</td>\n    </tr>\n    <tr>\n      <td>MMLU (Acc, 5-shot)</td>\n      <td>65.52</td>\n      <td>65.03</td>\n      <td>99.25</td>\n    </tr>\n    <tr>\n      <td>TruthfulQA (MC2, 0-shot)</td>\n      <td>60.57</td>\n      <td>60.02</td>\n      <td>99.09</td>\n    </tr>\n    <tr>\n      <td>Winogrande (Acc, 5-shot)</td>\n      <td>80.19</td>\n      <td>79.87</td>\n      <td>99.60</td>\n    </tr>\n    <tr>\n      <td><b>Average Score</b></td>\n      <td><b>70.30</b></td>\n      <td><b>70.26</b></td>\n      <td><b>99.95</b></td>\n    </tr>\n    <!-- OpenLLM Leaderboard V2 -->\n    <tr>\n      <td rowspan=\"7\"><b>OpenLLM V2</b></td>\n      <td>IFEval (Inst Level Strict Acc, 0-shot)</td>\n      <td>74.01</td>\n      <td>73.50</td>\n      <td>99.31</td>\n    </tr>\n    <tr>\n      <td>BBH (Acc-Norm, 3-shot)</td>\n      <td>53.19</td>\n      <td>52.59</td>\n      <td>98.87</td>\n    </tr>\n    <tr>\n      <td>Math-Hard (Exact-Match, 4-shot)</td>\n      <td>14.77</td>\n      <td>15.73</td>\n      <td>106.50</td>\n    </tr>\n    <tr>\n      <td>GPQA (Acc-Norm, 0-shot)</td>\n      <td>31.76</td>\n      <td>30.62</td>\n      <td>96.40</td>\n    </tr>\n    <tr>\n      <td>MUSR (Acc-Norm, 0-shot)</td>\n      <td>46.01</td>\n      <td>44.30</td>\n      <td>96.28</td>\n    </tr>\n    <tr>\n      <td>MMLU-Pro (Acc, 5-shot)</td>\n      <td>35.81</td>\n      <td>35.41</td>\n      <td>98.88</td>\n    </tr>\n    <tr>\n      <td><b>Average Score</b></td>\n      <td><b>42.61</b></td>\n      <td><b>42.03</b></td>\n      <td><b>98.64</b></td>\n    </tr>\n    <!-- HumanEval -->\n    <tr>\n      <td rowspan=\"2\"><b>Coding</b></td>\n      <td>HumanEval Pass@1</td>\n      <td>71.00</td>\n      <td>70.50</td>\n      <td><b>99.30</b></td>\n    </tr>\n  </tbody>\n</table>\n\n\n\n## Inference Performance\n\n\nThis model achieves up to 1.6x speedup in single-stream deployment and up to 1.7x speedup in multi-stream asynchronous deployment, depending on hardware and use-case scenario.\nThe following performance benchmarks were conducted with [vLLM](https://docs.vllm.ai/en/latest/) version 0.6.6.post1, and [GuideLLM](https://github.com/neuralmagic/guidellm).\n\n<details>\n<summary>Benchmarking Command</summary>\n\n```\nguidellm --model neuralmagic/granite-3.1-8b-instruct-quantized.w8a8 --target \"http://localhost:8000/v1\" --data-type emulated --data \"prompt_tokens=<prompt_tokens>,generated_tokens=<generated_tokens>\" --max seconds 360 --backend aiohttp_server\n```\n\n</details>\n\n### Single-stream performance (measured with vLLM version 0.6.6.post1)\n<table>\n  <tr>\n    <td></td>\n    <td></td>\n    <td></td>\n    <th style=\"text-align: center;\" colspan=\"7\" >Latency (s)</th>\n  </tr>\n  <tr>\n    <th>GPU class</th>\n    <th>Model</th>\n    <th>Speedup</th>\n    <th>Code Completion<br>prefill: 256 tokens<br>decode: 1024 tokens</th>\n    <th>Docstring Generation<br>prefill: 768 tokens<br>decode: 128 tokens</th>\n    <th>Code Fixing<br>prefill: 1024 tokens<br>decode: 1024 tokens</th>\n    <th>RAG<br>prefill: 1024 tokens<br>decode: 128 tokens</th>\n    <th>Instruction Following<br>prefill: 256 tokens<br>decode: 128 tokens</th>\n    <th>Multi-turn Chat<br>prefill: 512 tokens<br>decode: 256 tokens</th>\n    <th>Large Summarization<br>prefill: 4096 tokens<br>decode: 512 tokens</th>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A5000</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>28.3</td>\n    <td>3.7</td>\n    <td>28.8</td>\n    <td>3.8</td>\n    <td>3.6</td>\n    <td>7.2</td>\n    <td>15.7</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w8a8<br>(this model)</td>\n    <td>1.60</td>\n    <td>17.7</td>\n    <td>2.3</td>\n    <td>18.0</td>\n    <td>2.4</td>\n    <td>2.2</td>\n    <td>4.5</td>\n    <td>10.0</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16</td>\n    <td>2.61</td>\n    <td>10.3</td>\n    <td>1.5</td>\n    <td>10.7</td>\n    <td>1.5</td>\n    <td>1.3</td>\n    <td>2.7</td>\n    <td>6.6</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A6000</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>25.8</td>\n    <td>3.4</td>\n    <td>26.2</td>\n    <td>3.4</td>\n    <td>3.3</td>\n    <td>6.5</td>\n    <td>14.2</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w8a8<br>(this model)</td>\n    <td>1.50</td>\n    <td>17.4</td>\n    <td>2.3</td>\n    <td>16.9</td>\n    <td>2.2</td>\n    <td>2.2</td>\n    <td>4.4</td>\n    <td>9.8</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16</td>\n    <td>2.48</td>\n    <td>10.0</td>\n    <td>1.4</td>\n    <td>10.4</td>\n    <td>1.5</td>\n    <td>1.3</td>\n    <td>2.5</td>\n    <td>6.2</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A100</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>13.6</td>\n    <td>1.8</td>\n    <td>13.7</td>\n    <td>1.8</td>\n    <td>1.7</td>\n    <td>3.4</td>\n    <td>7.3</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w8a8<br>(this model)</td>\n    <td>1.31</td>\n    <td>10.4</td>\n    <td>1.3</td>\n    <td>10.5</td>\n    <td>1.4</td>\n    <td>1.3</td>\n    <td>2.6</td>\n    <td>5.6</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16</td>\n    <td>1.80</td>\n    <td>7.3</td>\n    <td>1.0</td>\n    <td>7.4</td>\n    <td>1.0</td>\n    <td>0.9</td>\n    <td>1.9</td>\n    <td>4.3</td>\n  </tr>\n</table>\n\n\n### Multi-stream asynchronous performance (measured with vLLM version 0.6.6.post1)\n<table>\n  <tr>\n    <td></td>\n    <td></td>\n    <td></td>\n    <th style=\"text-align: center;\" colspan=\"7\" >Maximum Throughput (Queries per Second)</th>\n  </tr>\n  <tr>\n    <th>GPU class</th>\n    <th>Model</th>\n    <th>Speedup</th>\n    <th>Code Completion<br>prefill: 256 tokens<br>decode: 1024 tokens</th>\n    <th>Docstring Generation<br>prefill: 768 tokens<br>decode: 128 tokens</th>\n    <th>Code Fixing<br>prefill: 1024 tokens<br>decode: 1024 tokens</th>\n    <th>RAG<br>prefill: 1024 tokens<br>decode: 128 tokens</th>\n    <th>Instruction Following<br>prefill: 256 tokens<br>decode: 128 tokens</th>\n    <th>Multi-turn Chat<br>prefill: 512 tokens<br>decode: 256 tokens</th>\n    <th>Large Summarization<br>prefill: 4096 tokens<br>decode: 512 tokens</th>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A5000</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>0.8</td>\n    <td>3.1</td>\n    <td>0.4</td>\n    <td>2.5</td>\n    <td>6.7</td>\n    <td>2.7</td>\n    <td>0.3</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w8a8<br>(this model)</td>\n    <td>1.71</td>\n    <td>1.3</td>\n    <td>5.2</td>\n    <td>0.9</td>\n    <td>4.0</td>\n    <td>10.5</td>\n    <td>4.4</td>\n    <td>0.5</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16</td>\n    <td>1.46</td>\n    <td>1.3</td>\n    <td>3.9</td>\n    <td>0.8</td>\n    <td>2.9</td>\n    <td>8.2</td>\n    <td>3.6</td>\n    <td>0.5</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A6000</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>1.3</td>\n    <td>5.1</td>\n    <td>0.9</td>\n    <td>4.0</td>\n    <td>0.3</td>\n    <td>4.3</td>\n    <td>0.6</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w8a8<br>(this model)</td>\n    <td>1.39</td>\n    <td>1.8</td>\n    <td>7.0</td>\n    <td>1.3</td>\n    <td>5.6</td>\n    <td>14.0</td>\n    <td>6.3</td>\n    <td>0.8</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16</td>\n    <td>1.09</td>\n    <td>1.9</td>\n    <td>4.8</td>\n    <td>1.0</td>\n    <td>3.8</td>\n    <td>10.0</td>\n    <td>5.0</td>\n    <td>0.6</td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\" rowspan=\"3\" >A100</td>\n    <td>granite-3.1-8b-instruct</td>\n    <td></td>\n    <td>3.1</td>\n    <td>10.7</td>\n    <td>2.1</td>\n    <td>8.5</td>\n    <td>20.6</td>\n    <td>9.6</td>\n    <td>1.4</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w8a8<br>(this model)</td>\n    <td>1.23</td>\n    <td>3.8</td>\n    <td>14.2</td>\n    <td>2.1</td>\n    <td>11.4</td>\n    <td>25.9</td>\n    <td>12.1</td>\n    <td>1.7</td>\n  </tr>\n  <tr>\n    <td>granite-3.1-8b-instruct-quantized.w4a16</td>\n    <td>0.96</td>\n    <td>3.4</td>\n    <td>9.0</td>\n    <td>2.6</td>\n    <td>7.2</td>\n    <td>18.0</td>\n    <td>8.8</td>\n    <td>1.3</td>\n  </tr>\n</table>\n"
      language:
        - en
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-generation
      createTimeSinceEpoch: "1736294400"
      lastUpdateTimeSinceEpoch: "1736294400"
      customProperties:
        8-bit:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        granite:
            metadataType: MetadataStringValue
            string_value: ""
        int8:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
        w8a8:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct-quantized-w8a8:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744994076000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/granite-3.1-8b-instruct-quantized.w8a8
      provider: Neural Magic
      description: Meta Llama 3.1 8B Instruct Quantized.w8a8 - An instruction-tuned language model
      readme: "# Meta-Llama-3.1-8B-Instruct-quantized.w8a8\n\n## Model Overview\n- **Model Architecture:** Llama\n  - **Input:** Text\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Activation quantization:** INT8\n  - **Weight quantization:** INT8\n- **Intended Use Cases:** Intended for commercial and research use multiple languages. Similarly to [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct), this models is intended for assistant-like chat.\n- **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws).\n- **Release Date:** 7/11/2024\n- **Version:** 1.0\n- **Model Developers:** Neural Magic\n\nThis model is a quantized version of [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).\nIt was evaluated on a several tasks to assess the its quality in comparison to the unquatized model, including multiple-choice, math reasoning, and open-ended text generation.\nMeta-Llama-3.1-8B-Instruct-quantized.w8a8 achieves 105.4% recovery for the Arena-Hard evaluation, 100.3% for OpenLLM v1 (using Meta's prompting when available), 101.5% for OpenLLM v2, 99.7% for HumanEval pass@1, and 98.8% for HumanEval+ pass@1.\n\n### Model Optimizations\n\nThis model was obtained by quantizing the weights of [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) to INT8 data type.\nThis optimization reduces the number of bits used to represent weights and activations from 16 to 8, reducing GPU memory requirements (by approximately 50%) and increasing matrix-multiply compute throughput (by approximately 2x).\nWeight quantization also reduces disk size requirements by approximately 50%.\n\nOnly weights and activations of the linear operators within transformers blocks are quantized.\nWeights are quantized with a symmetric static per-channel scheme, where a fixed linear scaling factor is applied between INT8 and floating point representations for each output channel dimension.\nActivations are quantized with a symmetric dynamic per-token scheme, computing a linear scaling factor at runtime for each token between INT8 and floating point representations.\nThe [GPTQ](https://arxiv.org/abs/2210.17323) algorithm is applied for quantization, as implemented in the [llm-compressor](https://github.com/vllm-project/llm-compressor) library.\nGPTQ used a 1% damping factor and 256 sequences of 8,192 random tokens.\n\n\n## Deployment\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel_id = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\"\nnumber_gpus = 1\nmax_model_len = 8192\n\nsampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\nprompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n\nllm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=max_model_len)\n\noutputs = llm.generate(prompts, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\nvLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n\n## Creation\n\nThis model was created by using the [llm-compressor](https://github.com/vllm-project/llm-compressor) library as presented in the code snipet below.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import Dataset\nfrom llmcompressor.transformers import oneshot\nfrom llmcompressor.modifiers.quantization import GPTQModifier\nimport random\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\nnum_samples = 256\nmax_seq_len = 8192\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmax_token_id = len(tokenizer.get_vocab()) - 1\ninput_ids = [[random.randint(0, max_token_id) for _ in range(max_seq_len)] for _ in range(num_samples)]\nattention_mask = num_samples * [max_seq_len * [1]]\nds = Dataset.from_dict({\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n\nrecipe = GPTQModifier(\n  targets=\"Linear\",\n  scheme=\"W8A8\",\n  ignore=[\"lm_head\"],\n  dampening_frac=0.01,\n)\n\nmodel = SparseAutoModelForCausalLM.from_pretrained(\n  model_id,\n  device_map=\"auto\",\n)\n\noneshot(\n  model=model,\n  dataset=ds,\n  recipe=recipe,\n  max_seq_length=max_seq_len,\n  num_calibration_samples=num_samples,\n)\n\nmodel.save_pretrained(\"Meta-Llama-3.1-8B-Instruct-quantized.w8a8\")\n```\n\n\n## Evaluation\n\nThis model was evaluated on the well-known Arena-Hard, OpenLLM v1, OpenLLM v2, HumanEval, and HumanEval+ benchmarks.\nIn all cases, model outputs were generated with the [vLLM](https://docs.vllm.ai/en/stable/) engine.\n\nArena-Hard evaluations were conducted using the [Arena-Hard-Auto](https://github.com/lmarena/arena-hard-auto) repository.\nThe model generated a single answer for each prompt form Arena-Hard, and each answer was judged twice by GPT-4.\nWe report below the scores obtained in each judgement and the average.\n\nOpenLLM v1 and v2 evaluations were conducted using Neural Magic's fork of [lm-evaluation-harness](https://github.com/neuralmagic/lm-evaluation-harness/tree/llama_3.1_instruct) (branch llama_3.1_instruct).\nThis version of the lm-evaluation-harness includes versions of MMLU, ARC-Challenge and GSM-8K that match the prompting style of [Meta-Llama-3.1-Instruct-evals](https://huggingface.co/datasets/meta-llama/Meta-Llama-3.1-8B-Instruct-evals) and a few fixes to OpenLLM v2 tasks.\n\nHumanEval and HumanEval+ evaluations were conducted using Neural Magic's fork of the [EvalPlus](https://github.com/neuralmagic/evalplus) repository.\n\nDetailed model outputs are available as HuggingFace datasets for [Arena-Hard](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-arena-hard-evals), [OpenLLM v2](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-leaderboard-v2-evals), and [HumanEval](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-humaneval-evals).\n\n**Note:** Results have been updated after Meta modified the chat template.\n\n### Accuracy\n\n<table>\n  <tr>\n   <th>Category\n   </th>\n   <th>Benchmark\n   </th>\n   <th>Meta-Llama-3.1-8B-Instruct\n   </th>\n   <th>Meta-Llama-3.1-8B-Instruct-quantized.w8a8 (this model)\n   </th>\n   <th>Recovery\n   </th>\n  </tr>\n  <tr>\n   <td rowspan=\"1\" ><strong>LLM as a judge</strong>\n   </td>    \n   <td>Arena Hard\n   </td>\n   <td>25.8 (25.1 / 26.5)\n   </td>\n   <td>27.2 (27.6 / 26.7)\n   </td>\n   <td>105.4%\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"8\" ><strong>OpenLLM v1</strong>\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>68.3\n   </td>\n   <td>67.8\n   </td>\n   <td>99.3%\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (CoT, 0-shot)\n   </td>\n   <td>72.8\n   </td>\n   <td>72.2\n   </td>\n   <td>99.1%\n   </td>\n  </tr>\n  <tr>\n   <td>ARC Challenge (0-shot)\n   </td>\n   <td>81.4\n   </td>\n   <td>81.7\n   </td>\n   <td>100.3%\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (CoT, 8-shot, strict-match)\n   </td>\n   <td>82.8\n   </td>\n   <td>84.8\n   </td>\n   <td>102.5%\n   </td>\n  </tr>\n  <tr>\n   <td>Hellaswag (10-shot)\n   </td>\n   <td>80.5\n   </td>\n   <td>80.3\n   </td>\n   <td>99.8%\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>78.1\n   </td>\n   <td>78.5\n   </td>\n   <td>100.5%\n   </td>\n  </tr>\n  <tr>\n   <td>TruthfulQA (0-shot, mc2)\n   </td>\n   <td>54.5\n   </td>\n   <td>54.7\n   </td>\n   <td>100.3%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>74.1</strong>\n   </td>\n   <td><strong>74.3</strong>\n   </td>\n   <td><strong>100.3%</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" ><strong>OpenLLM v2</strong>\n   </td>\n   <td>MMLU-Pro (5-shot)\n   </td>\n   <td>30.8\n   </td>\n   <td>30.9\n   </td>\n   <td>100.3%\n   </td>\n  </tr>\n  <tr>\n   <td>IFEval (0-shot)\n   </td>\n   <td>77.9\n   </td>\n   <td>78.0\n   </td>\n   <td>100.1%\n   </td>\n  </tr>\n  <tr>\n   <td>BBH (3-shot)\n   </td>\n   <td>30.1\n   </td>\n   <td>31.0\n   </td>\n   <td>102.9%\n   </td>\n  </tr>\n  <tr>\n   <td>Math-lvl-5 (4-shot)\n   </td>\n   <td>15.7\n   </td>\n   <td>15.5\n   </td>\n   <td>98.9%\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA (0-shot)\n   </td>\n   <td>3.7\n   </td>\n   <td>5.4\n   </td>\n   <td>146.2%\n   </td>\n  </tr>\n  <tr>\n   <td>MuSR (0-shot)\n   </td>\n   <td>7.6\n   </td>\n   <td>7.6\n   </td>\n   <td>100.0%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>27.6</strong>\n   </td>\n   <td><strong>28.0</strong>\n   </td>\n   <td><strong>101.5%</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" ><strong>Coding</strong>\n   </td>\n   <td>HumanEval pass@1\n   </td>\n   <td>67.3\n   </td>\n   <td>67.1\n   </td>\n   <td>99.7%\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval+ pass@1\n   </td>\n   <td>60.7\n   </td>\n   <td>60.0\n   </td>\n   <td>98.8%\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"9\" ><strong>Multilingual</strong>\n   </td>\n   <td>Portuguese MMLU (5-shot)\n   </td>\n   <td>59.96\n   </td>\n   <td>59.36\n   </td>\n   <td>99.0%\n   </td>\n  </tr>\n  <tr>\n   <td>Spanish MMLU (5-shot)\n   </td>\n   <td>60.25\n   </td>\n   <td>59.77\n   </td>\n   <td>99.2%\n   </td>\n  </tr>\n  <tr>\n   <td>Italian MMLU (5-shot)\n   </td>\n   <td>59.23\n   </td>\n   <td>58.61\n   </td>\n   <td>99.0%\n   </td>\n  </tr>\n  <tr>\n   <td>German MMLU (5-shot)\n   </td>\n   <td>58.63\n   </td>\n   <td>58.23\n   </td>\n   <td>99.3%\n   </td>\n  </tr>\n  <tr>\n   <td>French MMLU (5-shot)\n   </td>\n   <td>59.65\n   </td>\n   <td>58.70\n   </td>\n   <td>98.4%\n   </td>\n  </tr>\n  <tr>\n   <td>Hindi MMLU (5-shot)\n   </td>\n   <td>50.10\n   </td>\n   <td>49.33\n   </td>\n   <td>98.5%\n   </td>\n  </tr>\n  <tr>\n   <td>Thai MMLU (5-shot)\n   </td>\n   <td>49.12\n   </td>\n   <td>48.09\n   </td>\n   <td>97.9%\n   </td>\n  </tr>\n</table>\n\n\n### Reproduction\n\nThe results were obtained using the following commands:\n\n#### MMLU\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU-CoT\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,max_model_len=4064,max_gen_toks=1024,tensor_parallel_size=1 \\\n  --tasks mmlu_cot_0shot_llama_3.1_instruct \\\n  --apply_chat_template \\\n  --num_fewshot 0 \\\n  --batch_size auto\n```\n\n#### ARC-Challenge\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,max_model_len=3940,max_gen_toks=100,tensor_parallel_size=1 \\\n  --tasks arc_challenge_llama_3.1_instruct \\\n  --apply_chat_template \\\n  --num_fewshot 0 \\\n  --batch_size auto\n```\n\n#### GSM-8K\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,max_model_len=4096,max_gen_toks=1024,tensor_parallel_size=1 \\\n  --tasks gsm8k_cot_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 8 \\\n  --batch_size auto\n```\n\n#### Hellaswag\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \\\n  --tasks hellaswag \\\n  --num_fewshot 10 \\\n  --batch_size auto\n```\n\n#### Winogrande\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \\\n  --tasks winogrande \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### TruthfulQA\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \\\n  --tasks truthfulqa \\\n  --num_fewshot 0 \\\n  --batch_size auto\n```\n\n#### OpenLLM v2\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,max_model_len=4096,tensor_parallel_size=1,enable_chunked_prefill=True \\\n  --apply_chat_template \\\n  --fewshot_as_multiturn \\\n  --tasks leaderboard \\\n  --batch_size auto\n```\n\n#### MMLU Portuguese\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_pt_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU Spanish\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_es_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU Italian\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_it_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU German\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_de_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU French\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_fr_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU Hindi\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_hi_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### MMLU Thai\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \\\n  --tasks mmlu_th_llama_3.1_instruct \\\n  --fewshot_as_multiturn \\\n  --apply_chat_template \\\n  --num_fewshot 5 \\\n  --batch_size auto\n```\n\n#### HumanEval and HumanEval+\n##### Generation\n```\npython3 codegen/generate.py \\\n  --model neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8 \\\n  --bs 16 \\\n  --temperature 0.2 \\\n  --n_samples 50 \\\n  --root \".\" \\\n  --dataset humaneval\n```\n##### Sanitization\n```\npython3 evalplus/sanitize.py \\\n  humaneval/neuralmagic--Meta-Llama-3.1-8B-Instruct-quantized.w8a8_vllm_temp_0.2\n```\n##### Evaluation\n```\nevalplus.evaluate \\\n  --dataset humaneval \\\n  --samples humaneval/neuralmagic--Meta-Llama-3.1-8B-Instruct-quantized.w8a8_vllm_temp_0.2-sanitized\n```\n"
      language:
        - en
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-generation
      createTimeSinceEpoch: "1720656000"
      lastUpdateTimeSinceEpoch: "1720656000"
      customProperties:
        8-bit:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        granite:
            metadataType: MetadataStringValue
            string_value: ""
        int8:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
        vllm:
            metadataType: MetadataStringValue
            string_value: ""
        w8a8:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct-quantized-w8a8:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744886948000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/phi-4
      provider: Microsoft Research
      description: '[Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905)'
      readme: "# Phi-4 Model Card \n\n[Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905)\n\n## Model Summary \n\n|                         |                                                                               |     \n|-------------------------|-------------------------------------------------------------------------------|\n| **Developers**          | Microsoft Research                                                            |\n| **Description**         | `phi-4` is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. The goal of this approach was to ensure that small capable models were trained with data focused on high quality and advanced reasoning.<br><br>`phi-4` underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures                |\n| **Architecture**        | 14B parameters, dense decoder-only Transformer model                          |\n| **Inputs**              | Text, best suited for prompts in the chat format                              |\n| **Context length**      | 16K tokens                                                                    |\n| **GPUs**                | 1920 H100-80G                                                                 |\n| **Training time**       | 21 days                                                                       |\n| **Training data**       | 9.8T tokens                                                                   |\n| **Outputs**             | Generated text in response to input                                           |\n| **Dates**               | October 2024 – November 2024                                                  |\n| **Status**              | Static model trained on an offline dataset with cutoff dates of June 2024 and earlier for publicly available data                                                                               |\n| **Release date**        | December 12, 2024                                                             |\n| **License**             | MIT                                                                         |\n\n## Intended Use \n\n|                               |                                                                         |\n|-------------------------------|-------------------------------------------------------------------------|\n| **Primary Use Cases**         | Our model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:<br><br>1. Memory/compute constrained environments.<br>2. Latency bound scenarios.<br>3. Reasoning and logic.                                                                       |\n| **Out-of-Scope Use Cases**    | Our models is not specifically designed or evaluated for all downstream purposes, thus:<br><br>1. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.<br>2. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model’s focus on English.<br>3. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.                                                              |\n\n## Data Overview \n\n### Training Datasets \n\nOur training data is an extension of the data used for Phi-3 and includes a wide variety of sources from:\n\n1. Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code.\n\n2. Newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.).\n\n3. Acquired academic books and Q&A datasets.\n\n4. High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nMultilingual data constitutes about 8% of our overall data. We are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge.\n\n#### Benchmark datasets \n\nWe evaluated `phi-4` using [OpenAI’s SimpleEval](https://github.com/openai/simple-evals) and our own internal benchmarks to understand the model’s capabilities, more specifically: \n\n* **MMLU:** Popular aggregated dataset for multitask language understanding.\n\n* **MATH:** Challenging competition math problems.\n\n* **GPQA:** Complex, graduate-level science questions.\n\n* **DROP:** Complex comprehension and reasoning.\n\n* **MGSM:** Multi-lingual grade-school math.\n\n* **HumanEval:** Functional code generation.\n\n* **SimpleQA:** Factual responses.\n\n## Safety \n\n### Approach \n\n`phi-4` has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated synthetic datasets. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and iterative DPO (Direct Preference Optimization), including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories. \n\n### Safety Evaluation and Red-Teaming \n\nPrior to release, `phi-4` followed a multi-faceted evaluation approach. Quantitative evaluation was conducted with multiple open-source safety benchmarks and in-house tools utilizing adversarial conversation simulation. For qualitative safety evaluation, we collaborated with the independent AI Red Team (AIRT) at Microsoft to assess safety risks posed by `phi-4` in both average and adversarial user scenarios. In the average user scenario, AIRT emulated typical single-turn and multi-turn interactions to identify potentially risky behaviors. The adversarial user scenario tested a wide range of techniques aimed at intentionally subverting the model’s safety training including jailbreaks, encoding-based attacks, multi-turn attacks, and adversarial suffix attacks.   \n\nPlease refer to the technical report for more details on safety alignment. \n\n## Model Quality\n\nTo understand the capabilities, we compare `phi-4` with a set of models over OpenAI’s SimpleEval benchmark. \n\nAt the high-level overview of the model quality on representative benchmarks. For the table below, higher numbers indicate better performance: \n\n| **Category**                 | **Benchmark** | **phi-4** (14B) | **phi-3** (14B) | **Qwen 2.5** (14B instruct) | **GPT-4o-mini** | **Llama-3.3** (70B instruct) | **Qwen 2.5** (72B instruct) | **GPT-4o** |\n|------------------------------|---------------|-----------|-----------------|----------------------|----------------------|--------------------|-------------------|-----------------|\n| Popular Aggregated Benchmark | MMLU          | 84.8      | 77.9            | 79.9                 | 81.8                 | 86.3               | 85.3              | **88.1**            |\n| Science                      | GPQA          | **56.1**      | 31.2            | 42.9                 | 40.9                 | 49.1               | 49.0              | 50.6            |\n| Math                         | MGSM<br>MATH  | 80.6<br>**80.4** | 53.5<br>44.6 | 79.6<br>75.6 | 86.5<br>73.0 | 89.1<br>66.3* | 87.3<br>80.0              | **90.4**<br>74.6            |\n| Code Generation              | HumanEval     | 82.6      | 67.8            | 72.1                 | 86.2                 | 78.9*               | 80.4              | **90.6**            |\n| Factual Knowledge            | SimpleQA      | 3.0       | 7.6            | 5.4                 | 9.9                  | 20.9               | 10.2              | **39.4**             |\n| Reasoning                    | DROP          | 75.5      | 68.3            | 85.5                 | 79.3                 | **90.2**               | 76.7              | 80.9            |\n\n\\* These scores are lower than those reported by Meta, perhaps because simple-evals has a strict formatting requirement that Llama models have particular trouble following. We use the simple-evals framework because it is reproducible, but Meta reports 77 for MATH and 88 for HumanEval on Llama-3.3-70B.\n\n## Usage\n\n### Input Formats\n\nGiven the nature of the training data, `phi-4` is best suited for prompts using the chat format as follows: \n\n```bash\n<|im_start|>system<|im_sep|>\nYou are a medieval knight and must provide explanations to modern people.<|im_end|>\n<|im_start|>user<|im_sep|>\nHow should I explain the Internet?<|im_end|>\n<|im_start|>assistant<|im_sep|>\n```\n\n### With `transformers`\n\n```python\nimport transformers\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=\"microsoft/phi-4\",\n    model_kwargs={\"torch_dtype\": \"auto\"},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a medieval knight and must provide explanations to modern people.\"},\n    {\"role\": \"user\", \"content\": \"How should I explain the Internet?\"},\n]\n\noutputs = pipeline(messages, max_new_tokens=128)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\n## Responsible AI Considerations\n\nLike other language models, `phi-4` can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:   \n\n* **Quality of Service:** The model is trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. `phi-4` is not intended to support multilingual use. \n\n* **Representation of Harms & Perpetuation of Stereotypes:** These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.  \n\n* **Inappropriate or Offensive Content:** These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.  \n\n* **Information Reliability:** Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.   \n\n* **Limited Scope for Code:** Majority of `phi-4` training data is based in Python and uses common packages such as `typing`, `math`, `random`, `collections`, `datetime`, `itertools`. If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.  \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Using safety services like [Azure AI Content Safety](https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety) that have advanced guardrails is highly recommended. Important areas for consideration include:\n\n* **Allocation:** Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. \n\n* **High-Risk Scenarios:** Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.  \n\n* **Misinformation:** Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).    \n\n* **Generation of Harmful Content:** Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.  \n\n* **Misuse:** Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations."
      language:
        - en
      license: mit
      licenseLink: https://opensource.org/licenses/MIT
      tasks:
        - text-generation
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1744993914000"
      customProperties:
        chat:
            metadataType: MetadataStringValue
            string_value: ""
        code:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        llmcompressor:
            metadataType: MetadataStringValue
            string_value: ""
        math:
            metadataType: MetadataStringValue
            string_value: ""
        neuralmagic:
            metadataType: MetadataStringValue
            string_value: ""
        nlp:
            metadataType: MetadataStringValue
            string_value: ""
        phi:
            metadataType: MetadataStringValue
            string_value: ""
        phi3:
            metadataType: MetadataStringValue
            string_value: ""
        redhat:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-phi-4:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744993914000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/phi-4-FP8-dynamic
      provider: RedHat (Neural Magic)
      description: Phi 4 Fp8 Dynamic - A large language model
      readme: "# phi-4-FP8-dynamic\n\n## Model Overview\n- **Model Architecture:** Phi3ForCausalLM\n  - **Input:** Text\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Activation quantization:** FP8\n  - **Weight quantization:** FP8\n- **Intended Use Cases:** This model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:\n  1. Memory/compute constrained environments.\n  2. Latency bound scenarios.\n  3. Reasoning and logic.\n- **Out-of-scope:** This model is not specifically designed or evaluated for all downstream purposes, thus:\n  1. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.\n  2. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model’s focus on English.\n  3. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\n- **Release Date:** 03/03/2025\n- **Version:** 1.0\n- **Model Developers:** RedHat (Neural Magic)\n\n\n### Model Optimizations\n\nThis model was obtained by quantizing activation and weights of [phi-4](https://huggingface.co/microsoft/phi-4) to FP8 data type.\nThis optimization reduces the number of bits used to represent weights and activations from 16 to 8, reducing GPU memory requirements (by approximately 50%) and increasing matrix-multiply compute throughput (by approximately 2x).\nWeight quantization also reduces disk size requirements by approximately 50%.\n\nOnly weights and activations of the linear operators within transformers blocks are quantized.\nWeights are quantized with a symmetric static per-channel scheme, whereas activations are quantized with a symmetric dynamic per-token scheme.\nThe [llm-compressor](https://github.com/vllm-project/llm-compressor) library is used for quantization.\n\n## Deployment\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel_id = \"neuralmagic-ent/phi-4-FP8-dynamic\"\nnumber_gpus = 1\n\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Give me a short introduction to large language model.\"},\n]\n\nprompts = tokenizer.apply_chat_template(messages, tokenize=False)\n\nllm = LLM(model=model_id, tensor_parallel_size=number_gpus)\n\noutputs = llm.generate(prompts, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\nvLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\n<details>\n  <summary>Creation details</summary>\n  This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n\n\n  ```python\n  from transformers import AutoModelForCausalLM, AutoTokenizer\n  from llmcompressor.modifiers.quantization import QuantizationModifier\n  from llmcompressor.transformers import oneshot\n  \n  # Load model\n  model_stub = \"microsoft/phi-4\"\n  model_name = model_stub.split(\"/\")[-1]\n  \n  tokenizer = AutoTokenizer.from_pretrained(model_stub)\n  \n  model = AutoModelForCausalLM.from_pretrained(\n      model_stub,\n      device_map=\"auto\",\n      torch_dtype=\"auto\",\n  )\n  \n  # Configure the quantization algorithm and scheme\n  recipe = QuantizationModifier(\n      targets=\"Linear\",\n      scheme=\"FP8_dynamic\",\n      ignore=[\"lm_head\"],\n  )\n  \n  # Apply quantization\n  oneshot(\n      model=model,\n      recipe=recipe,\n  )\n  \n  # Save to disk in compressed-tensors format\n  save_path = model_name + \"-FP8-dynamic\"\n  model.save_pretrained(save_path)\n  tokenizer.save_pretrained(save_path)\n  print(f\"Model and tokenizer saved to: {save_path}\")\n  ```\n</details>\n \n\n\n## Evaluation\n\nThe model was evaluated on the OpenLLM leaderboard tasks (version 1) with the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) and the [vLLM](https://docs.vllm.ai/en/stable/) engine, using the following command:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic-ent/phi-4-FP8-dynamic\",dtype=auto,gpu_memory_utilization=0.6,max_model_len=4096,enable_chunk_prefill=True,tensor_parallel_size=1 \\\n  --tasks openllm \\\n  --batch_size auto\n```\n\n### Accuracy\n\n#### Open LLM Leaderboard evaluation scores\n<table>\n  <tr>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>phi-4</strong>\n   </td>\n   <td><strong>phi-4-FP8-dynamic<br>(this model)</strong>\n   </td>\n   <td><strong>Recovery</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (5-shot)\n   </td>\n   <td>80.30\n   </td>\n   <td>80.30\n   </td>\n   <td>100.0%\n   </td>\n  </tr>\n  <tr>\n   <td>ARC Challenge (25-shot)\n   </td>\n   <td>64.42\n   </td>\n   <td>64.25\n   </td>\n   <td>99.7%\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (5-shot, strict-match)\n   </td>\n   <td>90.07\n   </td>\n   <td>90.67\n   </td>\n   <td>100.7%\n   </td>\n  </tr>\n  <tr>\n   <td>Hellaswag (10-shot)\n   </td>\n   <td>84.37\n   </td>\n   <td>84.19\n   </td>\n   <td>99.8%\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>80.58\n   </td>\n   <td>79.87\n   </td>\n   <td>99.1%\n   </td>\n  </tr>\n  <tr>\n   <td>TruthfulQA (0-shot, mc2)\n   </td>\n   <td>59.37\n   </td>\n   <td>59.54\n   </td>\n   <td>100.3%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>76.52</strong>\n   </td>\n   <td><strong>76.47</strong>\n   </td>\n   <td><strong>99.9%</strong>\n   </td>\n  </tr>\n</table>\n\n"
      language:
        - en
      license: mit
      licenseLink: https://opensource.org/licenses/MIT
      tasks:
        - text-generation
      createTimeSinceEpoch: "1740960000"
      lastUpdateTimeSinceEpoch: "1740960000"
      customProperties:
        FP8:
            metadataType: MetadataStringValue
            string_value: ""
        chat:
            metadataType: MetadataStringValue
            string_value: ""
        code:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        llmcompressor:
            metadataType: MetadataStringValue
            string_value: ""
        math:
            metadataType: MetadataStringValue
            string_value: ""
        neuralmagic:
            metadataType: MetadataStringValue
            string_value: ""
        nlp:
            metadataType: MetadataStringValue
            string_value: ""
        phi:
            metadataType: MetadataStringValue
            string_value: ""
        phi3:
            metadataType: MetadataStringValue
            string_value: ""
        quantized:
            metadataType: MetadataStringValue
            string_value: ""
        redhat:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-phi-4-fp8-dynamic:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744995118000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/phi-4-quantized.w4a16
      provider: Red Hat
      description: This model was obtained by quantizing the weights of phi-4 to INT4 data type.
      readme: |
        # phi-4-quantized.w4a16
      language:
        - en
      license: mit
      licenseLink: https://opensource.org/licenses/MIT
      tasks:
        - text-to-text
      createTimeSinceEpoch: "1740960000"
      lastUpdateTimeSinceEpoch: "1740960000"
      customProperties:
        INT4:
            metadataType: MetadataStringValue
            string_value: ""
        W4A16:
            metadataType: MetadataStringValue
            string_value: ""
        chat:
            metadataType: MetadataStringValue
            string_value: ""
        code:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        conversational:
            metadataType: MetadataStringValue
            string_value: ""
        llmcompressor:
            metadataType: MetadataStringValue
            string_value: ""
        math:
            metadataType: MetadataStringValue
            string_value: ""
        neuralmagic:
            metadataType: MetadataStringValue
            string_value: ""
        nlp:
            metadataType: MetadataStringValue
            string_value: ""
        phi:
            metadataType: MetadataStringValue
            string_value: ""
        phi3:
            metadataType: MetadataStringValue
            string_value: ""
        quantized:
            metadataType: MetadataStringValue
            string_value: ""
        redhat:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-phi-4-quantized-w4a16:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744135896000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: RedHatAI/phi-4-quantized.w8a8
      provider: RedHat (Neural Magic)
      description: Phi 4 Quantized.w8a8 - A large language model
      readme: "# phi-4-quantized.w8a8\n\n## Model Overview\n- **Model Architecture:** Phi3ForCausalLM\n  - **Input:** Text\n  - **Output:** Text\n- **Model Optimizations:**\n  - **Activation quantization:** INT8\n  - **Weight quantization:** INT8\n- **Intended Use Cases:** This model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:\n  1. Memory/compute constrained environments.\n  2. Latency bound scenarios.\n  3. Reasoning and logic.\n- **Out-of-scope:** This model is not specifically designed or evaluated for all downstream purposes, thus:\n  1. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.\n  2. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model’s focus on English.\n  3. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\n- **Release Date:** 03/03/2025\n- **Version:** 1.0\n- **Model Developers:** RedHat (Neural Magic)\n\n\n### Model Optimizations\n\nThis model was obtained by quantizing activations and weights of [phi-4](https://huggingface.co/microsoft/phi-4) to INT8 data type.\nThis optimization reduces the number of bits used to represent weights and activations from 16 to 8, reducing GPU memory requirements (by approximately 50%) and increasing matrix-multiply compute throughput (by approximately 2x).\nWeight quantization also reduces disk size requirements by approximately 50%.\n\nOnly weights and activations of the linear operators within transformers blocks are quantized.\nWeights are quantized with a symmetric static per-channel scheme, whereas activations are quantized with a symmetric dynamic per-token scheme.\nA combination of the [SmoothQuant](https://arxiv.org/abs/2211.10438) and [GPTQ](https://arxiv.org/abs/2210.17323) algorithms is applied for quantization, as implemented in the [llm-compressor](https://github.com/vllm-project/llm-compressor) library.\n\n\n## Deployment\n\nThis model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel_id = \"neuralmagic-ent/phi-4-quantized.w8a8\"\nnumber_gpus = 1\n\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Give me a short introduction to large language model.\"},\n]\n\nprompts = tokenizer.apply_chat_template(messages, tokenize=False)\n\nllm = LLM(model=model_id, tensor_parallel_size=number_gpus)\n\noutputs = llm.generate(prompts, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\nvLLM aslo supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.\n\n## Creation\n\n<details>\n  <summary>Creation details</summary>\n  This model was created with [llm-compressor](https://github.com/vllm-project/llm-compressor) by running the code snippet below. \n\n\n  ```python\n  from transformers import AutoModelForCausalLM, AutoTokenizer\n  from llmcompressor.modifiers.quantization import GPTQModifier\n  from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n  from llmcompressor.transformers import oneshot\n  from datasets import load_dataset\n\n  # Load model\n  model_stub = \"microsoft/phi-4\"\n  model_name = model_stub.split(\"/\")[-1]\n  \n  num_samples = 1024\n  max_seq_len = 8192\n  \n  tokenizer = AutoTokenizer.from_pretrained(model_stub)\n  \n  model = AutoModelForCausalLM.from_pretrained(\n      model_stub,\n      device_map=\"auto\",\n      torch_dtype=\"auto\",\n  )\n  \n  def preprocess_fn(example):\n    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], add_generation_prompt=False, tokenize=False)}\n  \n  ds = load_dataset(\"neuralmagic/LLM_compression_calibration\", split=\"train\")\n  ds = ds.map(preprocess_fn)\n  \n  # Configure the quantization algorithm and scheme\n  recipe = [\n      SmoothQuantModifier(\n          smoothing_strength=0.7,\n          mappings=[\n              [[\"re:.*qkv_proj\"], \"re:.*input_layernorm\"],\n              [[\"re:.*gate_up_proj\"], \"re:.*post_attention_layernorm\"],\n          ],\n      ),\n      GPTQModifier(\n          ignore=[\"lm_head\"],\n          sequential_targets=[\"Phi3DecoderLayer\"],\n          dampening_frac=0.01,\n          targets=\"Linear\",\n          scheme=\"W8A8\",\n      ),\n  ]\n  \n  # Apply quantization\n  oneshot(\n      model=model,\n      dataset=ds, \n      recipe=recipe,\n      max_seq_length=max_seq_len,\n      num_calibration_samples=num_samples,\n  )\n  \n  # Save to disk in compressed-tensors format\n  save_path = model_name + \"-quantized.w8a8\"\n  model.save_pretrained(save_path)\n  tokenizer.save_pretrained(save_path)\n  print(f\"Model and tokenizer saved to: {save_path}\")\n  ```\n</details>\n \n\n\n## Evaluation\n\nThe model was evaluated on the OpenLLM leaderboard tasks (version 1) with the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) and the [vLLM](https://docs.vllm.ai/en/stable/) engine, using the following command:\n```\nlm_eval \\\n  --model vllm \\\n  --model_args pretrained=\"neuralmagic-ent/phi-4-quantized.w8a8\",dtype=auto,gpu_memory_utilization=0.6,max_model_len=4096,enable_chunk_prefill=True,tensor_parallel_size=1 \\\n  --tasks openllm \\\n  --batch_size auto\n```\n\n### Accuracy\n\n#### Open LLM Leaderboard evaluation scores\n<table>\n  <tr>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>phi-4</strong>\n   </td>\n   <td><strong>phi-4-quantized.w8a8<br>(this model)</strong>\n   </td>\n   <td><strong>Recovery</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (5-shot)\n   </td>\n   <td>80.30\n   </td>\n   <td>80.39\n   </td>\n   <td>100.1%\n   </td>\n  </tr>\n  <tr>\n   <td>ARC Challenge (25-shot)\n   </td>\n   <td>64.42\n   </td>\n   <td>64.33\n   </td>\n   <td>99.9%\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (5-shot, strict-match)\n   </td>\n   <td>90.07\n   </td>\n   <td>90.30\n   </td>\n   <td>100.3%\n   </td>\n  </tr>\n  <tr>\n   <td>Hellaswag (10-shot)\n   </td>\n   <td>84.37\n   </td>\n   <td>84.30\n   </td>\n   <td>99.9%\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>80.58\n   </td>\n   <td>79.95\n   </td>\n   <td>99.2%\n   </td>\n  </tr>\n  <tr>\n   <td>TruthfulQA (0-shot, mc2)\n   </td>\n   <td>59.37\n   </td>\n   <td>58.82\n   </td>\n   <td>99.1%\n   </td>\n  </tr>\n  <tr>\n   <td><strong>Average</strong>\n   </td>\n   <td><strong>76.52</strong>\n   </td>\n   <td><strong>76.35</strong>\n   </td>\n   <td><strong>99.8%</strong>\n   </td>\n  </tr>\n</table>\n\n"
      language:
        - en
      license: mit
      licenseLink: https://opensource.org/licenses/MIT
      tasks:
        - text-generation
      createTimeSinceEpoch: "1740960000"
      lastUpdateTimeSinceEpoch: "1740960000"
      customProperties:
        8-bit:
            metadataType: MetadataStringValue
            string_value: ""
        INT8:
            metadataType: MetadataStringValue
            string_value: ""
        W8A8:
            metadataType: MetadataStringValue
            string_value: ""
        chat:
            metadataType: MetadataStringValue
            string_value: ""
        code:
            metadataType: MetadataStringValue
            string_value: ""
        compressed-tensors:
            metadataType: MetadataStringValue
            string_value: ""
        llmcompressor:
            metadataType: MetadataStringValue
            string_value: ""
        math:
            metadataType: MetadataStringValue
            string_value: ""
        neuralmagic:
            metadataType: MetadataStringValue
            string_value: ""
        nlp:
            metadataType: MetadataStringValue
            string_value: ""
        phi:
            metadataType: MetadataStringValue
            string_value: ""
        phi3:
            metadataType: MetadataStringValue
            string_value: ""
        quantized:
            metadataType: MetadataStringValue
            string_value: ""
        redhat:
            metadataType: MetadataStringValue
            string_value: ""
        safetensors:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-phi-4-quantized-w8a8:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1744995117000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
    - name: granite-3.1-8b-starter-v2
      provider: IBM
      description: The model is designed to respond to general instructions and can be used to build AI assistants for multiple domains, including business applications.
      readme: |
        # granite-3.1-8b-starter-v2

        Version 2 of the base/student Granite 3.1 model for fine-tuning
      language:
        - en
        - de
        - es
        - fr
        - ja
        - pt
        - ar
        - cs
        - it
        - ko
        - nl
        - zh
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0
      tasks:
        - text-to-text
      createTimeSinceEpoch: "1739776988000"
      lastUpdateTimeSinceEpoch: "1747377277000"
      customProperties:
        conversational:
            metadataType: MetadataStringValue
            string_value: ""
        featured:
            metadataType: MetadataStringValue
            string_value: ""
        granite:
            metadataType: MetadataStringValue
            string_value: ""
        granite-3.1:
            metadataType: MetadataStringValue
            string_value: ""
        language:
            metadataType: MetadataStringValue
            string_value: ""
        text-generation-inference:
            metadataType: MetadataStringValue
            string_value: ""
        validated:
            metadataType: MetadataStringValue
            string_value: ""
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-starter-v2:1.5
          createTimeSinceEpoch: "1739776988000"
          lastUpdateTimeSinceEpoch: "1747377277000"
          customProperties:
            source:
                string_value: registry.redhat.io
            type:
                string_value: modelcar
